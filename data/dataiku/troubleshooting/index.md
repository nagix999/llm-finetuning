Troubleshooting[¶](#troubleshooting "Permalink to this heading")
================================================================



* [Diagnosing and debugging issues](diagnosing.html)
	+ [Initial investigation](diagnosing.html#initial-investigation)
	+ [Getting an instance diagnosis](diagnosing.html#getting-an-instance-diagnosis)
* [Obtaining support](obtaining-support.html)
	+ [Academy](obtaining-support.html#academy)
	+ [Community Answers](obtaining-support.html#community-answers)
	+ [Live chat](obtaining-support.html#live-chat)
	+ [Editor support (for Dataiku Cloud customers specifically)](obtaining-support.html#editor-support-for-dataiku-cloud-customers-specifically)
	+ [Editor support (for all other Dataiku customers)](obtaining-support.html#editor-support-for-all-other-dataiku-customers)
* [Support tiers](support-tiers.html)
	+ [Supported (default)](support-tiers.html#supported-default)
	+ [Experimental](support-tiers.html#experimental)
	+ [Tier 2 support](support-tiers.html#tier-2-support)
	+ [Not supported](support-tiers.html#not-supported)
	+ [Public Preview](support-tiers.html#public-preview)
	+ [Deprecated](support-tiers.html#deprecated)
* [Common issues](problems/index.html)
	+ [DSS does not start / Cannot connect](problems/does-not-start.html)
	+ [Cannot login to DSS](problems/cannot-login.html)
	+ [DSS crashes / The “Disconnected” overlay appears](problems/crashes.html)
	+ [Websockets problems](problems/websockets.html)
	+ [Cannot connect to a SQL database](problems/cannot-connect-sql.html)
	+ [A job fails](problems/job-fails.html)
	+ [A scenario fails](problems/scenario-fails.html)
	+ [A ML model training fails](problems/ml-train-fails.html)
	+ [“Your user profile does not allow” issues](problems/user-profile-issues.html)
	+ [Receiving “java.lang.NoClassDefFoundError”](problems/no-class-def-found.html)
	+ [The server selected protocol version TLS10 is not accepted by client preferences \[TLS12]](problems/tls-error.html)
* [Error codes](errors/index.html)
	+ [ERR\_ACTIVITY\_DIRECTORY\_SIZE\_LIMIT\_REACHED: Job activity directory size limit reached](errors/ERR_ACTIVITY_DIRECTORY_SIZE_LIMIT_REACHED.html)
	+ [ERR\_BUNDLE\_ACTIVATE\_BAD\_CONNECTION\_PERMISSIONS: Connection is not freely usable](errors/ERR_BUNDLE_ACTIVATE_BAD_CONNECTION_PERMISSIONS.html)
	+ [ERR\_BUNDLE\_ACTIVATE\_BAD\_CONNECTION\_TYPE: Connection is the wrong type](errors/ERR_BUNDLE_ACTIVATE_BAD_CONNECTION_TYPE.html)
	+ [ERR\_BUNDLE\_ACTIVATE\_CONNECTION\_NOT\_WRITABLE: Connection is not writable](errors/ERR_BUNDLE_ACTIVATE_CONNECTION_NOT_WRITABLE.html)
	+ [ERR\_BUNDLE\_ACTIVATE\_MISSING\_CONNECTION: Connection is missing](errors/ERR_BUNDLE_ACTIVATE_MISSING_CONNECTION.html)
	+ [ERR\_CLUSTERS\_INVALID\_SELECTED: Selected cluster does not exist](errors/ERR_CLUSTERS_INVALID_SELECTED.html)
	+ [ERR\_CODEENV\_CONTAINER\_IMAGE\_FAILED: Could not build container image for this code environment](errors/ERR_CODEENV_CONTAINER_IMAGE_FAILED.html)
	+ [ERR\_CODEENV\_CONTAINER\_IMAGE\_TAG\_NOT\_FOUND: Container image tag not found for this Code environment](errors/ERR_CODEENV_CONTAINER_IMAGE_TAG_NOT_FOUND.html)
	+ [ERR\_CODEENV\_CREATION\_FAILED: Could not create this code environment](errors/ERR_CODEENV_CREATION_FAILED.html)
	+ [ERR\_CODEENV\_DELETION\_FAILED: Could not delete this code environment](errors/ERR_CODEENV_DELETION_FAILED.html)
	+ [ERR\_CODEENV\_EXISTING\_ENV: Code environment already exists](errors/ERR_CODEENV_EXISTING_ENV.html)
	+ [ERR\_CODEENV\_INCORRECT\_ENV\_TYPE: Wrong type of Code environment](errors/ERR_CODEENV_INCORRECT_ENV_TYPE.html)
	+ [ERR\_CODEENV\_INVALID\_CODE\_ENV\_ARCHIVE: Invalid code environment archive](errors/ERR_CODEENV_INVALID_CODE_ENV_ARCHIVE.html)
	+ [ERR\_CODEENV\_JUPYTER\_SUPPORT\_INSTALL\_FAILED: Could not install Jupyter support in this code environment](errors/ERR_CODEENV_JUPYTER_SUPPORT_INSTALL_FAILED.html)
	+ [ERR\_CODEENV\_JUPYTER\_SUPPORT\_REMOVAL\_FAILED: Could not remove Jupyter support from this code environment](errors/ERR_CODEENV_JUPYTER_SUPPORT_REMOVAL_FAILED.html)
	+ [ERR\_CODEENV\_MISSING\_DEEPHUB\_ENV: Code environment for deep learning does not exist](errors/ERR_CODEENV_MISSING_DEEPHUB_ENV.html)
	+ [ERR\_CODEENV\_MISSING\_ENV: Code environment does not exists](errors/ERR_CODEENV_MISSING_ENV.html)
	+ [ERR\_CODEENV\_MISSING\_ENV\_VERSION: Code environment version does not exists](errors/ERR_CODEENV_MISSING_ENV_VERSION.html)
	+ [ERR\_CODEENV\_NO\_CREATION\_PERMISSION: User not allowed to create Code environments](errors/ERR_CODEENV_NO_CREATION_PERMISSION.html)
	+ [ERR\_CODEENV\_NO\_USAGE\_PERMISSION: User not allowed to use this Code environment](errors/ERR_CODEENV_NO_USAGE_PERMISSION.html)
	+ [ERR\_CODEENV\_NOT\_USING\_LATEST\_DEEPHUB\_ENV: Not using latest version of code environment for deep learning](errors/ERR_CODEENV_NOT_USING_LATEST_DEEPHUB_ENV.html)
	+ [ERR\_CODEENV\_UNSUPPORTED\_OPERATION\_FOR\_ENV\_TYPE: Operation not supported for this type of Code environment](errors/ERR_CODEENV_UNSUPPORTED_OPERATION_FOR_ENV_TYPE.html)
	+ [ERR\_CODEENV\_UPDATE\_FAILED: Could not update this code environment](errors/ERR_CODEENV_UPDATE_FAILED.html)
	+ [ERR\_CONNECTION\_ALATION\_REGISTRATION\_FAILED: Failed to register Alation integration](errors/ERR_CONNECTION_ALATION_REGISTRATION_FAILED.html)
	+ [ERR\_CONNECTION\_API\_BAD\_CONFIG: Bad configuration for connection](errors/ERR_CONNECTION_API_BAD_CONFIG.html)
	+ [ERR\_CONNECTION\_AZURE\_INVALID\_CONFIG: Invalid Azure connection configuration](errors/ERR_CONNECTION_AZURE_INVALID_CONFIG.html)
	+ [ERR\_CONNECTION\_DUMP\_FAILED: Failed to dump connection tables](errors/ERR_CONNECTION_DUMP_FAILED.html)
	+ [ERR\_CONNECTION\_INVALID\_CONFIG: Invalid connection configuration](errors/ERR_CONNECTION_INVALID_CONFIG.html)
	+ [ERR\_CONNECTION\_LIST\_HIVE\_FAILED: Failed to list indexable Hive connections](errors/ERR_CONNECTION_LIST_HIVE_FAILED.html)
	+ [ERR\_CONNECTION\_S3\_INVALID\_CONFIG: Invalid S3 connection configuration](errors/ERR_CONNECTION_S3_INVALID_CONFIG.html)
	+ [ERR\_CONNECTION\_SQL\_INVALID\_CONFIG: Invalid SQL connection configuration](errors/ERR_CONNECTION_SQL_INVALID_CONFIG.html)
	+ [ERR\_CONNECTION\_SSH\_INVALID\_CONFIG: Invalid SSH connection configuration](errors/ERR_CONNECTION_SSH_INVALID_CONFIG.html)
	+ [ERR\_CONTAINER\_CONF\_NO\_USAGE\_PERMISSION: User not allowed to use this containerized execution configuration](errors/ERR_CONTAINER_CONF_NO_USAGE_PERMISSION.html)
	+ [ERR\_CONTAINER\_CONF\_NOT\_FOUND: The selected container configuration was not found](errors/ERR_CONTAINER_CONF_NOT_FOUND.html)
	+ [ERR\_CONTAINER\_IMAGE\_PUSH\_FAILED: Container image push failed](errors/ERR_CONTAINER_IMAGE_PUSH_FAILED.html)
	+ [ERR\_DASHBOARD\_EXPORT\_SAND\_BOXING\_ERROR: Chrome cannot start in the “sandbox” mode](errors/ERR_DASHBOARD_EXPORT_SAND_BOXING_ERROR.html)
	+ [ERR\_DATASET\_ACTION\_NOT\_SUPPORTED: Action not supported for this kind of dataset](errors/ERR_DATASET_ACTION_NOT_SUPPORTED.html)
	+ [ERR\_DATASET\_CSV\_ROW\_TOO\_LARGE: Error in CSV file: Dataset row is too long to be processed](errors/ERR_DATASET_CSV_ROW_TOO_LARGE.html)
	+ [ERR\_DATASET\_CSV\_UNTERMINATED\_QUOTE: Error in CSV file: Unterminated quote](errors/ERR_DATASET_CSV_UNTERMINATED_QUOTE.html)
	+ [ERR\_DATASET\_HIVE\_INCOMPATIBLE\_SCHEMA: Dataset schema not compatible with Hive](errors/ERR_DATASET_HIVE_INCOMPATIBLE_SCHEMA.html)
	+ [ERR\_DATASET\_INVALID\_CONFIG: Invalid dataset configuration](errors/ERR_DATASET_INVALID_CONFIG.html)
	+ [ERR\_DATASET\_INVALID\_FORMAT\_CONFIG: Invalid format configuration for this dataset](errors/ERR_DATASET_INVALID_FORMAT_CONFIG.html)
	+ [ERR\_DATASET\_INVALID\_METRIC\_IDENTIFIER: Invalid metric identifier](errors/ERR_DATASET_INVALID_METRIC_IDENTIFIER.html)
	+ [ERR\_DATASET\_INVALID\_PARTITIONING\_CONFIG: Invalid dataset partitioning configuration](errors/ERR_DATASET_INVALID_PARTITIONING_CONFIG.html)
	+ [ERR\_DATASET\_PARTITION\_EMPTY: Input partition is empty](errors/ERR_DATASET_PARTITION_EMPTY.html)
	+ [ERR\_DATASET\_TRUNCATED\_COMPRESSED\_DATA: Error in compressed file: Unexpected end of file](errors/ERR_DATASET_TRUNCATED_COMPRESSED_DATA.html)
	+ [ERR\_ENDPOINT\_INVALID\_CONFIG: Invalid configuration for API Endpoint](errors/ERR_ENDPOINT_INVALID_CONFIG.html)
	+ [ERR\_EXPORT\_OUTPUT\_TOO\_LARGE: Export file size limit reached](errors/ERR_EXPORT_OUTPUT_TOO_LARGE.html)
	+ [ERR\_FOLDER\_INVALID\_CONFIG: Invalid managed folder configuration](errors/ERR_FOLDER_INVALID_CONFIG.html)
	+ [ERR\_FOLDER\_INVALID\_PARTITIONING\_CONFIG: Invalid folder partitioning configuration](errors/ERR_FOLDER_INVALID_PARTITIONING_CONFIG.html)
	+ [ERR\_FORMAT\_BOUNDING\_BOXES: Invalid format of column representing bounding boxes](errors/ERR_FORMAT_BOUNDING_BOXES.html)
	+ [ERR\_FORMAT\_LINE\_TOO\_LARGE: Line is too long to be processed](errors/ERR_FORMAT_LINE_TOO_LARGE.html)
	+ [ERR\_FORMAT\_TYPE\_MISSING: Dataset is missing a format type](errors/ERR_FORMAT_TYPE_MISSING.html)
	+ [ERR\_FSPROVIDER\_CANNOT\_CREATE\_FOLDER\_ON\_DIRECTORY\_UNAWARE\_FS: Cannot create a folder on this type of file system](errors/ERR_FSPROVIDER_CANNOT_CREATE_FOLDER_ON_DIRECTORY_UNAWARE_FS.html)
	+ [ERR\_FSPROVIDER\_DEST\_PATH\_ALREADY\_EXISTS: Destination path already exists](errors/ERR_FSPROVIDER_DEST_PATH_ALREADY_EXISTS.html)
	+ [ERR\_FSPROVIDER\_FSLIKE\_REACH\_OUT\_OF\_ROOT: Illegal attempt to access data out of connection root path](errors/ERR_FSPROVIDER_FSLIKE_REACH_OUT_OF_ROOT.html)
	+ [ERR\_FSPROVIDER\_HTTP\_CONNECTION\_FAILED: HTTP connection failed](errors/ERR_FSPROVIDER_HTTP_CONNECTION_FAILED.html)
	+ [ERR\_FSPROVIDER\_HTTP\_INVALID\_URI: Invalid HTTP URI](errors/ERR_FSPROVIDER_HTTP_INVALID_URI.html)
	+ [ERR\_FSPROVIDER\_HTTP\_REQUEST\_FAILED: HTTP request failed](errors/ERR_FSPROVIDER_HTTP_REQUEST_FAILED.html)
	+ [ERR\_FSPROVIDER\_ILLEGAL\_PATH: Illegal path for that file system](errors/ERR_FSPROVIDER_ILLEGAL_PATH.html)
	+ [ERR\_FSPROVIDER\_INVALID\_CONFIG: Invalid configuration](errors/ERR_FSPROVIDER_INVALID_CONFIG.html)
	+ [ERR\_FSPROVIDER\_INVALID\_FILE\_NAME: Invalid file name](errors/ERR_FSPROVIDER_INVALID_FILE_NAME.html)
	+ [ERR\_FSPROVIDER\_LOCAL\_LIST\_FAILED: Could not list local directory](errors/ERR_FSPROVIDER_LOCAL_LIST_FAILED.html)
	+ [ERR\_FSPROVIDER\_PATH\_DOES\_NOT\_EXIST: Path in dataset or folder does not exist](errors/ERR_FSPROVIDER_PATH_DOES_NOT_EXIST.html)
	+ [ERR\_FSPROVIDER\_ROOT\_PATH\_DOES\_NOT\_EXIST: Root path of the dataset or folder does not exist](errors/ERR_FSPROVIDER_ROOT_PATH_DOES_NOT_EXIST.html)
	+ [ERR\_FSPROVIDER\_SSH\_CONNECTION\_FAILED: Failed to establish SSH connection](errors/ERR_FSPROVIDER_SSH_CONNECTION_FAILED.html)
	+ [ERR\_FSPROVIDER\_TOO\_MANY\_FILES: Attempted to enumerate too many files](errors/ERR_FSPROVIDER_TOO_MANY_FILES.html)
	+ [ERR\_HIVE\_HS2\_CONNECTION\_FAILED: Failed to establish HiveServer2 connection](errors/ERR_HIVE_HS2_CONNECTION_FAILED.html)
	+ [ERR\_HIVE\_LEGACY\_UNION\_SUPPORT: Your current Hive version doesn’t support UNION clause but only supports UNION ALL, which does not remove duplicates](errors/ERR_HIVE_LEGACY_UNION_SUPPORT.html)
	+ [ERR\_JOB\_INPUT\_DATASET\_NOT\_READY\_NO\_FILES: Input dataset is not ready (no files found)](errors/ERR_JOB_INPUT_DATASET_NOT_READY_NO_FILES.html)
	+ [ERR\_LICENSING\_TRIAL\_INTERNAL\_ERROR: Internal error trying to get a trial token](errors/ERR_LICENSING_TRIAL_INTERNAL_ERROR.html)
	+ [ERR\_LICENSING\_TRIAL\_STATUS\_ERROR: Internal error trying to get a trial status](errors/ERR_LICENSING_TRIAL_STATUS_ERROR.html)
	+ [ERR\_METRIC\_DATASET\_COMPUTATION\_FAILED: Metrics computation completely failed](errors/ERR_METRIC_DATASET_COMPUTATION_FAILED.html)
	+ [ERR\_METRIC\_ENGINE\_RUN\_FAILED: One of the metrics engine failed to run](errors/ERR_METRIC_ENGINE_RUN_FAILED.html)
	+ [ERR\_MISC\_DISK\_FULL: Disk is almost full](errors/ERR_MISC_DISK_FULL.html)
	+ [ERR\_MISC\_EIDB: Missing, locked, unreachable or corrupted internal database](errors/ERR_MISC_EIDB.html)
	+ [ERR\_MISC\_ENOSPC: Out of disk space](errors/ERR_MISC_ENOSPC.html)
	+ [ERR\_MISC\_EOPENF: Too many open files](errors/ERR_MISC_EOPENF.html)
	+ [ERR\_ML\_MODEL\_DETAILS\_OVERFLOW: Model details exceed size limit](errors/ERR_ML_MODEL_DETAILS_OVERFLOW.html)
	+ [ERR\_ML\_VERTICA\_NOT\_SUPPORTED: Vertica ML backend is no longer supported](errors/ERR_ML_VERTICA_NOT_SUPPORTED.html)
	+ [ERR\_NOT\_USABLE\_FOR\_USER: You may not use this connection](errors/ERR_NOT_USABLE_FOR_USER.html)
	+ [ERR\_OBJECT\_OPERATION\_NOT\_AVAILABLE\_FOR\_TYPE: Operation not supported for this kind of object](errors/ERR_OBJECT_OPERATION_NOT_AVAILABLE_FOR_TYPE.html)
	+ [ERR\_PLUGIN\_CANNOT\_LOAD: Plugin cannot be loaded](errors/ERR_PLUGIN_CANNOT_LOAD.html)
	+ [ERR\_PLUGIN\_COMPONENT\_NOT\_INSTALLED: Plugin component not installed or removed](errors/ERR_PLUGIN_COMPONENT_NOT_INSTALLED.html)
	+ [ERR\_PLUGIN\_DEV\_INVALID\_COMPONENT\_PARAMETER: Invalid parameter for plugin component creation](errors/ERR_PLUGIN_DEV_INVALID_COMPONENT_PARAMETER.html)
	+ [ERR\_PLUGIN\_DEV\_INVALID\_DEFINITION: The descriptor of the plugin is invalid](errors/ERR_PLUGIN_DEV_INVALID_DEFINITION.html)
	+ [ERR\_PLUGIN\_MISSING\_IN\_CONTAINER\_IMAGE: Plugin is missing in container image](errors/ERR_PLUGIN_MISSING_IN_CONTAINER_IMAGE.html)
	+ [ERR\_PLUGIN\_INVALID\_DEFINITION: The plugin’s definition is invalid](errors/ERR_PLUGIN_INVALID_DEFINITION.html)
	+ [ERR\_PLUGIN\_NOT\_INSTALLED: Plugin not installed or removed](errors/ERR_PLUGIN_NOT_INSTALLED.html)
	+ [ERR\_PLUGIN\_WITHOUT\_CODEENV: The plugin has no code env specification](errors/ERR_PLUGIN_WITHOUT_CODEENV.html)
	+ [ERR\_PLUGIN\_WRONG\_TYPE: Unexpected type of plugin](errors/ERR_PLUGIN_WRONG_TYPE.html)
	+ [ERR\_PROJECT\_INVALID\_ARCHIVE: Invalid project archive](errors/ERR_PROJECT_INVALID_ARCHIVE.html)
	+ [ERR\_PROJECT\_INVALID\_PROJECT\_KEY: Invalid project key](errors/ERR_PROJECT_INVALID_PROJECT_KEY.html)
	+ [ERR\_PROJECT\_UNKNOWN\_PROJECT\_KEY: Unknown project key](errors/ERR_PROJECT_UNKNOWN_PROJECT_KEY.html)
	+ [ERR\_RECIPE\_CANNOT\_CHANGE\_ENGINE: Cannot change engine](errors/ERR_RECIPE_CANNOT_CHANGE_ENGINE.html)
	+ [ERR\_RECIPE\_CANNOT\_CHECK\_SCHEMA\_CONSISTENCY: Cannot check schema consistency](errors/ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY.html)
	+ [ERR\_RECIPE\_CANNOT\_CHECK\_SCHEMA\_CONSISTENCY\_EXPENSIVE: Cannot check schema consistency: expensive checks disabled](errors/ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_EXPENSIVE.html)
	+ [ERR\_RECIPE\_CANNOT\_CHECK\_SCHEMA\_CONSISTENCY\_NEEDS\_BUILD: Cannot compute output schema with an empty input dataset. Build the input dataset first.](errors/ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_NEEDS_BUILD.html)
	+ [ERR\_RECIPE\_CANNOT\_CHECK\_SCHEMA\_CONSISTENCY\_ON\_RECIPE\_TYPE: Cannot check schema consistency on this kind of recipe](errors/ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_ON_RECIPE_TYPE.html)
	+ [ERR\_RECIPE\_CANNOT\_CHECK\_SCHEMA\_CONSISTENCY\_WITH\_RECIPE\_CONFIG: Cannot check schema consistency because of recipe configuration](errors/ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_WITH_RECIPE_CONFIG.html)
	+ [ERR\_RECIPE\_CANNOT\_CHANGE\_ENGINE: Not compatible with Spark](errors/ERR_RECIPE_CANNOT_SPARK.html)
	+ [ERR\_RECIPE\_CANNOT\_USE\_ENGINE: Cannot use the selected engine for this recipe](errors/ERR_RECIPE_CANNOT_USE_ENGINE.html)
	+ [ERR\_RECIPE\_ENGINE\_NOT\_DWH: Error in recipe engine: SQLServer is not Data Warehouse edition](errors/ERR_RECIPE_ENGINE_NOT_DWH.html)
	+ [ERR\_RECIPE\_INCONSISTENT\_I\_O: Inconsistent recipe input or output](errors/ERR_RECIPE_INCONSISTENT_I_O.html)
	+ [ERR\_RECIPE\_SYNC\_AWS\_DIFFERENT\_REGIONS: Error in recipe engine: Redshift and S3 are in different AWS regions](errors/ERR_RECIPE_SYNC_AWS_DIFFERENT_REGIONS.html)
	+ [ERR\_RECIPE\_PDEP\_UPDATE\_REQUIRED: Partition dependency update required](errors/ERR_RECIPE_PDEP_UPDATE_REQUIRED.html)
	+ [ERR\_RECIPE\_SPLIT\_INVALID\_COMPUTED\_COLUMNS: Invalid computed column](errors/ERR_RECIPE_SPLIT_INVALID_COMPUTED_COLUMNS.html)
	+ [ERR\_SCENARIO\_INVALID\_STEP\_CONFIG: Invalid scenario step configuration](errors/ERR_SCENARIO_INVALID_STEP_CONFIG.html)
	+ [ERR\_SECURITY\_CRUD\_INVALID\_SETTINGS: The user attributes submitted for a change are invalid](errors/ERR_SECURITY_CRUD_INVALID_SETTINGS.html)
	+ [ERR\_SECURITY\_GROUP\_EXISTS: The new requested group already exists](errors/ERR_SECURITY_GROUP_EXISTS.html)
	+ [ERR\_SECURITY\_INVALID\_NEW\_PASSWORD: The new password is invalid](errors/ERR_SECURITY_INVALID_NEW_PASSWORD.html)
	+ [ERR\_SECURITY\_INVALID\_PASSWORD: The password hash from the database is invalid](errors/ERR_SECURITY_INVALID_PASSWORD.html)
	+ [ERR\_SECURITY\_DECRYPTION\_FAILED: Decryption failed due to invalid HMAC](errors/ERR_SECURITY_DECRYPTION_FAILED.html)
	+ [ERR\_SECURITY\_MUS\_USER\_UNMATCHED: The DSS user is not configured to be matched onto a system user](errors/ERR_SECURITY_MUS_USER_UNMATCHED.html)
	+ [ERR\_SECURITY\_PATH\_ESCAPE: The requested file is not within any allowed directory](errors/ERR_SECURITY_PATH_ESCAPE.html)
	+ [ERR\_SECURITY\_USER\_EXISTS: The requested user for creation already exists](errors/ERR_SECURITY_USER_EXISTS.html)
	+ [ERR\_SECURITY\_WRONG\_PASSWORD: The old password provided for password change is invalid](errors/ERR_SECURITY_WRONG_PASSWORD.html)
	+ [ERR\_SPARK\_FAILED\_DRIVER\_OOM: Spark failure: out of memory in driver](errors/ERR_SPARK_FAILED_DRIVER_OOM.html)
	+ [ERR\_SPARK\_FAILED\_TASK\_OOM: Spark failure: out of memory in task](errors/ERR_SPARK_FAILED_TASK_OOM.html)
	+ [ERR\_SPARK\_FAILED\_YARN\_KILLED\_MEMORY: Spark failure: killed by YARN (excessive memory usage)](errors/ERR_SPARK_FAILED_YARN_KILLED_MEMORY.html)
	+ [ERR\_SPARK\_PYSPARK\_CODE\_FAILED\_UNSPECIFIED: Pyspark code failed](errors/ERR_SPARK_PYSPARK_CODE_FAILED_UNSPECIFIED.html)
	+ [ERR\_SPARK\_SQL\_LEGACY\_UNION\_SUPPORT: Your current Spark version doesn’t support UNION clause but only supports UNION ALL, which does not remove duplicates](errors/ERR_SPARK_SQL_LEGACY_UNION_SUPPORT.html)
	+ [ERR\_SQL\_CANNOT\_LOAD\_DRIVER: Failed to load database driver](errors/ERR_SQL_CANNOT_LOAD_DRIVER.html)
	+ [ERR\_SQL\_DB\_UNREACHABLE: Failed to reach database](errors/ERR_SQL_DB_UNREACHABLE.html)
	+ [ERR\_SQL\_IMPALA\_MEMORYLIMIT: Impala memory limit exceeded](errors/ERR_SQL_IMPALA_MEMORYLIMIT.html)
	+ [ERR\_SQL\_POSTGRESQL\_TOOMANYSESSIONS: too many sessions open concurrently](errors/ERR_SQL_POSTGRESQL_TOOMANYSESSIONS.html)
	+ [ERR\_SQL\_TABLE\_NOT\_FOUND: SQL Table not found](errors/ERR_SQL_TABLE_NOT_FOUND.html)
	+ [ERR\_SQL\_VERTICA\_TOOMANYROS: Error in Vertica: too many ROS](errors/ERR_SQL_VERTICA_TOOMANYROS.html)
	+ [ERR\_SQL\_VERTICA\_TOOMANYSESSIONS: Error in Vertica: too many sessions open concurrently](errors/ERR_SQL_VERTICA_TOOMANYSESSIONS.html)
	+ [ERR\_SYNAPSE\_CSV\_DELIMITER: Bad delimiter setup](errors/ERR_SYNAPSE_CSV_DELIMITER.html)
	+ [ERR\_TRANSACTION\_FAILED\_ENOSPC: Out of disk space](errors/ERR_TRANSACTION_FAILED_ENOSPC.html)
	+ [ERR\_TRANSACTION\_GIT\_COMMMIT\_FAILED: Failed committing changes](errors/ERR_TRANSACTION_GIT_COMMIT_FAILED.html)
	+ [ERR\_USER\_ACTION\_FORBIDDEN\_BY\_PROFILE: Your user profile does not allow you to perform this action](errors/ERR_USER_ACTION_FORBIDDEN_BY_PROFILE.html)
	+ [INFO\_RECIPE\_POTENTIAL\_FAST\_PATH: Potential fast path configuration](errors/INFO_RECIPE_POTENTIAL_FAST_PATH.html)
	+ [INFO\_RECIPE\_IMPALA\_POTENTIAL\_FAST\_PATH: Potential Impala fast path configuration](errors/INFO_RECIPE_IMPALA_POTENTIAL_FAST_PATH.html)
	+ [WARN\_ACTIVITY\_WAITING\_K8S\_CONTAINERSTARTING\_CLOUD: Execution container is initializing](errors/WARN_ACTIVITY_WAITING_K8S_CONTAINERSTARTING_CLOUD.html)
	+ [WARN\_ACTIVITY\_WAITING\_K8S\_POD\_PENDING\_CLOUD: Container will start soon](errors/WARN_ACTIVITY_WAITING_K8S_POD_PENDING_CLOUD.html)
	+ [WARN\_ACTIVITY\_WAITING\_K8S\_QUOTA\_EXCEEDED\_CLOUD: You have exceeded your RAM and CPU quotas](errors/WARN_ACTIVITY_WAITING_K8S_QUOTA_EXCEEDED_CLOUD.html)
	+ [WARN\_ACTIVITY\_WAITING\_QUEUED\_CLOUD: Your activity is queued](errors/WARN_ACTIVITY_WAITING_QUEUED_CLOUD.html)
	+ [WARN\_CLUSTERS\_NONE\_SELECTED\_GLOBAL: No default cluster selected](errors/WARN_CLUSTERS_NONE_SELECTED_GLOBAL.html)
	+ [WARN\_CLUSTERS\_NONE\_SELECTED\_PROJECT: No cluster selected in project](errors/WARN_CLUSTERS_NONE_SELECTED_PROJECT.html)
	+ [WARN\_CONNECTION\_HDFS\_ACL\_SUBDIRECTORY: subdirectory ACL synchronization mode](errors/WARN_CONNECTION_HDFS_ACL_SUBDIRECTORY.html)
	+ [WARN\_CONNECTION\_NO\_HADOOP\_INTERFACE: no Hadoop interface set](errors/WARN_CONNECTION_NO_HADOOP_INTERFACE.html)
	+ [WARN\_CONNECTION\_DATABRICKS\_NO\_AUTOFASTWRITE: automatic fast\-write disabled](errors/WARN_CONNECTION_DATABRICKS_NO_AUTOFASTWRITE.html)
	+ [WARN\_CONNECTION\_SNOWFLAKE\_NO\_AUTOFASTWRITE: automatic fast\-write disabled](errors/WARN_CONNECTION_SNOWFLAKE_NO_AUTOFASTWRITE.html)
	+ [WARN\_CONNECTION\_SPARK\_NO\_GROUP\_WITH\_DETAILS\_READ\_ACCESS: no groups allowed to read connection details](errors/WARN_CONNECTION_SPARK_NO_GROUP_WITH_DETAILS_READ_ACCESS.html)
	+ [WARN\_FOLDER\_CONNECTION\_TYPE\_ERROR: Invalid connection linked to a managed folder](errors/WARN_FOLDER_CONNECTION_TYPE_ERROR.html)
	+ [WARN\_JOBS\_MAX\_OVER\_MAX\_ACTIVITIES: Jobs \- Max jobs is over max activities](errors/WARN_JOBS_MAX_OVER_MAX_ACTIVITIES.html)
	+ [WARN\_JOBS\_MAX\_TOO\_HIGH: Jobs \- Max value too high](errors/WARN_JOBS_MAX_TOO_HIGH.html)
	+ [WARN\_JOBS\_NO\_LIMIT: Jobs \- No limits set](errors/WARN_JOBS_NO_LIMIT.html)
	+ [WARN\_JVM\_CONFIG\_XMX\_IN\_RED\_ZONE: Sub optimal Xmx value](errors/WARN_JVM_CONFIG_XMX_IN_RED_ZONE.html)
	+ [WARN\_JVM\_CONFIG\_KERNEL\_XMX\_OVER\_THRESHOLD: Xmx value for kernel over threshold](errors/WARN_JVM_CONFIG_KERNEL_XMX_OVER_THRESHOLD.html)
	+ [WARN\_MISC\_AUDIT\_NO\_LOG4J\_LOCAL\_TARGET: No Log4j local target](errors/WARN_MISC_AUDIT_NO_LOG4J_LOCAL_TARGET.html)
	+ [WARN\_MISC\_CODE\_ENV\_BUILTIN\_MODIFIED: Built\-in code env modified](errors/WARN_MISC_CODE_ENV_BUILTIN_MODIFIED.html)
	+ [WARN\_MISC\_CODE\_ENV\_DEPRECATED\_INTERPRETER: Deprecated Python interpreter](errors/WARN_MISC_CODE_ENV_DEPRECATED_INTERPRETER.html)
	+ [WARN\_MISC\_CODE\_ENV\_USES\_PYSPARK: pyspark installed in a code environment](errors/WARN_MISC_CODE_ENV_USES_PYSPARK.html)
	+ [WARN\_MISC\_DISK\_MOUNT\_TYPE: non recommended filesystem type](errors/WARN_MISC_DISK_MOUNT_TYPE.html)
	+ [WARN\_MISC\_DISK\_NOEXEC\_FLAG: noexec flag](errors/WARN_MISC_DISK_NOEXEC_FLAG.html)
	+ [WARN\_MISC\_DISK\_ROTATIONAL: Rotational hard drives](errors/WARN_MISC_DISK_ROTATIONAL.html)
	+ [WARN\_MISC\_ENVVAR\_SPECIAL\_CHAR: Environment variables with special characters](errors/WARN_MISC_ENVVAR_SPECIAL_CHAR.html)
	+ [WARN\_MISC\_EVENT\_SERVER\_NO\_TARGET: No target](errors/WARN_MISC_EVENT_SERVER_NO_TARGET.html)
	+ [WARN\_MISC\_JDBC\_JARS\_CONFLICT: JDBC drivers \- some JARs are prone to version conflicts](errors/WARN_MISC_JDBC_JARS_CONFLICT.html)
	+ [WARN\_MISC\_LARGE\_INTERNAL\_DB: internal runtime database is too large](errors/WARN_MISC_LARGE_INTERNAL_DB.html)
	+ [WARN\_PROJECT\_LARGE\_JOB\_HISTORY: Projects \- Too old or too many job logs](errors/WARN_PROJECT_LARGE_JOB_HISTORY.html)
	+ [WARN\_PROJECT\_LARGE\_SCENARIO\_HISTORY: Projects \- Too old or too many scenario run logs](errors/WARN_PROJECT_LARGE_SCENARIO_HISTORY.html)
	+ [WARN\_PROJECT\_LARGE\_STREAMING\_HISTORY: Projects \- Too old or too many continuous activities logs](errors/WARN_PROJECT_LARGE_STREAMING_HISTORY.html)
	+ [WARN\_RECIPE\_SPARK\_INDIRECT\_HDFS: No direct access to read/write HDFS dataset](errors/WARN_RECIPE_SPARK_INDIRECT_HDFS.html)
	+ [WARN\_RECIPE\_SPARK\_INDIRECT\_S3: No direct access to read/write S3 dataset](errors/WARN_RECIPE_SPARK_INDIRECT_S3.html)
	+ [WARN\_SPARK\_NON\_DISTRIBUTED\_READ: Input dataset is read in a non\-distributed way](errors/WARN_SPARK_NON_DISTRIBUTED_READ.html)
	+ [WARN\_SPARK\_NON\_DISTRIBUTED\_WRITE: Output dataset is written in a non\-distributed way](errors/WARN_SPARK_NON_DISTRIBUTED_WRITE.html)
	+ [WARN\_SPARK\_UDFS\_MAY\_BE\_BROKEN: Python UDFs may fail](errors/WARN_SPARK_UDFS_MAY_BE_BROKEN.html)
	+ [WARN\_SPARK\_TASK\_OOM: Some Spark tasks encountered out of memory](errors/WARN_SPARK_TASK_OOM.html)
	+ [WARN\_SPARK\_TASK\_DISKFULL: Some Spark tasks encountered disk space issues](errors/WARN_SPARK_TASK_DISKFULL.html)
	+ [WARN\_SPARK\_K8S\_KILLED\_EXECUTORS: Some Kubernetes executors were killed](errors/WARN_SPARK_K8S_KILLED_EXECUTORS.html)
	+ [WARN\_SPARK\_MISSING\_DRIVER\_TO\_EXECUTOR\_CONNECTIVITY: The Spark driver cannot call into the executors](errors/WARN_SPARK_MISSING_DRIVER_TO_EXECUTOR_CONNECTIVITY.html)
	+ [WARN\_SPARK\_WITH\_DATABRICKS\_DATASET: Not leveraging Databricks compute](errors/WARN_SPARK_WITH_DATABRICKS_DATASET.html)
	+ [WARN\_SECURITY\_NO\_CGROUPS: cgroups for resource control are not enabled](errors/WARN_SECURITY_NO_CGROUPS.html)
	+ [WARN\_SECURITY\_UIF\_NOT\_ENABLED: User Isolation Framework is not enabled](errors/WARN_SECURITY_UIF_NOT_ENABLED.html)
	+ [Undocumented error](errors/undocumented-error.html)