question,A,B,C,D,answer,file
Which keyboard shortcut in Dataiku DSS is used to go to the Flow?,G + F,G + D,G + A,G + R,A,../data/dataiku\accessibility\index.md
"In Dataiku DSS, which shortcut allows you to toggle between the Details and Actions tab for a selected object within the Flow?",SHIFT + CLICK,SPACE,SHIFT + DRAG,Z,B,../data/dataiku\accessibility\index.md
What is the keyboard shortcut to validate a script within a Code Recipe in Dataiku DSS?,CTRL + ENTER,ALT + A,SHIFT + V,CTRL + F,A,../data/dataiku\accessibility\index.md
Which of the following APIs is NOT listed as an additional API in DSS?,Javascript API,Scala API,Python API,R API,C,../data/dataiku\api\index.md
Which of the following sampling methods returns the first rows of the dataset?,HEAD,RANDOM,FULL,RANDOM-COLUMN,A,../data/dataiku\api\js\index.md
What is the default number of rows returned by the Dataiku Javascript API when no sampling method is specified?,10000,15000,20000,25000,C,../data/dataiku\api\js\index.md
Which method would you use to apply a function to each record object in a DataFrame?,mapRows,mapRecords,getRows,getRecord,B,../data/dataiku\api\js\index.md
Which of the following is NOT a designed use case for the DSS Scala API?,In Scala recipes,In Scala notebooks,In custom (plugin) Scala recipes,In external applications outside DSS,D,../data/dataiku\api\scala\index.md
Which of the following is NOT a type of infrastructure managed by the API Deployer?,Static infrastructure,Kubernetes infrastructure,External platform,Virtual machine infrastructure,D,../data/dataiku\apinode\api-deployment-infrastructures.md
What is the purpose of the Event Server in the context of API infrastructure?,To manage API nodes,To store prediction logs for MLOps monitoring,To authenticate users,To deploy API services,B,../data/dataiku\apinode\api-deployment-infrastructures.md
"Which permission allows a user to create, update, or delete deployments on an API infrastructure?",View,Deploy,Admin,None,B,../data/dataiku\apinode\api-deployment-infrastructures.md
Which tool in Dataiku is used to add documentation to an API endpoint?,API Designer,API Deployer,API Node,API Manager,A,../data/dataiku\apinode\api-documentation.md
What format is used for the documentation of API endpoints in Dataiku?,XML,YAML,OpenAPI JSON,HTML,C,../data/dataiku\apinode\api-documentation.md
Which command can be used to query the OpenAPI documentation from outside Dataiku?,curl -X GET http://<deployment uri>/swagger,curl -X POST http://<deployment uri>/swagger,curl -X PUT http://<deployment uri>/swagger,curl -X DELETE http://<deployment uri>/swagger,A,../data/dataiku\apinode\api-documentation.md
Which of the following is NOT a type of endpoint supported by the API node?,Prediction or Clustering endpoint,Python prediction endpoint,Java function endpoint,SQL query endpoint,C,../data/dataiku\apinode\concepts.md
What is the main object that you manage in the API Deployer?,Endpoint,Deployment,Package,API Service,B,../data/dataiku\apinode\concepts.md
Which of the following is a function of the API Deployer?,Creating and designing APIs,Managing the lifecycle of APIs from development to production,Answering HTTP requests,Performing data lookups in DSS datasets,B,../data/dataiku\apinode\concepts.md
Which of the following types of endpoints are supported for an API Service to be compatible with External Deployments in Dataiku?,"Standard Prediction, Clustering, Custom Prediction (Python) or Python Function","Standard Prediction, Clustering, Custom Prediction (R) or R Function","Standard Prediction, Clustering, Custom Prediction (Java) or Java Function","Standard Prediction, Clustering, Custom Prediction (Scala) or Scala Function",A,../data/dataiku\apinode\deploy-anywhere.md
What is the first step in monitoring a deployed API Service in Dataiku?,Checking the model health,Ensuring the endpoint is up and in sync with the expected setup,Building a feedback loop based on logs,Validating the endpoint in the 'Run and test' tab,B,../data/dataiku\apinode\deploy-anywhere.md
Which of the following is NOT a feature of the 'dataset lookup' endpoint?,Lookup in multiple datasets at once,Retrieve arbitrary number of columns,Return multiple dataset lines for each input lookup record,Lookup based on multiple lookup keys,C,../data/dataiku\apinode\endpoint-dataset-lookup.md
What is the default maximum number of allocated pipelines for a dataset lookup endpoint when using the API Deployer?,1,2,8,16,C,../data/dataiku\apinode\endpoint-dataset-lookup.md
Which of the following is a step in creating a dataset lookup endpoint?,Go to the project homepage,Create a new dataset,Deploy the API service,Configure the database connection,A,../data/dataiku\apinode\endpoint-dataset-lookup.md
Which of the following is a method to deploy an MLflow model?,Using the API,From an Experiment Tracking run,Directly from a Jupyter notebook,Both A and B,D,../data/dataiku\apinode\endpoint-mlflow.md
What are the two possible outputs that can be set in the endpoint settings when exposing an MLflow model?,Raw data and restructured,Raw data and formatted,Structured and unstructured,Binary and text,A,../data/dataiku\apinode\endpoint-mlflow.md
What is the main difference between a Python prediction endpoint and a Python function endpoint in Dataiku?,A Python prediction endpoint can only be used for prediction-like use cases.,A Python function endpoint must output a prediction.,A Python prediction endpoint can store data in a database.,A Python function endpoint has a strict concept of input records.,A,../data/dataiku\apinode\endpoint-python-function.md
Which of the following is NOT a parameter that can be configured for performance tuning of a Python function endpoint in Dataiku?,floor,ceil,cruise,speed,D,../data/dataiku\apinode\endpoint-python-function.md
What is the primary purpose of a DSS managed folder in a custom Python prediction model?,To store the serialized version of the model,To store the training data,To store the API documentation,To store the user credentials,A,../data/dataiku\apinode\endpoint-python-prediction.md
Which method must a regression predictor implement in a custom Python prediction model?,"def classify(self, features_df)","def predict(self, features_df)","def train(self, features_df)","def evaluate(self, features_df)",B,../data/dataiku\apinode\endpoint-python-prediction.md
What is the recommended way to manage external libraries for custom model packages in DSS?,Using the builtin env,Using a code env,Installing them manually on the server,Including them in the project’s libraries folder,B,../data/dataiku\apinode\endpoint-python-prediction.md
Which of the following is NOT a key difference between an R prediction endpoint and an R function endpoint?,An R prediction endpoint has a strict concept of input records and output prediction.,An R function endpoint can perform any kind of action and return any form of result.,An R prediction endpoint can store data in a database or a file.,An R function endpoint does not use dataset-based enrichment features.,C,../data/dataiku\apinode\endpoint-r-function.md
What is the recommended method for deploying custom endpoints that use external libraries in Dataiku?,Using the builtin env,Using a code env,Manually installing libraries on the API node,Using the default DSS environment,B,../data/dataiku\apinode\endpoint-r-function.md
Which parameter in the endpoint tuning settings specifies the maximum number of allocated pipelines at any given time?,floor,ceil,cruise,queue,B,../data/dataiku\apinode\endpoint-r-function.md
What is the purpose of a DSS managed folder in a custom R prediction model?,To store the serialized version of the model,To store the API service logs,To store user credentials,To store the API documentation,A,../data/dataiku\apinode\endpoint-r-prediction.md
Which of the following is a mandatory return value for a classification prediction function in a custom R model?,A single numerical value,A list of class to probability mappings,A single character vector representing the predicted class,A list containing additional response keys,C,../data/dataiku\apinode\endpoint-r-prediction.md
What is the recommended method for using external R packages in a custom model?,Using the builtin env,Using a code env,Installing them manually on the server,Including them in the API request,B,../data/dataiku\apinode\endpoint-r-prediction.md
Which of the following statements is true about exposing a SQL query as a DSS API node endpoint?,Hive and Impala are supported.,Only 'regular' SQL connections are possible.,You do not need to install the JDBC driver on the API node servers.,The API node does not handle pooling connections to the database.,B,../data/dataiku\apinode\endpoint-sql-query.md
What is the purpose of the 'post-commit' option in a SQL query endpoint?,To automatically rollback the transaction after the query execution.,To commit the connection after the execution of the query.,To split the query into multiple statements.,To handle the JDBC driver installation automatically.,B,../data/dataiku\apinode\endpoint-sql-query.md
Which of the following parameters is used to set the maximum number of connections that can be opened to the database in the connection pool?,connectionsEvictionTimeMS,evictionIntervalMS,maxPooledConnections,SQL eviction interval,C,../data/dataiku\apinode\endpoint-sql-query.md
What is the primary function of the DSS API Deployer and API Node?,To create visualizations for data analysis,To expose a model trained using the DSS visual machine learning component as an API,To manage user authentication and authorization,To store large datasets,B,../data/dataiku\apinode\endpoint-std.md
Which of the following is NOT a step in exposing a model as an API?,Train the model in Analysis,Deploy the model to Flow,Create a new API service,Create a new dataset,D,../data/dataiku\apinode\endpoint-std.md
What is the default URL format to query a prediction model API?,/public/api/v1/<service_id>/<endpoint_id>/predict,/public/api/v1/<service_id>/<endpoint_id>/forecast,/public/api/v1/<service_id>/<endpoint_id>/predict-effect,/public/api/v1/<service_id>/<endpoint_id>/analyze,A,../data/dataiku\apinode\endpoint-std.md
Which of the following is NOT a type of endpoint supported by the API node?,Exposing a Visual Model,Exposing a Python prediction model,Exposing a Java function,Exposing a SQL query,C,../data/dataiku\apinode\endpoints.md
What is the primary use case for the API node in Dataiku DSS?,To send the features of a record and receive the prediction for this record.,To store large datasets for machine learning models.,To visualize data in real-time dashboards.,To manage user permissions and roles.,A,../data/dataiku\apinode\enrich-prediction-queries.md
Which deployment mode should generally be preferred when the lookup tables are fairly small?,Referenced data,Bundled data,Distributed data,Centralized data,B,../data/dataiku\apinode\enrich-prediction-queries.md
"In the context of Dataiku DSS, what does the 'data enrichment' feature of the API node do?",It visualizes data in real-time.,It enriches records to predict by looking up additional data from specified tables.,It compresses data for faster processing.,It manages user permissions and roles.,B,../data/dataiku\apinode\enrich-prediction-queries.md
Which of the following steps is NOT part of the process to expose a prediction model as a REST API service?,Create the model,Create the API Service,Perform real queries,Install the DSS API Deployer,D,../data/dataiku\apinode\first-service-apideployer.md
What is the URL format to query a prediction model API created directly from the Flow?,/public/api/v1/<service_id>/<endpoint_id>/predict,/public/api/v1/<service_id>/<endpoint_id>/forecast,/public/api/v1/<service_id>/<endpoint_id>/query,/public/api/v1/<service_id>/<endpoint_id>/model,A,../data/dataiku\apinode\first-service-apideployer.md
What is a good practice to ensure that your endpoint is working as expected?,Deploy the model without testing,Add test queries,Skip the API Deployer step,Directly perform real queries,B,../data/dataiku\apinode\first-service-apideployer.md
Which of the following steps is NOT part of the process to expose a prediction model as a REST API service?,Create the model,Create the API Service,Perform a test query,Install a new DSS instance,D,../data/dataiku\apinode\first-service-manual.md
What is the purpose of creating a version and transferring the package in the API deployment process?,To create a backup of the model,To prepare the model for real-time predictions,To download the associated version package for API node activation,To test the model with new data,C,../data/dataiku\apinode\first-service-manual.md
Which method can only be used for prediction or clustering endpoints when creating an API Service?,Create directly from the Flow,Create the API service then the endpoint in API Designer,Add test queries,Perform real queries,A,../data/dataiku\apinode\first-service-manual.md
Which of the following is NOT a function of the API Deployer in Dataiku DSS?,Defining API infrastructures,Deploying API services on an infrastructure,Monitoring the health and status of deployed APIs,Creating predictive models,D,../data/dataiku\apinode\index.md
"In Dataiku DSS, what is the role of an API node?",Managing the deployment of projects,Answering REST API calls,Creating API services,Monitoring the health of the DSS instance,B,../data/dataiku\apinode\index.md
Which of the following can be exposed using the API Deployer in Dataiku DSS?,Predictive models,Arbitrary Python and R functions,SQL queries,All of the above,D,../data/dataiku\apinode\index.md
Under which circumstances do you need to manually install API nodes?,When using Kubernetes-based infrastructures in the API Deployer.,When using Dataiku Cloud stacks.,When creating a static infrastructure in the API Deployer.,When using Dataiku Cloud.,C,../data/dataiku\apinode\installing-apinode.md
Which of the following is NOT a capability that can be exposed through the DSS API Deployer?,Exposing predictive models,Exposing arbitrary Python and R functions,Exposing SQL queries,Exposing Java functions,D,../data/dataiku\apinode\introduction.md
What is the main use case for API services in Dataiku?,To expose predictive models through a REST API,To manage datasets,To create visualizations,To perform data cleaning,A,../data/dataiku\apinode\introduction.md
Which feature is NOT provided by the DSS API Deployer for predictive models?,Powerful logging and auditing capabilities,A/B testing and multi-version evaluation of models,User-aware version dispatch,Automated data labeling,D,../data/dataiku\apinode\introduction.md
Which command is used to activate the latest version of your endpoint?,./bin/apinode-admin service-switch-to-newest <SERVICE_ID>,./bin/apinode-admin service-set-mapping <SERVICE_ID>,./bin/apinode-admin service-list-generations <SERVICE_ID>,./bin/apinode-admin service-activate-latest <SERVICE_ID>,A,../data/dataiku\apinode\managing_versions.md
What is the purpose of running multiple versions of an endpoint simultaneously?,To reduce server load,To perform A/B testing of machine learning models,To increase the speed of query processing,To ensure backward compatibility,B,../data/dataiku\apinode\managing_versions.md
Which command provides information about the versions of an endpoint currently being served and their associated probabilities?,./bin/apinode-admin service-switch-to-newest <SERVICE_ID>,./bin/apinode-admin service-set-mapping <SERVICE_ID>,./bin/apinode-admin service-list-generations <SERVICE_ID>,./bin/apinode-admin service-monitor-versions <SERVICE_ID>,C,../data/dataiku\apinode\managing_versions.md
Which of the following is NOT a characteristic of the API keys authorization method?,You can create an arbitrary number of API keys,API keys are per API Service,Each API key has fine-grained access to the API service,Using multiple API keys gives traceability and the ability to revoke a compromised API key,C,../data/dataiku\apinode\security.md
What is the recommended method for providing public keys to DSS for JWT verification?,Using a JWKs_URI,Providing static JWKs set,Using a symmetric key,Embedding keys directly in the API node,A,../data/dataiku\apinode\security.md
"Which of the following permissions allows a user to manage infrastructure settings, including managing permissions?",View,Deploy,Admin,Read,C,../data/dataiku\apinode\security.md
Which of the following is required for authentication on the admin API?,OAuth Token,API Key,Username and Password,SSH Key,B,../data/dataiku\apinode\api\admin-api.md
What is the format of the request body for POST and PUT requests in the API Node administration API?,XML,HTML,JSON,Plain Text,C,../data/dataiku\apinode\api\admin-api.md
Which command-line tool can be used to manage the API node?,apinode-admin,api-node-cli,dataiku-admin,node-admin,A,../data/dataiku\apinode\api\admin-api.md
What is a common use-case for having an API Service with several endpoints in Dataiku?,To manage user authentication.,To orchestrate several prediction models.,To store large datasets.,To perform data cleaning operations.,B,../data/dataiku\apinode\api\endpoints-api.md
Which of the following is NOT required for a dispatcher endpoint to query other endpoints in Dataiku?,An API key.,The service identifier.,The port on which the API node server is running.,The database connection string.,D,../data/dataiku\apinode\api\endpoints-api.md
"In Dataiku, where must the call to `utils.get_self_client()` be placed within your function?",At the beginning of the script.,Within your function or `predict` method.,In the initialization section.,At the end of the script.,B,../data/dataiku\apinode\api\endpoints-api.md
Which of the following methods is used to predict a single record using the API Python client?,APINodeClient.predict_records(),APINodeClient.predict_record(),APINodeClient.forecast(),APINodeClient.lookup_record(),B,../data/dataiku\apinode\api\index.md
Which method would you use to create a new service using the API node administration API?,APINodeAdminClient.create_service(),APINodeAdminClient.list_services(),APINodeAdminClient.service(),APINodeAdminClient.auth(),A,../data/dataiku\apinode\api\index.md
Which of the following HTTP status codes indicates a successful request in the Dataiku API?,1xx,2xx,3xx,4xx,B,../data/dataiku\apinode\api\user-api.md
What is the required format for the request body in POST and PUT requests when using the Dataiku REST API?,XML,HTML,JSON,Plain Text,C,../data/dataiku\apinode\api\user-api.md
Which of the following methods is NOT a valid method for computing explanations in the Dataiku API?,SHAPLEY,ICE,LIME,None of the above,C,../data/dataiku\apinode\api\user-api.md
Which of the following commands is used to build a CUDA-enabled base image for API Deployer?,./bin/dssadmin build-base-image --type apideployer --with-cuda,./bin/dssadmin build-base-image --type apideployer --with-gpu,./bin/dssadmin build-base-image --type apideployer --with-nvidia,./bin/dssadmin build-base-image --type apideployer --with-tensorflow,A,../data/dataiku\apinode\kubernetes\aks.md
What is the minimum recommended memory allocation for each AKS cluster node?,4GB,8GB,16GB,32GB,B,../data/dataiku\apinode\kubernetes\aks.md
Which of the following Kubernetes versions has been tested for cluster management?,1.20,1.21,1.23,1.22,C,../data/dataiku\apinode\kubernetes\aks.md
Which command is used to build a CUDA-enabled base image for API Deployer deployments?,./bin/dssadmin build-base-image --type apideployer --with-cuda,./bin/dssadmin build-base-image --type apideployer --with-gpu,./bin/dssadmin build-base-image --type apideployer --with-nvidia,./bin/dssadmin build-base-image --type apideployer --with-tensorflow,A,../data/dataiku\apinode\kubernetes\eks.md
What is the minimum recommended memory allocation for each node in an AWS EKS cluster?,4 GB,7 GB,10 GB,12 GB,B,../data/dataiku\apinode\kubernetes\eks.md
Which of the following Kubernetes versions has cluster management been tested with?,1.20,1.21,1.23,1.30,C,../data/dataiku\apinode\kubernetes\eks.md
Which command is used to ensure that the local 'docker' command can successfully push images to the 'gcr.io' repository?,gcloud container clusters get-credentials <cluster_id>,gcloud auth configure-docker,kubectl apply -f <file.yaml>,docker push <image_name>,B,../data/dataiku\apinode\kubernetes\gke.md
What must be added to the command line to build a CUDA-enabled base image for API Deployer?,--with-cuda,--enable-gpu,--cuda-support,--gpu-enable,A,../data/dataiku\apinode\kubernetes\gke.md
Which of the following is a required step to use GPU-enabled instances for API Deployer deployments on Google Cloud Platform?,Create a cluster with GPU accelerators,Install Docker on the local machine,Use Kubernetes version 1.23,Enable API Gateway,A,../data/dataiku\apinode\kubernetes\gke.md
Which of the following is NOT a step in setting up the infrastructure for deploying on Google Kubernetes Engine (GKE)?,Create your GKE cluster,Prepare your local `docker` and `kubectl` commands,Create your ACR registry,Setup the infrastructure,C,../data/dataiku\apinode\kubernetes\index.md
What is the purpose of a Kubernetes service in the context of API Service Deployment?,To manage the deployment of multiple pods,To expose a publicly available URL for querying the API,To build the base image for deployment,To create a cluster with GPUs,B,../data/dataiku\apinode\kubernetes\index.md
Which of the following is NOT true about Minikube?,Minikube is suitable for simple experimentation.,Minikube is a supported feature.,Minikube does not have an image repository.,Minikube uses the Docker daemon running within the VM.,B,../data/dataiku\apinode\kubernetes\minikube.md
What is the recommended memory allocation for each Minikube cluster node?,2 GB,4 GB,7 GB,10 GB,C,../data/dataiku\apinode\kubernetes\minikube.md
Which of the following is NOT a prerequisite for deploying API services on Kubernetes?,An existing Kubernetes cluster,A fully functional 'kubectl' command,A MacOS installation of DSS,An accessible image registry,C,../data/dataiku\apinode\kubernetes\setup.md
What must be done after each upgrade of DSS?,Rebuild all base images,Restart the Kubernetes cluster,Reconfigure the 'kubectl' command,Reinstall the Docker command,A,../data/dataiku\apinode\kubernetes\setup.md
Which of the following is NOT applicable to SQL Query endpoints in a Kubernetes deployment?,SQL query endpoints,Dataset lookup endpoints,Query enrichments in prediction endpoints,Storage of bundled data,D,../data/dataiku\apinode\kubernetes\sql-connections.md
What must be replaced in the connection settings for Kubernetes deployments due to lack of support?,Connection names,Encrypted passwords,Dataset lookup endpoints,SQL query endpoints,B,../data/dataiku\apinode\kubernetes\sql-connections.md
Which of the following commands is used to list the available services in the DSS API node?,service-create,services-list,service-delete,service-import-generation,B,../data/dataiku\apinode\operations\cli-tool.md
Which command would you use to switch to the newest generation of a service?,service-switch-to-newest,service-switch-to-generation,service-set-mapping,service-enable,A,../data/dataiku\apinode\operations\cli-tool.md
Which of the following methods is NOT available for API node administration on Dataiku Cloud?,Using the apinode-admin tool,Using the REST Admin API,Using the Python client,Using the Dataiku Cloud Console,A,../data/dataiku\apinode\operations\cli-tool.md
Which of the following is NOT a mechanism for client-side high availability handling?,Using a load balancer,Using client-side dispatch,Using client-side retry,Using server-side fallback,D,../data/dataiku\apinode\operations\ha-deployment.md
What is a key advantage of using a load balancer for high availability?,It is cost-effective and simple to deploy,It is transparent from the clients,It requires no monitoring of API node instances,It eliminates the need for multiple API node instances,B,../data/dataiku\apinode\operations\ha-deployment.md
What is the first step in a rolling software update for an API node?,Stopping the node,Upgrading the node,Taking the node out of high availability,Uploading the new version of the package,C,../data/dataiku\apinode\operations\ha-deployment.md
What HTTP success code does the isAlive probe return if the node is considered alive?,1xx,2xx,3xx,4xx,B,../data/dataiku\apinode\operations\health-monitoring.md
How can you force the isAlive probe to indicate that the node is not alive?,Restart the server,Create a file called 'apinode-not-alive.txt' in the API node data directory,Delete the 'apinode-not-alive.txt' file from the API node data directory,Send a specific HTTP request to the /isAlive/ endpoint,B,../data/dataiku\apinode\operations\health-monitoring.md
Which of the following is a method for ensuring high availability and scalability in Dataiku?,Using the apinode-admin tool,Client-side dispatch,Logging queries to Kafka,Global isAlive probe,B,../data/dataiku\apinode\operations\index.md
What is the purpose of the 'isAlive API' in Dataiku?,To manage the list of services,To handle software updates,To monitor the status of services,To check the global health status of the node,D,../data/dataiku\apinode\operations\index.md
Which of the following loggers is generally not recommended to enable?,dku.apinode.audit.queries,dku.apinode.audit.auth,dku.apinode.audit.admin,dku.apinode.audit.allcalls,D,../data/dataiku\apinode\operations\logging-audit.md
What is the default file where administration API audit logs are written?,run/audit/audit.log,run/api-queries/queries.log,run/apimain.log,run/audit,C,../data/dataiku\apinode\operations\logging-audit.md
Which logging mechanism is used for audit and query logging in Dataiku DSS?,Apache Kafka,Java Log4J,Syslog,Fluentd,B,../data/dataiku\apinode\operations\logging-audit.md
Who can contribute to the development of an Application-as-recipe?,Any user with a Dataiku account,Only users with the Develop plugins permission,Only users that are administrators of the project,Only users with the Write project content permission,C,../data/dataiku\applications\application-as-recipe.md
What is mandatory to specify in the Recipe definition of an Application-as-recipe?,Icon,Category,Scenario,Settings,C,../data/dataiku\applications\application-as-recipe.md
Which of the following is NOT supported in an Application-as-recipe?,Partitioned inputs and outputs,Writable outputs by DSS,Python code recipes,Visual recipes like Prepare or Join,A,../data/dataiku\applications\application-as-recipe.md
Which of the following panels allows you to configure the user interface of the application instances in Dataiku?,Application header,Application features,Included content,Application instance home,D,../data/dataiku\applications\index.md
What permission is required for a user to configure project variable tiles with custom code in Dataiku?,Execute App,Develop plugins,Administrator,View dashboard,B,../data/dataiku\applications\index.md
What is the purpose of the projectRandomKey variable in Dataiku applications?,To store user credentials,To build table names for SQL datasets,To manage application permissions,To configure the application image,B,../data/dataiku\applications\index.md
Which of the following tiles allows modifying the configuration of an 'Uploaded files' dataset and automatically infers the schema?,Upload file and automatically redetect format,Upload file and automatically redetect format and infer schema,Only upload file,Go to dataset settings,B,../data/dataiku\applications\tiles.md
What is the purpose of the 'Propagate schema' tile in Dataiku?,To run a selected scenario,To view a selected dashboard,To facilitate updating recipes when input dataset columns change,To download the selected dataset,C,../data/dataiku\applications\tiles.md
Which tile interaction allows browsing and selecting a file in a 'Filesystem' dataset?,Only browse file,Go to dataset settings,Browse file and automatically redetect format,Browse file and automatically redetect format and infer schema,A,../data/dataiku\applications\tiles.md
Which cloud providers does DSS have advanced support for?,"Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)","Amazon Web Services (AWS), IBM Cloud, Google Cloud Platform (GCP)","Microsoft Azure, IBM Cloud, Google Cloud Platform (GCP)","Amazon Web Services (AWS), Microsoft Azure, Oracle Cloud",A,../data/dataiku\cloud\index.md
"In the reference architecture for DSS on AWS, which service is used for managed compute?",EKS,EC2,Lambda,Fargate,A,../data/dataiku\cloud\index.md
What is the first step in setting up DSS in the reference architecture for Azure?,Install DSS,Prepare the instance,Setup ADLS gen2 connections,Install AKS plugin,B,../data/dataiku\cloud\index.md
Which AWS service is NOT mentioned as being deeply integrated with DSS?,S3,RDS,Redshift,Lambda,D,../data/dataiku\cloud\aws\index.md
Which of the following steps is NOT part of the main steps in the reference architecture for managed compute on EKS with Glue and Athena?,Setup connectivity to AWS,Install DSS,Setup S3 connections,Setup Lambda functions,D,../data/dataiku\cloud\aws\index.md
Which AWS service is used by DSS for interactive SQL queries?,Glue,Athena,Redshift,SageMaker,B,../data/dataiku\cloud\aws\reference-architectures\eks-glue-athena.md
What is the role of EKS in the Dataiku DSS architecture?,For in-database SQL processing,"For containerized Python, R, and Spark data processing and Machine Learning",For interactive SQL queries,For managing backups,B,../data/dataiku\cloud\aws\reference-architectures\eks-glue-athena.md
Which AWS service does DSS use as a metastore?,Athena,Redshift,Glue,SageMaker,C,../data/dataiku\cloud\aws\reference-architectures\eks-glue-athena.md
Which of the following Azure services does DSS integrate with for data storage?,WASB and ADLS,AWS S3 and Google Cloud Storage,Azure Blob Storage and Google Drive,Dropbox and OneDrive,A,../data/dataiku\cloud\azure\index.md
What is one of the main steps in the reference architecture for managing compute on AKS and storage on ADLS gen2?,Setup containerized execution configuration in DSS,Install Hadoop,Configure AWS Lambda,Setup Google Cloud Functions,A,../data/dataiku\cloud\azure\index.md
Which of the following is NOT a main step in setting up the reference architecture for managing compute on AKS and storage on ADLS gen2?,Prepare the instance,Install DSS,Setup containerized execution configuration in DSS,Install Hadoop,D,../data/dataiku\cloud\azure\reference-architectures\azure-aks-adls.md
What is the purpose of setting 'your-cr.azurecr.io' as the 'Image registry URL' in the DSS setup?,To configure the Azure VM,To push base images,To install Docker CE,To setup ADLS gen2 connections,B,../data/dataiku\cloud\azure\reference-architectures\azure-aks-adls.md
Which of the following is required to create a new 'AKS connection' preset?,Azure subscription ID,Docker CE installation,Non-root user for 'dssuser',Standalone Spark binaries,A,../data/dataiku\cloud\azure\reference-architectures\azure-aks-adls.md
Which of the following GCP services does DSS integrate with for dynamically-managed clusters?,Google Cloud SQL,BigQuery,Dataproc,GCS,C,../data/dataiku\cloud\gcp\index.md
Which of the following is NOT a main step in setting up the DSS instance on GCE?,Prepare the instance,Install DSS,Setup containerized execution configuration in DSS,Install Apache Hadoop,D,../data/dataiku\cloud\gcp\reference-architectures\gke-gcs.md
What is required for the 'dssuser' to manage GKE clusters and push Docker images to Google Container Registry?,A GCP Service Account with sufficient permissions,A Kubernetes Service Account,An AWS IAM Role,A local machine with Docker installed,A,../data/dataiku\cloud\gcp\reference-architectures\gke-gcs.md
What should be set as the 'Image registry URL' when setting up containerized execution configuration in DSS?,docker.io/your-docker-project,gcr.io/your-gcp-project,registry.hub.docker.com,quay.io/your-quay-project,B,../data/dataiku\cloud\gcp\reference-architectures\gke-gcs.md
What is the purpose of preloading a bundle in Dataiku?,To scan for needed code environments and compare them to the ones available on the automation node.,To delete all existing code environments on the automation node.,To automatically activate the bundle without any further actions.,To create a backup of the current project.,A,../data/dataiku\code-envs\automation.md
Which of the following is NOT a possible behavior when faced with a missing or outdated code environment during the Preloading phase?,Do nothing.,Stop preloading if a code environment is missing or not up-to-date.,Ensure existence of an appropriate code environment by creating or updating it.,Automatically delete the project.,D,../data/dataiku\code-envs\automation.md
How are managed code environments on the design node treated when preloaded on the automation node?,They become non-managed code environments.,They become versioned code environments.,They are deleted.,They remain unchanged.,B,../data/dataiku\code-envs\automation.md
What are the two sets of packages that can be pre-installed in environments for Dataiku?,Core packages and Jupyter packages,Core packages and Visualization packages,Jupyter packages and Visualization packages,Core packages and Analysis packages,A,../data/dataiku\code-envs\base-packages.md
What is the primary reason Dataiku does not generally recommend using Conda for code environments?,Conda is not compatible with Python.,"Conda packages repositories tend to be very bleeding-edge and move quickly, with frequent backwards-incompatible changes.",Conda does not support R packages.,Conda is not supported by Dataiku at all.,B,../data/dataiku\code-envs\conda.md
What must be installed and included in the PATH of DSS to use Conda environments?,Virtualenv,Miniconda or Anaconda,Docker,Jupyter Notebook,B,../data/dataiku\code-envs\conda.md
"When managing packages in a Conda-based environment, what lists of packages need to be managed?",Only the list of Conda packages,Only the list of pip requirements,The list of Conda packages and the 'regular' list of language packages,The list of Docker images and the list of Conda packages,C,../data/dataiku\code-envs\conda.md
"Where can custom options for virtualenv, pip, R, and conda be configured in Dataiku DSS?",Administration > Settings > Misc,Administration > Settings > Security,Administration > Settings > Network,Administration > Settings > Users,A,../data/dataiku\code-envs\custom-options.md
Which of the following commands is used to add a trusted host for pip installs?,pip install --trusted-host,pip install --proxy,pip install --index-url,pip install --no-index,A,../data/dataiku\code-envs\custom-options.md
What is the purpose of the '--no-index' option in pip installs?,To add a trusted host,To apply a proxy,To use a custom python package repository,To install from a local directory without scanning remote package indexes,D,../data/dataiku\code-envs\custom-options.md
What is a code environment in DSS?,A shared environment for all users to run code.,A standalone and self-contained environment to run Python or R code.,An environment that only supports Python 3.9.,A cloud-based environment for running machine learning models.,B,../data/dataiku\code-envs\index.md
Which of the following is true about code environments in DSS?,All code environments must use the same version of Python.,Code environments are dependent on each other.,Each code environment can have its own set of packages.,You cannot install different versions of packages in different environments.,C,../data/dataiku\code-envs\index.md
Which of the following is NOT a required step to create a non-managed code environment in DSS?,Go to Administration > Code envs,Create a new Python or R environment,Select 'Managed path' as environment type,Give an identifier to your code environment,C,../data/dataiku\code-envs\non-managed.md
What must be present in the location of a non-managed Python code environment for DSS?,bin/python and bin/pip,bin/R and bin/pip,bin/python and bin/R,bin/R and bin/Rscript,A,../data/dataiku\code-envs\non-managed.md
Which of the following Python versions is NOT compatible with Visual Deep Learning code environments in DSS?,3.6,3.7,3.11,3.10,C,../data/dataiku\code-envs\operations-python.md
What must be installed on your system to use a specific version of Python in DSS?,Python development headers packages,Python runtime environment,Python libraries,Python scripts,A,../data/dataiku\code-envs\operations-python.md
Which of the following is a typical use case for the resources directory in a managed code environment?,Storing user data,Downloading heavy pre-trained deep learning models,Saving temporary files,Backing up code environments,B,../data/dataiku\code-envs\operations-python.md
What is the recommended method for managing R packages in a code environment to benefit from pre-compiled packages?,Using CRAN,Using Conda,Using GitHub,Using Bioconductor,B,../data/dataiku\code-envs\operations-r.md
Where can you override the default CRAN mirror for R packages in Dataiku?,Administration > Code envs,Administration > Settings > Misc,Administration > Users,Administration > Projects,B,../data/dataiku\code-envs\operations-r.md
Which of the following permissions is advisory only in the context of Dataiku code environments?,Owner,Usable by all,Use,Update / Manage,C,../data/dataiku\code-envs\permissions.md
What are the two types of code environments for plugins in DSS?,Managed and semi-managed,Managed and non-managed,Non-managed and semi-managed,Automatic and manual,B,../data/dataiku\code-envs\plugins.md
What is the preferred mode for defining the requirements of a plugin in DSS?,Using a code environment definition,Using a manual installation process,Using a semi-managed environment,Using a predefined package,A,../data/dataiku\code-envs\plugins.md
What should a user do after installing a plugin that contains a code environment definition?,Restart the DSS instance,Create a code environment for the plugin,Manually install all required packages,Update the plugin configuration,B,../data/dataiku\code-envs\plugins.md
Which of the following commands would you use to install Python development headers on a Debian/Ubuntu system for Python 3.7?,sudo yum install python3.7-devel,sudo apt-get install python3.7-dev,sudo zypper install python3.7-devel,sudo yum install python3-devel,B,../data/dataiku\code-envs\troubleshooting.md
What is the likely cause of a 'Python.h not found' error during package installation?,Missing system development headers,Incorrect Python version,Insufficient user permissions,Network connectivity issues,A,../data/dataiku\code-envs\troubleshooting.md
What should you install on some Ubuntu systems if you encounter 'No module named distutils.spawn' error?,python3.7-dev,python3-distutils,python3-devel,build-essential,B,../data/dataiku\code-envs\troubleshooting.md
Which of the following packages is required to produce PDF documents from R Markdown reports in DSS?,rmarkdown,pandoc,adjustbox,knitr,C,../data/dataiku\code-reports\rmarkdown.md
What is the recommended method for running Python blocks in R Markdown reports in DSS?,Using the built-in DSS Python environment,Creating a Python code environment in DSS,Using the default R environment,Running Python scripts separately and importing results,B,../data/dataiku\code-reports\rmarkdown.md
Where are Project libraries available by default in the Code Studio?,/home/dataiku/workspace/project-lib-versioned,/home/dataiku/workspace/project-lib-resources,/home/dataiku/workspace/recipes,/home/dataiku/workspace/notebooks,A,../data/dataiku\code-studios\code-studio-operations.md
Which type of files are specific to each Code Studio and are not shared between Code Studios?,Project libraries,Project resources,Code Studio versioned files,User config files,C,../data/dataiku\code-studios\code-studio-operations.md
What is the recommended location for large files in a Code Studio?,Project libraries,Project resources,Code Studio versioned files,User config files,B,../data/dataiku\code-studios\code-studio-operations.md
Which of the following is a requirement for using Code Studios in Dataiku?,Having a containerized execution config.,Using Dataiku Cloud Stacks.,Having a cluster defined.,All of the above.,D,../data/dataiku\code-studios\code-studio-requirements.md
Which of the following blocks is used to add the VS Code IDE in the Code Studio?,JupyterLab server,RStudio,Visual Studio Code,Streamlit,C,../data/dataiku\code-studios\code-studio-templates.md
What is the purpose of the 'Add an entry point' block in Code Studio templates?,To add predefined files inside the code-studio-specific file zones,To define the actual entry point of the container and start HTTP servers,To add arbitrary Dockerfile statements to the image,To control advanced settings for Kubernetes deployment,B,../data/dataiku\code-studios\code-studio-templates.md
Which block allows you to add arbitrary Dockerfile statements to the image in a Code Studio template?,Add custom action,Append to DockerFile,Add starter files,NGINX,B,../data/dataiku\code-studios\code-studio-templates.md
Which of the following is NOT a framework natively managed in DSS for web applications?,Flask,Bokeh,Streamlit,Dash,C,../data/dataiku\code-studios\code-studios-as-webapps.md
What is the purpose of the 'Append to Dockerfile' block in the Code Studio template?,To install necessary software onto the image,To define the entrypoint for the webapp,To expose a port for the webapp,To add starter files to the template,A,../data/dataiku\code-studios\code-studios-as-webapps.md
What is the default exposition method for webapps in DSS?,Direct IP access,Port forwarding,Load balancing,Reverse proxy,B,../data/dataiku\code-studios\code-studios-as-webapps.md
Which of the following is NOT a capability made possible by Code Studios in Dataiku?,Editing and debugging Python recipes in Visual Studio Code,Developing custom web applications using the Streamlit framework,Editing and debugging R recipes in RStudio Server,Accessing the DSS host filesystem directly,D,../data/dataiku\code-studios\concepts.md
What is required for a DSS administrator to set up before users can run a Code Studio?,A Code Studio template,A dedicated server,A specific user account,A separate database,A,../data/dataiku\code-studios\concepts.md
"In Dataiku Cloud, who needs to activate the 'Code-Studio' feature in the Launchpad?",A regular user,A space-admin,A system administrator,A project manager,B,../data/dataiku\code-studios\concepts.md
Which of the following IDEs can be used for editing and debugging Python recipes in Code Studios?,Visual Studio Code,JupyterLab,RStudio Server,Streamlit,A,../data/dataiku\code-studios\index.md
Which framework is mentioned for developing custom web applications in Code Studios?,Gradio,Voila,Streamlit,JupyterLab,C,../data/dataiku\code-studios\index.md
What is the purpose of the 'Sync files with DSS' button in a Code Studio?,To start the Code Studio.,To create a new Code Studio template.,To synchronize modified files in the container back to the DSS server’s filesystem.,To set a default cluster for the project.,C,../data/dataiku\code-studios\running.md
Which of the following steps is NOT required to create a JupyterLab Code Studio template?,Click on 'Create Code Studio template' in 'Administration > Code Studios',Add a 'Jupyterlab server' block in the 'Definition' tab,Grant access to DSS groups in the 'General' tab,Install the 'ripgrep' package,D,../data/dataiku\code-studios\code-studio-ides\jupyterlab.md
What is the purpose of the 'jupyter-resource-usage' extension in JupyterLab?,To enable search and replace functionality,To monitor resource usage of current notebooks,To synchronize files with DSS,To edit project libraries,B,../data/dataiku\code-studios\code-studio-ides\jupyterlab.md
What must be done to ensure work done in a JupyterLab Code Studio is not lost when the instance is stopped?,Install the 'ripgrep' package,Use the 'Sync files with DSS' button,Rebuild the JupyterLab template,Grant access to all DSS groups,B,../data/dataiku\code-studios\code-studio-ides\jupyterlab.md
What is the purpose of the 'Sync files with DSS' button in the RStudio Code Studio?,To start a new RStudio server instance.,To synchronize modified files in the container back to the DSS server’s filesystem.,To create a new Code Studio template.,To grant permission to use RStudio runtimes to DSS groups.,B,../data/dataiku\code-studios\code-studio-ides\rstudio.md
Which of the following steps is NOT required to launch a Code Studio instance?,Ensure the project is associated with a cluster.,Click 'New Code Studio' and select the 'rstudio-template' template.,Edit the project libraries in the RStudio file explorer.,Click 'Start' once in the Code Studio.,C,../data/dataiku\code-studios\code-studio-ides\rstudio.md
What is the purpose of the 'Sync files with DSS' button in a Code Studio instance?,To start the Visual Studio Code server instance.,To synchronize modified files in the container back to the DSS server’s filesystem.,To create a new Code Studio template.,To grant permission to specific DSS groups.,B,../data/dataiku\code-studios\code-studio-ides\vs-code.md
Which folder in the Visual Studio Code file explorer contains all the files from your project Libraries?,recipes,project-lib-versioned,project-settings,code-studio,B,../data/dataiku\code-studios\code-studio-ides\vs-code.md
Which of the following steps is NOT involved in creating a Code Studio template for Gradio?,Clicking 'Create Code Studio template' in 'Administration > Code Studios',Adding a block and selecting 'Gradio' in the 'Definition' tab,Adding a block and selecting 'Visual Studio Code' in the 'Definition' tab,Publishing the Code Studio template directly from the 'Permissions' tab,D,../data/dataiku\code-studios\code-studio-webapps\gradio.md
Where is the starter file for the Gradio webapp located when launching a Code Studio instance?,code_studio-versioned/gradio/app.py,code_studio/gradio/app.py,gradio/code_studio/app.py,app.py/gradio/code_studio,A,../data/dataiku\code-studios\code-studio-webapps\gradio.md
What action should be taken to persist changes upon Code Studio restart?,Click 'Build' in the 'Definition' tab,Click 'Sync files with DSS' in the VS Code tab,Click 'Publish' in the action panel,Click 'Create' in the action panel,B,../data/dataiku\code-studios\code-studio-webapps\gradio.md
Which of the following steps is NOT involved in creating a Code Studio template for Streamlit?,Clicking 'Create Code Studio template' in 'Administration > Code Studios',Adding a block and selecting 'Streamlit' in the 'Definition' tab,Adding a block and selecting 'Visual Studio Code' in the 'Definition' tab,Publishing the Code Studio template directly from the 'Permissions' tab,D,../data/dataiku\code-studios\code-studio-webapps\streamlit.md
Where can you edit the webapp when launching a Code Studio instance?,From the 'Streamlit' tab,From the 'VS Code' tab,From the 'Permissions' tab,From the 'Definition' tab,B,../data/dataiku\code-studios\code-studio-webapps\streamlit.md
What is the purpose of the 'Sync files with DSS' button in the VS Code tab?,To start the Code Studio,To visualize and interact with the webapp,To persist changes upon Code Studio restart,To publish the Code Studio as a webapp,C,../data/dataiku\code-studios\code-studio-webapps\streamlit.md
What is the primary function of the Voila Python package?,Convert a Jupyter Notebook into a static HTML file.,Convert a Jupyter Notebook into an interactive dashboard.,Convert a Jupyter Notebook into a PDF document.,Convert a Jupyter Notebook into a Python script.,B,../data/dataiku\code-studios\code-studio-webapps\voila.md
Which of the following steps is NOT part of creating a Code Studio template named 'voila-template'?,Click on 'Create Code Studio template' in 'Administration > Code Studios'.,Add a block and select 'Voila' in the 'Definition' tab.,Add a block and select 'JupyterLab Server' in the 'Definition' tab.,Publish the template directly as a webapp.,D,../data/dataiku\code-studios\code-studio-webapps\voila.md
What is the purpose of the 'Sync files with DSS' option in the Jupyter Lab tab?,To synchronize the notebook with an external database.,To persist changes upon Code Studio restart.,To share the notebook with other users.,To convert the notebook into a webapp.,B,../data/dataiku\code-studios\code-studio-webapps\voila.md
Which of the following steps is necessary to enable the AI Explain feature in Dataiku DSS?,Go to Administration > Settings > OTHER > AI Services and check 'Enable AI Explain',Go to the Launchpad > Extensions > AI Services and accept the 'Dataiku AI Services Terms of Use',Select the 'Explain' tab in the left-hand side pane on the code recipe screen,Click 'Settings' to adjust the generated explanations,A,../data/dataiku\code_recipes\code-explanations.md
What can be adjusted in the explanation options after clicking 'Settings'?,"Language, Purpose, Length","Language, Audience, Detail","Purpose, Audience, Length","Language, Detail, Purpose",A,../data/dataiku\code_recipes\code-explanations.md
Which of the following actions is NOT a way to create a code recipe in DSS?,"From the Flow, by clicking on the New recipe button","From the Flow, in the actions menu of the dataset",From the Actions menu while being on a dataset,"From the Dashboard, by clicking on the New recipe button",D,../data/dataiku\code_recipes\common.md
What is the first step when creating a code recipe in DSS?,Select the output dataset(s) or folders,Select the input dataset(s),Write the code,Validate the recipe,B,../data/dataiku\code_recipes\common.md
What happens when you click the 'Run' button in a code recipe?,The recipe is validated,The output dataset is created,A new job is started,The code is autofilled with starter code,C,../data/dataiku\code_recipes\common.md
What is the purpose of the 'Validate' button when writing a Hive recipe in Data Science Studio?,To execute the Hive query immediately.,To perform a comprehensive validation of the script.,To save the Hive recipe.,To delete the Hive recipe.,B,../data/dataiku\code_recipes\hive.md
Which of the following is a prerequisite for writing Hive recipes in Data Science Studio?,Ensuring DSS and Hadoop are properly configured together.,Creating a new managed dataset.,Writing a temporary table.,Selecting the input datasets.,A,../data/dataiku\code_recipes\hive.md
What happens to the previous content of the dataset when you run a Hive query in Data Science Studio?,It is appended to the new results.,It is archived.,It is erased and replaced with the new results.,It is moved to a different location.,C,../data/dataiku\code_recipes\hive.md
What is the primary function of Impala in a Hadoop cluster?,To manage HDFS file storage.,To run Impala SQL queries with performance gains over Hive.,To handle data streaming in real-time.,To perform data visualization.,B,../data/dataiku\code_recipes\impala.md
Which option should be used if you want to avoid Impala's limitations when writing query output back to HDFS?,Append mode,Stream mode,Copy partitioning,Validate mode,B,../data/dataiku\code_recipes\impala.md
What does the 'Validate' button do when writing an Impala recipe?,It runs the query and stores the results.,"It performs a comprehensive validation of the script, including syntax errors and schema compatibility.",It copies the partitioning of the input dataset.,It compresses the output dataset.,B,../data/dataiku\code_recipes\impala.md
Which of the following is NOT a type of recipe available in DSS for executing user-defined code?,Python recipes,R recipes,Java recipes,Shell recipes,C,../data/dataiku\code_recipes\index.md
Which of the following is NOT a reason why the PySpark icon might be greyed out in Dataiku DSS?,Spark is not installed.,You don’t have write access on the project.,You don’t have the proper user profile.,The dataset is too large.,D,../data/dataiku\code_recipes\pyspark.md
What is the first step in creating a PySpark recipe in Dataiku DSS?,Write your Spark code in Python.,Create a PySpark recipe by clicking the corresponding icon.,Add the input Datasets and/or Folders.,Select or create the output Datasets and/or Folder.,B,../data/dataiku\code_recipes\pyspark.md
Which method is recommended for most use cases when manipulating datasets in Dataiku DSS?,Using regular Python code to iterate on the rows of the input datasets,Using Pandas dataframes,Using SQL queries,Using HDFS commands,B,../data/dataiku\code_recipes\python.md
What is the purpose of the `dataiku.Dataset.write_with_schema()` method?,To read a dataset into a Pandas DataFrame,To write a Pandas DataFrame to an output dataset,To iterate over the rows of a dataset,To modify the schema of an input dataset,B,../data/dataiku\code_recipes\python.md
What is a critical consideration when using Pandas to manipulate datasets in Dataiku DSS?,The dataset must be stored in SQL,The dataset must fit in the memory of the DSS server or container,The dataset must be in CSV format,The dataset must be partitioned,B,../data/dataiku\code_recipes\python.md
Which method is used to load the content of a dataset into a native R dataframe in Dataiku?,load.dataset(),read.dataset(),import.dataset(),fetch.dataset(),B,../data/dataiku\code_recipes\r.md
What is the purpose of the method write.dataset_schema() in Dataiku's R API?,To read a dataset into a dataframe,To write a dataframe into a dataset,To set the schema of the output dataset,To delete a dataset,C,../data/dataiku\code_recipes\r.md
Which of the following is NOT a reason why the Spark-Scala icon might be greyed out in Dataiku?,Spark is not installed.,You don't have write access on the project.,The dataset schema is not defined.,You don't have the proper user profile.,C,../data/dataiku\code_recipes\scala.md
What is the first step in creating a Spark-Scala recipe in Dataiku?,Write your Spark code in Scala.,Add the input Datasets and/or Folders.,Create a new Spark-Scala recipe through a dataset’s Actions menu or in +Recipe > Hadoop & Spark > Spark-Scala.,Select or create the output Datasets and/or Folder.,C,../data/dataiku\code_recipes\scala.md
Which of the following environment variables would contain the identifier of the second input to a shell recipe in DSS?,DKU_INPUT_0_DATASET_ID,DKU_INPUT_1_DATASET_ID,DKU_OUTPUT_0_DATASET_ID,DKU_OUTPUT_1_DATASET_ID,B,../data/dataiku\code_recipes\shell.md
What format is used for piping datasets in and out of a shell script in DSS?,JSON,XML,TAB-SEPARATED CSV,PIPE-SEPARATED CSV,C,../data/dataiku\code_recipes\shell.md
Which of the following is NOT a reason why the SPARKR icon might be greyed out?,Spark is not installed.,You don’t have write access on the project.,You don’t have the proper user profile.,The dataset is too large.,D,../data/dataiku\code_recipes\sparkr.md
What is a limitation of the sparklyr API mentioned in the document?,It cannot be used on Cloudera.,It has limited support for SQL queries.,It does not support DataFrames.,It cannot connect to Spark clusters.,A,../data/dataiku\code_recipes\sparkr.md
What is the default API mode when creating a new Spark / R recipe in DSS?,sparklyr,SparkSQL,SparkR,PySpark,C,../data/dataiku\code_recipes\sparkr.md
What is a SparkSQL recipe in DSS primarily used for?,Writing Python scripts,Creating machine learning models,Writing a SparkSQL query to populate an output dataset,Managing user permissions,C,../data/dataiku\code_recipes\sparksql.md
What could be a reason for the SparkSQL icon being greyed out in DSS?,The user does not have read access to the project,Spark is not installed,The dataset is too large,The output schema is not defined,B,../data/dataiku\code_recipes\sparksql.md
What happens when you validate a SparkSQL recipe in DSS?,The recipe is executed immediately,The syntax is verified and the output schema is computed,The input datasets are deleted,The global metastore is updated,B,../data/dataiku\code_recipes\sparksql.md
Which of the following is a limitation of the SQL query recipe in DSS?,It cannot handle multiple statements.,It cannot manage CREATE/DROP statements automatically.,It cannot handle certain advanced SQL constructs like CTE ('WITH').,It cannot infer the schema of the output dataset.,C,../data/dataiku\code_recipes\sql.md
In which scenario would you prefer to use a SQL script recipe over a SQL query recipe?,When you need to write a simple SELECT statement.,"When you need to handle data types not natively supported by DSS, like Geometry datatypes.",When you want DSS to automatically manage CREATE/DROP statements.,When you want to validate your query without executing it.,B,../data/dataiku\code_recipes\sql.md
What does the 'Validate' button do in a SQL query recipe?,Executes the query and displays the first rows.,Checks the query for syntax and grammar errors and fetches the names and types of columns created by the query.,Runs the entire SQL script.,Creates and drops the output tables automatically.,B,../data/dataiku\code_recipes\sql.md
Which of the following objects in DSS can have discussions associated with them?,"Projects, Datasets, Recipes","Notebooks, Models, Dashboards",Workspaces and workspace objects,All of the above,D,../data/dataiku\collaboration\discussions.md
Where can you access discussions for a specific object in DSS?,From the object's page by clicking the discussions button,From the Flow or list of objects by selecting the object and clicking the 'Discussions tab',From the workspace and workspace objects by clicking the 'Discussions tab',All of the above,D,../data/dataiku\collaboration\discussions.md
What is required to receive email notifications for discussions in DSS?,A valid email address registered in your account,An SMTP 'messaging channel' set as the 'Notification emails' channel,Enabling notification options in your Profile page,All of the above,D,../data/dataiku\collaboration\discussions.md
Which of the following is NOT a benefit of using version control in DSS?,Traceability into all actions performed in DSS,The ability to understand the history of each object,The ability to revert changes,Automatic optimization of code performance,D,../data/dataiku\collaboration\git.md
What is required for DSS to interact with SSH-based remote repositories?,A passwordless SSH key or use of SSH agent/Kerberos authentication,A valid API key,A specific GitHub token,A Docker container,A,../data/dataiku\collaboration\git.md
What should you do if you encounter an 'Unknown Host Key' error when pushing to a remote repository?,Restart the DSS server,Run a single 'ssh' or remote git command to the origin to get and verify the SSH host key,Reinstall the Git client,Update the DSS software,B,../data/dataiku\collaboration\git.md
What is the purpose of importing a Git repository into DSS project libraries?,To create a backup of the project,"To use external code in DSS capabilities like recipes, notebooks, and webapps",To manage user permissions,To monitor project performance,B,../data/dataiku\collaboration\import-code-from-git.md
Which action allows you to push local changes from DSS to Git?,Right-click on the library and select 'Commit and push...',Click 'Git' > 'Import from Git...',Right-click on the root path and select 'Reset from remote HEAD',Click 'Git' > 'Unlink reference',A,../data/dataiku\collaboration\import-code-from-git.md
What happens when you select 'Reset from remote HEAD'?,Local changes are committed and pushed to Git,The repository is deleted,Local changes are lost and the repository is reset to the remote HEAD,A new branch is created,C,../data/dataiku\collaboration\import-code-from-git.md
Which of the following steps is NOT involved in importing a new Jupyter Notebook from a Git repository into a DSS project?,Go to the project’s Notebook list,Click New Notebook > Import from Git,Enter the URL of the Git repository,Click on the button Commit and push,D,../data/dataiku\collaboration\import-notebooks-from-git.md
What action should you take if you want to save your local modifications of a Notebook back into the remote repository?,Click on List Notebooks,Click on the button Commit and push,Click on the button Pull,Click New Notebook > Import from Git,B,../data/dataiku\collaboration\import-notebooks-from-git.md
What should you do if a conflict is detected when pulling the latest modifications from your remote git in your local Notebook?,Override the local file on pull,Override the remote file on push,Solve the conflict outside of DSS,All of the above,D,../data/dataiku\collaboration\import-notebooks-from-git.md
Which of the following is NOT a type of request that can be made in Dataiku DSS?,Request to access a project,Request to execute an application,Request to create a new dataset,Request to install or update a plugin,C,../data/dataiku\collaboration\index.md
"In the context of version control in Dataiku DSS, what can you revert to a previous revision?",A project,An object,A single commit,All of the above,D,../data/dataiku\collaboration\index.md
Which of the following text fields in DSS support Markdown?,Short and long descriptions of any DSS object,Wiki articles of a project,Discussions on any DSS object,All of the above,D,../data/dataiku\collaboration\markdown.md
What is the purpose of Markdown in DSS?,To write code snippets,"To prettify text by allowing the use of pictures, formatted text, tables, lists, etc.",To create complex machine learning models,To manage datasets,B,../data/dataiku\collaboration\markdown.md
Which type of request allows you to request project-level permissions on a project with 'access requests' enabled?,Request to execute an application,Request to share an object,Request to access a project,Request to install or update a plugin,C,../data/dataiku\collaboration\requests.md
Who receives the requests to install or update a plugin in Dataiku DSS?,Project owner,All users with Admin permissions on the instance,All users with Manage shared objects permissions,Application owner,B,../data/dataiku\collaboration\requests.md
Where can all requests made from DSS be found and managed?,Project's security section,Notification panel,Requests section of the recipient’s Inbox,Administration > Settings > Other > Misc > Access & requests,C,../data/dataiku\collaboration\requests.md
Who can create tags on the fly in DSS projects?,Only DSS administrators,Users with 'Write Project content' permission,Project viewers,Only Project administrators,B,../data/dataiku\collaboration\tags.md
What is the purpose of global tag categories in DSS?,To allow users to create tags freely,To enforce consistency and avoid misspelled tags,To categorize projects by their completion status,To manage user permissions,B,../data/dataiku\collaboration\tags.md
Which of the following actions can be performed from the project’s Version Control page in DSS?,Reverting a project to a specific revision,Viewing the history of all commits,Creating and switching branches,All of the above,D,../data/dataiku\collaboration\version-control.md
What is a potential risk of reverting a single object to a previous revision in DSS?,It may cause Git conflicts,It will delete all data in the project,It will automatically merge branches,It will disable version control,A,../data/dataiku\collaboration\version-control.md
What is the recommended approach when working with branches in DSS?,Switch the current project to the branch,Duplicate the current project to a new project that will be initialized on the branch,Disable version control,Use a single project for all branches,B,../data/dataiku\collaboration\version-control.md
Which of the following is NOT a use of the Wiki in a DSS project?,To document the project’s goals,To document the project’s inputs and outputs,To document the inner workings of the project,To manage user permissions,D,../data/dataiku\collaboration\wiki.md
What is the purpose of promoting a Wiki in a DSS project?,To make it available in the DSS-wide wikis list,To enable automatic exports to PDF,To allow direct linking to DSS objects,To switch between folder and article layouts,A,../data/dataiku\collaboration\wiki.md
Which of the following options is NOT available when exporting a Wiki article to PDF?,Export the entire wiki of the project,Export the current article and its children,Export the current article only,Export linked DSS items,D,../data/dataiku\collaboration\wiki.md
Which of the following is NOT a type of recipe supported by Dataiku DSS?,Executing a SQL query,Executing a Python script,Executing a JavaScript script,Executing a Hive query,C,../data/dataiku\concepts\index.md
What is the primary purpose of partitioning in Dataiku DSS?,To split the dataset along meaningful dimensions,To create duplicate records,To remove the need for recipes,To eliminate the need for managed datasets,A,../data/dataiku\concepts\index.md
Which of the following is an example of a managed dataset in Dataiku DSS?,A SQL table created by Data Science Studio,A collection in MongoDB,A folder with data files on your server,A folder with data files on a Hadoop cluster,A,../data/dataiku\concepts\index.md
What permissions can be granted to folders in Dataiku?,"Read, Write, and Execute","Read, Write, and Admin","Read, Execute, and Admin","Write, Execute, and Admin",B,../data/dataiku\concepts\homepage\project-folders.md
How can you move a project or folder in Dataiku?,By dragging it into another folder,By clicking the ellipsis and selecting 'Move to',"By holding Shift and clicking each project, then clicking the ellipsis",All of the above,D,../data/dataiku\concepts\homepage\project-folders.md
What happens to the permissions of a folder created within another folder in Dataiku?,It inherits the permissions of the root folder,It inherits the permissions of the parent folder,It has no permissions until manually set,It inherits the permissions of the first project added to it,B,../data/dataiku\concepts\homepage\project-folders.md
What is one of the benefits of using project creation macros for administrators?,Reducing the need for user permissions.,Allowing users to create projects with administrator-controlled code.,Eliminating the need for a default code environment.,Increasing the administrative burden.,B,../data/dataiku\concepts\projects\creating-through-macros.md
What must administrators create to use project creation macros?,A new user role.,A dev plugin.,A new database.,A new code environment.,B,../data/dataiku\concepts\projects\creating-through-macros.md
Which of the following is a technical requirement for copying a Dataiku project?,Admin rights on the Dataiku instance,Dataiku user with the Create projects global permission,Read-only access to the project,No specific permissions are required,B,../data/dataiku\concepts\projects\duplicate.md
What must be unique when duplicating a project on the current Dataiku instance?,Project name,Project ID,Database connection,User permissions,B,../data/dataiku\concepts\projects\duplicate.md
Which of the following actions can be performed from the Actions menu in a Dataiku DSS project?,Duplicate a project,Export a project to a ZIP file,Delete a project,All of the above,D,../data/dataiku\concepts\projects\index.md
What does a Dataiku DSS project contain?,A single Flow showing the pipeline of datasets and recipes,Multiple Flows for different datasets,Only the datasets,Only the recipes,A,../data/dataiku\concepts\projects\index.md
Which of the following is NOT a limitation of Azure Blob as a filesystem-like storage?,Keys must not start with a '/',Files with names containing '/' are not supported,Folders (prefixes) '.' and '..' are supported,Multiple successive '/' are not supported,C,../data/dataiku\connecting\azure-blob.md
When should you use per-user credentials for connecting DSS to Azure Blob Storage?,When DSS users don’t have direct access to the resources in Azure,When you don’t need resource access filtering per user,When DSS users have access to your Azure project and especially to Azure Blob storage,When you don’t want an audit in Azure of your users accesses,C,../data/dataiku\connecting\azure-blob.md
Which of the following is a mandatory configuration parameter when defining a connection to a Cassandra cluster in Data Science Studio?,User and password,Hosts,Protocol version,Read timeout,B,../data/dataiku\connecting\cassandra.md
What is the default TCP port on which the Cassandra server listens for native transport queries?,8080,9042,3306,5432,B,../data/dataiku\connecting\cassandra.md
Which of the following is NOT a valid DSS type for a Cassandra column type?,String,Boolean,Integer,Character,D,../data/dataiku\connecting\cassandra.md
What is the primary risk associated with enabling the option to clear non-managed datasets in DSS?,Data duplication,Unrecoverable data loss,Increased processing time,Data corruption,B,../data/dataiku\connecting\clearing-nonmanaged.md
What is the recommended workflow to avoid risking the integrity of underlying data sources when dealing with non-managed datasets in DSS?,Directly modify the non-managed dataset,Create a managed dataset based off the non-managed input dataset,Disable all data clearing options,Use only external filesystem stores,B,../data/dataiku\connecting\clearing-nonmanaged.md
Which of the following connectors in DSS does not support writing data?,HTTP,Google BigQuery,Amazon Redshift,PostgreSQL,A,../data/dataiku\connecting\connections.md
Which file format can DSS read but not write?,"Delimited values (CSV, TSV, …)",Avro,Fixed width,Parquet,C,../data/dataiku\connecting\connections.md
What is the monthly data transfer limit out of Dataiku Cloud for all editions?,500 GB,"1,000 GB","2,000 GB",Unlimited,B,../data/dataiku\connecting\data-transfer-cloud.md
Where can you browse existing DSS plugins?,From the 'New dataset' menus or pages.,From Administration > Plugins.,From the Dataiku homepage.,From the DSS dashboard.,B,../data/dataiku\connecting\dataset-plugins.md
What happens when you install a DSS plugin that contains a new dataset kind?,It appears in the 'New dataset' menus or pages.,It needs to be manually added to the dataset list.,It replaces the existing dataset kinds.,It requires a system restart to be available.,A,../data/dataiku\connecting\dataset-plugins.md
In which programming language can you write your own DSS plugin to connect to a new kind of data source?,Java,C++,Python,Ruby,C,../data/dataiku\connecting\dataset-plugins.md
Which of the following statements about Editable Datasets in Dataiku is true?,Editable Datasets can be edited from the dataiku Python API.,"Editable Datasets are limited to 50,000 rows.",Editable Datasets can only be modified from the UI.,Editable Datasets cannot be created from other datasets.,C,../data/dataiku\connecting\editable-datasets.md
Which of the following Elasticsearch versions are supported by Data Science Studio for reading and writing datasets?,1.x to 4.x,5.0 to 8.4,6.0 to 9.0,2.x to 7.x,B,../data/dataiku\connecting\elasticsearch.md
What is the default port parameter for Elasticsearch’s HTTP API?,9300,9200,9400,9000,B,../data/dataiku\connecting\elasticsearch.md
Which of the following privileges are required for DSS to use Elasticsearch?,monitor at the cluster level,"read, view_index_metadata, monitor on indices",create and manage on indices,All of the above,D,../data/dataiku\connecting\elasticsearch.md
What is a managed folder in DSS?,A folder that contains only text files.,A generic object in the DSS flow that contains files.,A folder that can only be read by Python recipes.,A folder that is used exclusively for storing images.,B,../data/dataiku\connecting\files-in-folder.md
Which action should be used to recreate a dataset from a managed folder?,Create Dataset,Create Folder,Create Recipe,Create Table,A,../data/dataiku\connecting\files-in-folder.md
What should be used instead of a 'Files in folder' dataset when the folder is on HDFS and needs to be used as a Hive table?,A normal HDFS dataset pointing at the same location as the folder.,A managed folder.,A Python recipe.,An R recipe.,A,../data/dataiku\connecting\files-in-folder.md
Who can create and manage Filesystem connections in DSS?,Any user,Project managers,DSS administrators,Data scientists,C,../data/dataiku\connecting\filesystem.md
What is the first step to create a filesystem dataset in DSS?,Click on New dataset > Server filesystem,Select the connection in which your files are located,Click on 'Browse' to locate your files,Create a new project,A,../data/dataiku\connecting\filesystem.md
Which of the following is a mandatory parameter when creating an FTP connection in Data Science Studio?,User,Password,Host,Path,C,../data/dataiku\connecting\ftp.md
What is the default behavior of the DSS Download recipe when its output folder is rebuilt?,It does not check the FTP server for updates.,It checks the FTP server for updates.,It deletes the existing data in the output folder.,It creates a new connection to the FTP server.,B,../data/dataiku\connecting\ftp.md
Which of the following is NOT a supported case for writing data to an FTP dataset in Data Science Studio?,Writing data in multiple files within a folder.,Writing data to a pre-created empty file.,Writing data to a file without creating it beforehand.,Writing a managed dataset in a directory.,C,../data/dataiku\connecting\ftp.md
Which of the following is NOT a limitation of Google Cloud Storage when used as a filesystem-like storage?,Keys must not start with a '/',Files with names containing '/' are not supported,Folders (prefixes) '.' and '..' are not supported,Multiple successive '/' are supported,D,../data/dataiku\connecting\gcs.md
When should you use a service account for connecting DSS to Google Cloud Storage?,When your DSS users have direct access to the resources in GCP,When you need resources access filtering per user,When you want an audit in GCP of your users' accesses,When your DSS users don’t have direct access to the resources in GCP,D,../data/dataiku\connecting\gcs.md
What is required to configure a connection to Google Cloud Storage using a service account?,OAuth2 client ID and secret,Project ID and service account private key in JSON format,User credentials for each DSS user,Google Cloud Storage XML API enabled,B,../data/dataiku\connecting\gcs.md
Which of the following is NOT a reason to use a service account for connecting Dataiku to Google Sheets?,Your Dataiku users don’t have direct access to the resources in Google Sheets,You don’t need resources access filtering per user,Your Dataiku users have access to your Google Sheets resources,You need to share documents with the service account’s email address,C,../data/dataiku\connecting\googlesheets.md
What is the correct format for exporting a private key when creating a service account for Dataiku?,XML,CSV,JSON,TXT,C,../data/dataiku\connecting\googlesheets.md
"When using OAuth2 to connect Dataiku to Google Sheets, what is the application type that should be selected?",Desktop application,Web application,Mobile application,Service application,B,../data/dataiku\connecting\googlesheets.md
Which of the following steps is NOT part of creating a cached version of an HTTP source using the download recipe in DSS?,"Click on 'New dataset', then 'Network > HTTP (with cache)'","Enter the HTTP, HTTPS or FTP URL, and click on Check",Create a 'Files in folder' dataset based on the previous output managed folder,Manually create a partitioned download recipe,D,../data/dataiku\connecting\http-cached.md
What is the primary purpose of the Download recipe in DSS?,To directly read data stored on HTTP or HTTPS servers,To create a cached version of a HTTP source,To manage partitioned cases automatically,To parse data automatically,B,../data/dataiku\connecting\http-cached.md
Which of the following statements is true about creating a HTTP dataset in DSS?,You can use wildcard expansion patterns in HTTP/HTTPS URLs.,HTTP datasets can be used as both input and output in DSS.,"You need to enter the URL(s) to download, one per line.",HTTP datasets are always cached by default.,C,../data/dataiku\connecting\http.md
What is a key difference when partitioning a HTTP dataset compared to other file-based datasets?,HTTP datasets can be partitioned by specifying folder patterns.,HTTP datasets are enumerable.,"Expansion variables like %Y, %M, %D, %H are not available for HTTP datasets.",HTTP datasets are partitioned by including expansion variables in the URL.,D,../data/dataiku\connecting\http.md
Which of the following is NOT a type of dataset mentioned in the context of Dataiku?,SQL databases,Amazon S3,Google Sheets,Microsoft Excel,D,../data/dataiku\connecting\index.md
Which of the following is a requirement for writing data into Snowflake?,OAuth2 Authentication,Advanced JDBC properties,Cloud storage connection,Custom JDBC URL,C,../data/dataiku\connecting\index.md
Which of the following is a supported connection type in Dataiku?,FTP,Google Drive,Dropbox,OneDrive,A,../data/dataiku\connecting\index.md
Which of the following types of information is NOT included in the internal stats dataset of Dataiku DSS?,Cluster tasks,Commits (internal Git),Jobs,User login history,D,../data/dataiku\connecting\internal-stats.md
What can you restrict the internal stats dataset to when retrieving statistics for cluster tasks?,Project key,Connection,User role,Job type,B,../data/dataiku\connecting\internal-stats.md
Which of the following steps is NOT required to create a kdb+ connection in DSS?,Create a new 'Other databases (JDBC)' connection,Use 'kx.jdbc.jdbc' in driver class,Use '<jdbc:q:HOST:PORT>' in URL,Install the 'kdb+' plugin,D,../data/dataiku\connecting\kdbplus.md
Which of the following is NOT a supported connection type for creating a managed folder in DSS?,HDFS,S3,FTP,SQL,D,../data/dataiku\connecting\managed_folders.md
What is the primary purpose of managed folders in DSS?,To store and manipulate structured data,To store and manipulate unstructured data,To create visual recipes,To manage user permissions,B,../data/dataiku\connecting\managed_folders.md
Which of the following is a correct method to read a file from a managed folder using the Dataiku Python API?,handle.read_file('myinputfile.txt'),handle.get_file('myinputfile.txt'),handle.get_download_stream('myinputfile.txt'),handle.download_file('myinputfile.txt'),C,../data/dataiku\connecting\managed_folders.md
What type of data does a metrics dataset in Dataiku DSS contain?,Raw data from external sources.,Computed metrics or the outcome of checks.,User activity logs.,Configuration settings.,B,../data/dataiku\connecting\metrics.md
Which of the following scopes can a metrics dataset in Dataiku DSS cover?,Metrics for a specified object in the project.,All objects within the project.,Project-level metrics for the project.,All of the above.,D,../data/dataiku\connecting\metrics.md
Which type of ordering stores the rows physically in the same order as they are pushed into the dataset?,Read-time ordering,Write ordering,Batch ordering,Stream ordering,B,../data/dataiku\connecting\ordering.md
Which type of ordering is compatible with most SQL databases?,Write ordering,Batch ordering,Read-time ordering,Stream ordering,C,../data/dataiku\connecting\ordering.md
Which of the following is NOT a type of dataset that can be produced using the Attribute search Dataset feature?,A list of assets paths,A list of assets and metrics,A list of event frames,A list of metrics only,D,../data/dataiku\connecting\pi-system.md
What is required for the PI System servers to be accessible?,PIWebAPI enabled,SSL certificate,NTLM authentication,Basic authentication,A,../data/dataiku\connecting\pi-system.md
Which of the following data types returns the interpolated values across the specified time range with a chosen sampling interval?,Recorded,Plot,Interpolated,Summary,C,../data/dataiku\connecting\pi-system.md
What is the main instrument in ensuring the relocatability of managed datasets in DSS?,Using unique dataset names,Using variables within storage settings,Using different connections for each dataset,Using default settings,B,../data/dataiku\connecting\relocation.md
Which of the following is NOT a property of relocatable managed datasets?,Storage settings don’t overlap if two datasets with the same name are created in different projects.,Storage settings don’t overlap if a project is duplicated within a DSS instance.,Storage settings don’t overlap if a project is imported into an existing DSS instance.,Storage settings are automatically updated for existing datasets when relocatability settings are changed.,D,../data/dataiku\connecting\relocation.md
"In the context of HDFS datasets, what can be configured using variables in the connection settings?",Table name prefix and suffix,Path prefix and suffix,Hive database,All of the above,D,../data/dataiku\connecting\relocation.md
Which of the following permissions is NOT required to write data to an S3 bucket?,s3:PutObject,s3:DeleteObject,s3:ListAllMyBuckets,s3:AbortMultipartUpload,C,../data/dataiku\connecting\s3.md
What is the purpose of the 'bucket-owner-full-control' canned ACL in DSS?,To allow public access to the bucket,To transfer ownership of uploaded data to the bucket owner,To enable server-side encryption,To restrict access to specific IP addresses,B,../data/dataiku\connecting\s3.md
"In 'path restriction mode', what must be specified in the connection settings?",Bucket name and path within the bucket,AWS keypair,Server-side encryption mode,Default path for managed datasets,A,../data/dataiku\connecting\s3.md
Which of the following is NOT a mandatory parameter when defining an SCP/SFTP connection in DSS?,Host,User,Password,Key passphrase,D,../data/dataiku\connecting\scp-sftp.md
What is the purpose of using the DSS Download recipe with SCP/SFTP servers?,To improve performance by caching contents from the server,To encrypt data during transfer,To automatically update the server with new data,To create new datasets on the server,A,../data/dataiku\connecting\scp-sftp.md
Which of the following is NOT required to create a SharePoint Online connection in Dataiku?,Tenant ID,App ID,App Secret,User's Email Address,D,../data/dataiku\connecting\sharepoint-online.md
What is the correct OAuth2 redirect URL format for Dataiku if accessed at https://dss.mycompany.corp/?,https://dss.mycompany.corp/dip/api/oauth2-callback,https://dss.mycompany.corp/oauth2-callback,https://dss.mycompany.corp/dip/api/callback,https://dss.mycompany.corp/api/oauth2-callback,A,../data/dataiku\connecting\sharepoint-online.md
Where are files stored by default when uploaded to DSS?,In the 'uploads' folder within the DSS data directory,In the 'temporary' folder within the DSS data directory,In the 'datasets' folder within the DSS data directory,In the 'files' folder within the DSS data directory,A,../data/dataiku\connecting\upload.md
What should you do if you encounter a '413 error' while uploading files to DSS?,Restart the DSS server,Increase the upload size limitations in your browser settings,Ask your IT administrator to increase the upload size limitations or use alternate ways to get your data to the DSS server,Delete some files to free up space,C,../data/dataiku\connecting\upload.md
Which of the following is required to use the Excel Templater plugin in Dataiku?,A pre-built Excel template,A Python script,A SQL database,A Jupyter notebook,A,../data/dataiku\connecting\excel\excel-templater.md
How can you reference a dataset shared from another project in an Excel template using the Excel Templater plugin?,DATASET.<my dataset in a flow>,DATASET.<my other project>.<my shared dataset>,DATASET.<my shared dataset>,DATASET.<my project>.<my dataset>,B,../data/dataiku\connecting\excel\excel-templater.md
Which of the following is NOT a feature of the Avro file format?,"Compact, fast, binary data format","Rich data structures like map, union, array, record, and enum",Schema is stored along with the data,Supports Avro Date type natively,D,../data/dataiku\connecting\formats\avro.md
How does Data Science Studio handle the Avro 'union' type when reading files?,It converts all union types to string,It replaces the union type with the most specific type able to represent all unified types,It does not support union types,It converts union types to integer,B,../data/dataiku\connecting\formats\avro.md
What must be done when dealing with a large partitioned dataset composed of many Avro files of different versions?,Convert all files to a single version,Manually define the schema in the dataset’s settings,Use only the latest version of the Avro files,Automatically merge all schemas,B,../data/dataiku\connecting\formats\avro.md
Which of the following quoting and escaping styles is also known as RFC4180?,Excel-style,Unix-style,Escaping only,"No escaping, no quoting",A,../data/dataiku\connecting\formats\csv.md
In which quoting and escaping style is the double quote not a special character?,Excel-style,Unix-style,Escaping only,"No escaping, no quoting",C,../data/dataiku\connecting\formats\csv.md
Which quoting and escaping style is recommended to use a very exotic separator like u0001?,Excel-style,Unix-style,Escaping only,"No escaping, no quoting",D,../data/dataiku\connecting\formats\csv.md
Which of the following storage options is NOT mentioned as a place where Delta Lake datasets can be stored?,S3,Azure Blob Storage,Google Cloud Storage,Local Disk,D,../data/dataiku\connecting\formats\deltalake.md
What is required for Delta Lake support to appear as a file format in Dataiku?,Python integration,Spark integration,SQL integration,Hadoop integration,B,../data/dataiku\connecting\formats\deltalake.md
Which of the following file formats is known for its case-sensitivity issues?,CSV,Parquet,Avro,Excel,B,../data/dataiku\connecting\formats\index.md
Which file format uses XPath to select data?,JSON,CSV,XML,Parquet,C,../data/dataiku\connecting\formats\index.md
Which file format is associated with the Vecmath library?,ESRI Shapefiles,Delta Lake,Hive ORCFile,Fixed width,A,../data/dataiku\connecting\formats\index.md
Which of the following statements about JSON data handling in Data Science Studio is correct?,Data Science Studio can handle both concatenated JSON objects and arrays of JSON objects directly in the dataset settings.,Data Science Studio can only handle concatenated JSON objects.,Data Science Studio can flatten arrays while controlling the number of unflattened objects created.,Data Science Studio cannot flatten arrays upon ingesting the data.,C,../data/dataiku\connecting\formats\json.md
What is the output format when flattening arrays with a limit of 2 on the index of flattened elements in Data Science Studio?,A single JSON object with nested arrays.,A table with columns representing the flattened elements.,A concatenated JSON object.,An array of JSON objects.,B,../data/dataiku\connecting\formats\json.md
Which of the following Hive data types is NOT supported by Data Science Studio when reading or writing ORCFiles?,OBJECT,MAP,DATE,ARRAY,C,../data/dataiku\connecting\formats\orcfile.md
Which of the following is a limitation of the ORCFile format?,"It cannot store complex types like object, map, and array.",It can only be used on Hadoop filesystems.,It was added in Hive 0.11.,It supports all Hive data types.,B,../data/dataiku\connecting\formats\orcfile.md
Which of the following is NOT a main point of the Parquet file format?,"Column-oriented, even for nested complex types",Block-based compression,Ability to 'push down' filtering predicates to avoid useless reads,Row-oriented data storage,D,../data/dataiku\connecting\formats\parquet.md
Which of the following cloud storage and Hadoop connections can Parquet datasets be stored on?,"HDFS, S3, GCS, Azure Blob storage","HDFS, S3, GCS, Azure Data Lake","HDFS, S3, GCS, Azure SQL Database","HDFS, S3, GCS, Azure Data Factory",A,../data/dataiku\connecting\formats\parquet.md
What is strongly recommended when dealing with Parquet files to avoid issues with Hive?,Using uppercase identifiers,Using mixed-case identifiers,Using lowercase identifiers,Using numeric identifiers,C,../data/dataiku\connecting\formats\parquet.md
Which library is required for handling some projections in data preparation when working with ESRI Shapefiles in DSS?,Vecmath,MathLib,GeoMath,ProjLib,A,../data/dataiku\connecting\formats\shapefile.md
Where should the Vecmath jar file be placed after downloading for DSS to use it?,DSS_DATA_DIR/lib/jdbc,DSS_DATA_DIR/lib/projections,DSS_DATA_DIR/lib/shapefiles,DSS_DATA_DIR/lib/math,A,../data/dataiku\connecting\formats\shapefile.md
What is the minimum version of Vecmath required to open some Shapefiles in DSS?,1.3.1,1.4.0,1.5.2,1.6.0,A,../data/dataiku\connecting\formats\shapefile.md
Which of the following is a limitation of using XPath in Data Science Studio?,It supports all downward operations from the root elements.,It allows operations accessing siblings of the root elements.,It supports downward operations from a parent of the root elements.,It allows only a portion of the document to reside in memory at any given time.,B,../data/dataiku\connecting\formats\xml.md
"When creating a dataset from an XML source in Data Science Studio, what is the goal regarding the source data?",To load the entire XML document into the dataset.,To trim the source data and only load the part that will be used in analyses.,To convert all XML data into JSON format.,To load all parent elements of the root elements.,B,../data/dataiku\connecting\formats\xml.md
How does Data Science Studio represent data from an XML source that cannot be reduced to simple values upon import?,As plain text.,As XML attributes.,As JSON objects.,As CSV files.,C,../data/dataiku\connecting\formats\xml.md
Which versions of MongoDB are fully supported by Data Science Studio?,Up to v5.0,Up to v6.0,Up to v6.1,Up to v7.0,B,../data/dataiku\connecting\nosql\mongo.md
What is the default port for MongoDB connections?,8080,3306,27017,5432,C,../data/dataiku\connecting\nosql\mongo.md
Which of the following is NOT a feature supported by DSS on Google AlloyDB for PostgreSQL?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,Running machine learning models directly,D,../data/dataiku\connecting\sql\alloydb.md
What is the recommended setup for secure connections to an AlloyDB for PostgreSQL server?,Using SSL without certificate validation,Using SSL with certificate validation,Using a non-secure connection,Using a custom encryption method,B,../data/dataiku\connecting\sql\alloydb.md
Which of the following geospatial types are supported by DSS for PostGIS integration?,geopoint and geoline,geopoint and geometry,geometry and geoline,geography and geoline,B,../data/dataiku\connecting\sql\alloydb.md
Which of the following is NOT supported when using AWS Athena with DSS?,Running interactive SQL notebooks on Athena based on previously-built S3 datasets.,Using Athena as charts engine for S3 datasets.,Running SQL queries on Athena based on previously-built S3 datasets.,Writing data directly to Athena.,D,../data/dataiku\connecting\sql\athena.md
What is the primary purpose of integrating Athena in DSS?,To serve as a standalone SQL database.,To query S3 datasets built in DSS.,To write data directly to Athena.,To replace Amazon S3.,B,../data/dataiku\connecting\sql\athena.md
What is the recommended credentials mode for an S3 connection when setting up an Athena connection in DSS?,AWS Keypair or STS with AssumeRole.,IAM User.,Access Key and Secret Key.,None of the above.,A,../data/dataiku\connecting\sql\athena.md
Which of the following is NOT a limitation of the built-in driver for connecting to BigQuery?,The built-in driver waits for queries completion without any configurable time out.,Some complex NESTED fields are not read correctly using the Simba driver.,Creation of BigQuery tables partitioned by ingestion time.,SQL script recipes.,B,../data/dataiku\connecting\sql\bigquery.md
Which of the following is a benefit of using the built-in driver for connecting to BigQuery?,You can configure labels to be applied to BigQuery jobs which helps monitor costs.,The built-in driver incurs billing from Google for dataset preview and explore.,The built-in driver does not support viewing the cost of a query after its execution.,The built-in driver requires additional setup.,A,../data/dataiku\connecting\sql\bigquery.md
When should you use OAuth2 for connecting to BigQuery?,When your DSS users don’t have direct access to the resources in GCP.,When you don’t need resources access filtering per user.,When you want an audit in GCP of your users accesses.,When you want DSS to access all resources associated with a service account.,C,../data/dataiku\connecting\sql\bigquery.md
Which of the following is NOT a requirement for using the Databricks Connect integration in Dataiku?,A Databricks cluster with a runtime in version at least 13.0,A Databricks connection using personal access token credentials,A Dataiku Code Env based on Python 3.10,OAuth authentication for the Databricks connection,D,../data/dataiku\connecting\sql\databricks.md
What is the recommended way to load data into a Databricks table for optimal performance?,Using regular SQL INSERT statements,Using regular SQL COPY statements,Using a bulk COPY from files stored in Amazon S3 or Azure Blob Storage,Using JDBC for direct data loading,C,../data/dataiku\connecting\sql\databricks.md
Which of the following is a necessary step when setting up OAuth for a Databricks connection with per-user credentials?,Creating a service principal in Databricks,Generating an OAuth App in Databricks,Using a Databricks cluster with a runtime in version at least 13.0,Setting up a Dataiku Code Env based on Python 3.10,B,../data/dataiku\connecting\sql\databricks.md
Which of the following is NOT a feature supported by DSS on IBM DB2?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,Creating a dedicated 'DB2' connection,D,../data/dataiku\connecting\sql\db2.md
What is the JDBC Driver class required to create a DB2 connection in DSS?,com.ibm.db2.jcc.DB2Driver,org.postgresql.Driver,com.mysql.jdbc.Driver,oracle.jdbc.OracleDriver,A,../data/dataiku\connecting\sql\db2.md
Which of the following features is NOT supported by DSS on Exasol?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,Parallel writes to the same table,D,../data/dataiku\connecting\sql\exasol.md
What is a limitation when using Exasol with DSS?,You cannot read datasets,You cannot execute SQL recipes,Parallel writes to the same table are not supported,You cannot use live engine for charts,C,../data/dataiku\connecting\sql\exasol.md
Which of the following distribution strategies in Greenplum uses the first column of the table for distribution?,Auto,Specify distribution columns,Distribute randomly,Manual,A,../data/dataiku\connecting\sql\greenplum.md
Where can you override the 'Table creation SQL statement' to set Greenplum 'DISTRIBUTE BY' and 'SORT BY' clauses in DSS?,Connection settings,Dataset settings,Advanced tab in dataset settings,Greenplum settings,C,../data/dataiku\connecting\sql\greenplum.md
Which of the following is NOT a capability of DSS when working with SQL databases?,Create datasets representing SQL tables,Execute Visual recipes directly in-database,Execute Charts directly in-database,Create datasets using the results of a Python script,D,../data/dataiku\connecting\sql\index.md
Which of the following databases is not fully supported by DSS?,Snowflake,AWS Athena,Google BigQuery,Amazon Redshift,B,../data/dataiku\connecting\sql\introduction.md
What should you do before trying to connect to a database in DSS?,Install the proper JDBC driver,Create a new project,Set up a custom JDBC URL,Enable advanced JDBC properties,A,../data/dataiku\connecting\sql\introduction.md
What is the purpose of the 'fetch size' parameter in DSS?,To set the maximum number of connections,To determine the size of data batches fetched from the database,To specify the database URL,To enable advanced JDBC properties,B,../data/dataiku\connecting\sql\introduction.md
Which of the following features is NOT supported by DSS on MySQL?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,Running machine learning models,D,../data/dataiku\connecting\sql\mysql.md
What is the key used to enable SSL in the MySQL connection settings?,useSSL,enableSSL,SSL,secureConnection,A,../data/dataiku\connecting\sql\mysql.md
Which of the following is a necessary step to connect DSS to a MySQL server using secure connections?,Install the MySQL server on the same machine as DSS,Add the root certificate of the authority which issued the MySQL server certificate to the JVM truststore,Disable SSL in the MySQL server settings,Use a non-secure connection for faster performance,B,../data/dataiku\connecting\sql\mysql.md
Which of the following features is NOT supported by DSS on IBM Netezza?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,SQL Pipelines,D,../data/dataiku\connecting\sql\netezza.md
Which of the following is NOT a feature supported by DSS on Oracle and Exadata?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,Running machine learning models directly on Oracle,D,../data/dataiku\connecting\sql\oracle.md
What is required for DSS to connect to an Oracle database using Kerberos authentication?,Kerberos 5 authentication enabled on the database,Kerberos client configuration file correctly configured on the DSS host,Create a Kerberos account for DSS on the KDC,All of the above,D,../data/dataiku\connecting\sql\oracle.md
What is the purpose of the 'Impersonated user' field in the Oracle DSS connection configuration?,To define the Oracle account name to impersonate for data manipulation and SQL queries,To set the password for the Oracle account,To specify the database schema to use,To enable SSL encryption for the connection,A,../data/dataiku\connecting\sql\oracle.md
Which of the following PostgreSQL versions is NOT supported by DSS?,9.0,9.4,10.0,8.0,D,../data/dataiku\connecting\sql\postgresql.md
What is required to compute column statistics metrics on PostgreSQL datasets?,PostgreSQL version 8 or greater,PostgreSQL version 9 or greater,PostgreSQL version 9.4 or greater,PostgreSQL version 10 or greater,C,../data/dataiku\connecting\sql\postgresql.md
Which of the following is a widely used PostgreSQL database extension that allows storing and processing geospatial data?,PostGIS,GeoPost,SpatialDB,GeoSQL,A,../data/dataiku\connecting\sql\postgresql.md
Which of the following is NOT a limitation when setting up Dataiku Cloud with Amazon Redshift?,Using an IAM role for connecting is not supported,Reading external tables (also known as Redshift Spectrum) is not supported,"For read-write connections, multiple output schemas can be selected",The fast data load from S3 to Redshift ignores the timezone information of dates,C,../data/dataiku\connecting\sql\redshift.md
What is required for the automatic fast-write method to work when writing data into Redshift?,The S3 bucket and the Redshift cluster must be in the same Amazon AWS region,The S3 bucket must be in a different AWS region than the Redshift cluster,The data must be stored in JSON format,The data must be stored in XML format,A,../data/dataiku\connecting\sql\redshift.md
Which JDBC driver is required for reading external tables in Redshift?,PostgreSQL driver,MySQL driver,Dedicated Redshift driver,Oracle driver,C,../data/dataiku\connecting\sql\redshift.md
Which of the following features is NOT supported by DSS when using SAP HANA?,Reading and writing datasets,Executing SQL recipes,Filtering by date in 'free range' mode in charts (live mode),Performing visual recipes in-database,C,../data/dataiku\connecting\sql\saphana.md
Which of the following is a requirement for using the 'Snowflake to Cloud' sync engine?,The source dataset must be stored on Amazon S3.,The destination dataset must be stored on Snowflake.,The cloud storage and the Snowflake cluster must be in the same cloud region.,The source dataset must be stored in a JSON format.,C,../data/dataiku\connecting\sql\snowflake.md
What is a limitation of the Snowflake Connector for Spark?,"It only supports Spark versions 2.2, 2.3, and 2.4.",It does not support reading from Snowflake datasets.,It only supports writing to Snowflake datasets with column names that contain periods.,It does not support any version of Spark.,A,../data/dataiku\connecting\sql\snowflake.md
Which of the following is NOT supported by the 'Sample/Filter' visual recipe in Snowflake?,Aggregating on booleans,Cumulative average,Contains operator,Unix CSV quoting style,C,../data/dataiku\connecting\sql\snowflake.md
Which of the following is required for DSS to connect to SQL Server using Kerberos authentication?,OAuth token,Kerberos client configuration file,SQL Server version 7.2 or higher,Azure Active Directory,B,../data/dataiku\connecting\sql\sqlserver.md
What is a necessary step to perform before DSS can login as an app using OAuth on Azure SQL Server?,Create a new SQLServer connection,Create a client secret for the application,Enable 'Login with Kerberos',Add the app as a user in the Azure SQL Server database,D,../data/dataiku\connecting\sql\sqlserver.md
Which of the following features is supported by DSS on Microsoft SQL Server?,Reading and writing datasets,Performing visual recipes in-database,Using live engine for charts,All of the above,D,../data/dataiku\connecting\sql\sqlserver.md
Which of the following is the recommended method for loading data into an Azure Synapse table?,Using regular SQL 'INSERT' statements,Using a bulk COPY from files stored in Azure Blob Storage,Using a direct JDBC connection,Using a REST API,B,../data/dataiku\connecting\sql\synapse.md
What is required for unloading data from Synapse to Azure Blob using the sync recipe?,The source dataset must be stored on Azure Synapse,The destination dataset must be stored on Azure Blob,The Azure Blob bucket and the Synapse cluster must be in the same Azure region,All of the above,D,../data/dataiku\connecting\sql\synapse.md
What is the minimum version of the JDBC driver required for OAuth login to Azure Synapse?,Version 6.0,Version 7.0,Version 7.2,Version 8.0,C,../data/dataiku\connecting\sql\synapse.md
Which of the following authentication mechanisms is NOT supported by the Teradata connector in DSS?,TD2,LDAP,OAuth,Kerberos,C,../data/dataiku\connecting\sql\teradata.md
What is the purpose of the 'SET QUERY_BAND' statement in Teradata connections?,To set the primary index of a table,To implement impersonation on Teradata,To enable autocommit mode,To install the JDBC driver,B,../data/dataiku\connecting\sql\teradata.md
Which of the following is a limitation when using Teradata with DSS?,Reading and writing datasets,Executing SQL recipes,Creating personal connections,Performing visual recipes in-database,C,../data/dataiku\connecting\sql\teradata.md
Which of the following features is NOT supported by DSS on Vertica?,Reading and writing datasets,Executing SQL recipes,Performing visual recipes in-database,Running machine learning models,D,../data/dataiku\connecting\sql\vertica.md
What should you do after copying the Vertica JDBC driver JAR file to the 'lib/jdbc' directory of DSS?,Restart DSS,Update the database schema,Reconfigure the data connection,Run a test query,A,../data/dataiku\connecting\sql\vertica.md
Which version of the Vertica JDBC driver is recommended if your database version is above 7.1?,7.1 driver,7.2 driver,Latest driver,8.0 driver,C,../data/dataiku\connecting\sql\vertica.md
Which of the following is NOT compatible with container-based execution in Dataiku?,Python recipes,Custom in-memory machine learning models,Non-managed code environments,Deep learning models,C,../data/dataiku\containers\code-envs.md
What must be done after each upgrade of DSS to ensure compatibility with container-based execution?,Rebuild all base images and code env images,Restart the DSS server,Reinstall all custom packages,Update the Python interpreter,A,../data/dataiku\containers\code-envs.md
Which of the following is NOT a benefit of running DSS processes in containers?,Improved scalability,Improved computing capabilities,Ease of resource management,Increased storage capacity,D,../data/dataiku\containers\concepts.md
What is a limitation when using libraries from plugins in containers?,Libraries from plugins are not supported in code recipes and notebooks.,Libraries from plugins can only be used in Spark activities.,Libraries from plugins require a custom base image.,Libraries from plugins can only be used with GPU-enabled training.,A,../data/dataiku\containers\concepts.md
Which of the following processes can DSS run in elastic compute clusters powered by Kubernetes?,Python and R recipes and notebooks,Initial training of in-memory machine learning models,Scoring of in-memory machine learning models when using the Optimized engine,All of the above,D,../data/dataiku\containers\concepts.md
What is the primary benefit of running visual recipes inside a Kubernetes cluster instead of on the DSS node?,It reduces the need for Spark.,It frees CPU and memory on the DSS node.,It allows the use of custom Java libraries.,It improves the accuracy of the recipes.,B,../data/dataiku\containers\containerized-dss-engine.md
Which of the following is NOT a notable case where a visual recipe is unsuitable for containerization?,A Prepare recipe uses a 'Python Function' processor.,A recipe with input or output on Hive datasets.,A recipe with input or output on SCP datasets.,A recipe with input or output on SQL databases hosted on remote servers.,D,../data/dataiku\containers\containerized-dss-engine.md
What command is used to build the base image for deploying to Kubernetes?,./bin/dssadmin build-base-image --type cde,./bin/dssadmin build-cde-plugins-image,./bin/dsscli container-exec-base-images-push,./bin/dssadmin build-k8s-image --type base,A,../data/dataiku\containers\containerized-dss-engine.md
Which command is used to build a base image with CUDA support in Dataiku?,./bin/dssadmin build-base-image --type container-exec,./bin/dssadmin build-base-image --type container-exec --with-cuda,./bin/dssadmin build-base-image --type container-exec --with-py310,./bin/dssadmin build-base-image --type container-exec --without-r,B,../data/dataiku\containers\custom-base-images.md
What should you do after each upgrade of DSS?,Rebuild all base images and update code environments,Reinstall Python and R,Delete all custom Dockerfiles,Reset all DSS configurations,A,../data/dataiku\containers\custom-base-images.md
Which option allows you to specify another DSS-provided CUDA version when building a base image?,--with-cuda,--cuda-version X.Y,--tag,--system-packages,B,../data/dataiku\containers\custom-base-images.md
Which of the following is NOT a benefit of using Kubernetes over Docker?,A native ability to run on a cluster of machines.,An ability to globally control resource usage.,A capability to auto scale for managed cloud services.,Easier setup on any recent operating system.,D,../data/dataiku\containers\docker.md
What is a prerequisite for running workloads in Docker?,Using podman as the container engine.,Having an existing Docker daemon.,Direct outgoing Internet access for the DSS machine.,Using Kubernetes for container management.,B,../data/dataiku\containers\docker.md
What must be done after each upgrade of DSS to continue deploying to Docker?,Rebuild all base images.,Reconfigure the Docker daemon.,Restart the DSS machine.,Update the Docker client.,A,../data/dataiku\containers\docker.md
What technology does DSS use to scale most of its processing?,Docker,Kubernetes,Apache Hadoop,Microsoft Azure,B,../data/dataiku\containers\index.md
Which of the following is a prerequisite for setting up Kubernetes with DSS?,Setting up a Spark cluster,Installing Docker and kubectl,Creating a new execution configuration,Building an image with CUDA support,B,../data/dataiku\containers\index.md
Which of the following is NOT a step in the initial setup of managed EKS clusters?,Install the EKS plugin,Prepare your local commands,Create a new containerized execution configuration,Enable GPU support on the cluster,D,../data/dataiku\containers\index.md
Which of the following cloud providers is NOT supported by DSS for managed Kubernetes clusters?,Amazon Web Services,Azure,Google Cloud Platform,IBM Cloud,D,../data/dataiku\containers\managed-k8s-clusters.md
What must you do first to create managed clusters in DSS?,Go to Administration > Clusters,Install the DSS plugin corresponding to your cloud provider,Fill in the required parameters,Click Start/Attach,B,../data/dataiku\containers\managed-k8s-clusters.md
What is the purpose of the 'Setup cluster' step in a scenario using a dynamic cluster?,To denote the contextual cluster to use at the project level,To create a dynamic cluster and set its identifier into a variable,To automatically stop the dynamic cluster at the end of the scenario,To use the variables expansion mechanism of DSS,B,../data/dataiku\containers\managed-k8s-clusters.md
What is the primary unit for access control and resources control in Kubernetes?,Pod,Service,Namespace,Cluster,C,../data/dataiku\containers\namespaces.md
Which of the following is NOT a way DSS can manage namespaces?,Single namespace,Multiple static namespaces,Multiple dynamic namespaces,Multiple ephemeral namespaces,D,../data/dataiku\containers\namespaces.md
What must policy elements be in order to apply a namespace policy in DSS?,JSON representations of Kubernetes quota-level objects,YAML representations of Kubernetes quota-level objects,XML representations of Kubernetes quota-level objects,HTML representations of Kubernetes quota-level objects,B,../data/dataiku\containers\namespaces.md
Which of the following is a prerequisite for running workloads in Kubernetes with Dataiku DSS?,An existing Docker daemon,A local MySQL database,A Hadoop cluster,A local Apache server,A,../data/dataiku\containers\setup-k8s.md
What must be done after each upgrade of DSS?,Reinstall Docker,Rebuild all base images,Restart the Kubernetes cluster,Reconfigure Spark settings,B,../data/dataiku\containers\setup-k8s.md
What is recommended when deploying on AWS EKS?,Disable auto-create namespace,Enable push to ECR,Use a local image registry,Disable managed Spark on K8S,B,../data/dataiku\containers\setup-k8s.md
What is a common cause for the 'requests.exceptions.ConnectionError' when running a container in DSS?,The container tries to connect to a name that cannot be resolved to an IP address.,The container is running out of memory.,The container is using an outdated version of DSS.,The container has insufficient storage space.,A,../data/dataiku\containers\troubleshooting.md
"What should you do if you encounter a 'Kubernetes job failed, exitCode=1, reason=Error' message?",Restart the Kubernetes cluster.,Check for a Python stack trace in previous log lines.,Increase the memory allocation for the container.,Update the DSS software to the latest version.,B,../data/dataiku\containers\troubleshooting.md
What configuration change is recommended if you see a 'requests.exceptions.ConnectionError' in a Spark on Kubernetes container?,Set 'spark.driver.host' to the DNS name or IP address of DSS backend.,Increase the number of Spark executors.,Update the Spark version to the latest release.,Restart the Spark cluster.,A,../data/dataiku\containers\troubleshooting.md
What is the minimum Kubernetes version required to use a single unmanaged cluster with DSS?,1.8,1.9,1.10,1.11,C,../data/dataiku\containers\unmanaged-k8s-clusters.md
Which tool does DSS leverage for Kubernetes management?,kubectl,helm,minikube,kubeadm,A,../data/dataiku\containers\unmanaged-k8s-clusters.md
Which of the following steps is NOT part of the initial setup for using managed AKS clusters?,Create your ACR registry,Install the AKS plugin,Enable GPU support on the cluster,"Prepare your local `az`, `docker`, and `kubectl` commands",C,../data/dataiku\containers\aks\index.md
What is recommended for a complete Elastic AI setup in Azure?,Reading the dedicated Azure documentation,Using unmanaged AKS clusters,Using only elastic storage,Using only elastic compute,A,../data/dataiku\containers\aks\index.md
Which command is used to log in to the Azure CLI with a service principal?,az login --service-principal --username client_id --password client_secret --tenant tenant_id,az acr login --name your-registry-name,kubectl login --service-principal --username client_id --password client_secret --tenant tenant_id,docker login --service-principal --username client_id --password client_secret --tenant tenant_id,A,../data/dataiku\containers\aks\managed.md
What must be added to the command line to build a CUDA-enabled base image?,--with-cuda,--enable-gpu,--cuda-support,--gpu-enabled,A,../data/dataiku\containers\aks\managed.md
What key-value pair should be added in the 'Custom limits' section to request 1 GPU?,gpu.com/nvidia: 1,nvidia.com/gpu: 1,cuda.com/gpu: 1,gpu.com/cuda: 1,B,../data/dataiku\containers\aks\managed.md
Which command is used to build a CUDA-enabled base image for containerized execution in Dataiku?,./bin/dssadmin build-base-image --type container-exec --with-cuda,./bin/dssadmin build-base-image --type container-exec --with-gpu,./bin/dssadmin build-base-image --type container-exec --with-nvidia,./bin/dssadmin build-base-image --type container-exec --with-tensorflow,A,../data/dataiku\containers\aks\unmanaged.md
What is the recommended minimum memory allocation for each node in an Azure Kubernetes Service (AKS) cluster?,8GB,16GB,32GB,64GB,B,../data/dataiku\containers\aks\unmanaged.md
Which Kubernetes versions have been tested for cluster management in Dataiku?,1.20 to 1.25,1.21 to 1.26,1.23 to 1.29,1.22 to 1.28,C,../data/dataiku\containers\aks\unmanaged.md
Which of the following is required to use DGX Systems with Dataiku?,Adding DGX Kubernetes cluster as a managed cluster in Dataiku.,Building an image with CUDA support.,Using only cloud-based DGX systems.,Ensuring Multi-Instance GPU (MIG) support.,B,../data/dataiku\containers\dgxsystems\index.md
What type of support does NVidia DGX have in Dataiku?,Tier 1 support,Tier 2 support,Tier 3 support,No support,B,../data/dataiku\containers\dgxsystems\index.md
Which of the following commands is necessary to build a CUDA-enabled base image for GPU support in Dataiku DSS?,./bin/dssadmin build-base-image --type container-exec --with-cuda,./bin/dssadmin build-base-image --type container-exec --with-gpu,./bin/dssadmin build-base-image --type container-exec --enable-cuda,./bin/dssadmin build-base-image --type container-exec --cuda-support,A,../data/dataiku\containers\eks\managed.md
What is the recommended way to define the connection to AWS for EKS cluster configuration in Dataiku DSS?,Provide the connection value inline in each cluster,Use the AWS credentials found by the 'aws' command in '~/.aws/credentials',Use a hardcoded value in the plugin’s settings,Manually set up the connection in the DSS host,B,../data/dataiku\containers\eks\managed.md
Which of the following is NOT a required command to prepare your local machine for using EKS with Dataiku DSS?,aws,aws-iam-authenticator,kubectl,terraform,D,../data/dataiku\containers\eks\managed.md
Which of the following commands is used to build a CUDA-enabled base image for Dataiku DSS?,./bin/dssadmin build-base-image --type container-exec --with-cuda,./bin/dssadmin build-base-image --type container-exec --with-gpu,./bin/dssadmin build-base-image --type container-exec --with-nvidia,./bin/dssadmin build-base-image --type container-exec --with-tensorflow,A,../data/dataiku\containers\eks\unmanaged.md
What is the minimum recommended memory allocation for each Amazon EKS cluster node?,10 GB,15 GB,20 GB,25 GB,B,../data/dataiku\containers\eks\unmanaged.md
Which of the following is NOT a required step to enable GPU support on an EKS cluster?,Install the NVidia Driver,Install the Cuda driver,Install the NVidia docker runtime,Install the CUDA toolkit,D,../data/dataiku\containers\eks\unmanaged.md
Which command is used to build a CUDA-enabled base image for containerized execution in Dataiku DSS?,./bin/dssadmin build-base-image --type container-exec --with-cuda,./bin/dssadmin build-base-image --type container-exec --with-gpu,./bin/dssadmin build-base-image --type container-exec --with-nvidia,./bin/dssadmin build-base-image --type container-exec --with-tensorflow,A,../data/dataiku\containers\gke\managed.md
What is the recommended way to define the connection to GCP in Dataiku DSS?,Inline in each cluster,As a preset in the 'GKE connection' plugin settings,Using a separate configuration file,Through the Dataiku DSS command line interface,B,../data/dataiku\containers\gke\managed.md
Which of the following is NOT a required command to prepare your local machine for using GKE with Dataiku DSS?,gcloud,gke-gcloud-auth-plugin,kubectl,terraform,D,../data/dataiku\containers\gke\managed.md
Which command is used to build a CUDA-enabled base image for containerized execution in Dataiku DSS?,./bin/dssadmin build-base-image --type container-exec --with-cuda,./bin/dssadmin build-base-image --type container-exec --with-gpu,./bin/dssadmin build-base-image --type container-exec --with-nvidia,./bin/dssadmin build-base-image --type container-exec --with-tensorflow,A,../data/dataiku\containers\gke\unmanaged.md
What is the recommended minimum memory allocation for each GKE cluster node when creating a GKE cluster for Dataiku DSS?,8GB,16GB,32GB,64GB,B,../data/dataiku\containers\gke\unmanaged.md
Which of the following Kubernetes versions has cluster management been tested with for Dataiku DSS?,1.20,1.21,1.23,1.30,C,../data/dataiku\containers\gke\unmanaged.md
Which of the following is NOT a type of tile that can be added to a DSS dashboard?,Simple tile,Insight tile,Filters panel tile,Interactive tile,D,../data/dataiku\dashboards\concepts.md
What is an insight in the context of a DSS dashboard?,A static text tile,A piece of information that can be shared on a dashboard,A filter applied to a dashboard,A user permission setting,B,../data/dataiku\dashboards\concepts.md
Who can modify a dashboard created by a given user?,Only the owner,People with 'Read dashboards' permission,People with 'Write Dashboard' access and DSS administrators,Anyone with access to the project,C,../data/dataiku\dashboards\concepts.md
Which of the following is NOT an option for displaying the title of a tile on a Dataiku dashboard?,Never,Permanently,Only if you hover on the tile,Only if you click on the tile,D,../data/dataiku\dashboards\display-settings.md
What happens by default when you click on a chart or dataset tile in Dataiku?,It opens the insight,It does nothing,It opens another insight,It captures all clicks,B,../data/dataiku\dashboards\display-settings.md
Which of the following is NOT a way to create dashboard exports automatically in Dataiku?,Using the 'mail reporters' mechanism in a scenario,Using a dedicated scenario step to store in a managed folder,Downloading manually from the dashboard interface,Selecting a 'dashboard export' attachment in a mail reporter,C,../data/dataiku\dashboards\exports.md
What must be done before the dashboards export feature can be used in Dataiku?,Install a third-party plugin,Enable the export feature on your DSS instance,Create a new user account,Update the software to the latest version,B,../data/dataiku\dashboards\exports.md
What is the primary purpose of applying filters to a dashboard page?,To remove all data from the dashboard,To refine insights and focus on specific data,To change the layout of the dashboard,To add new datasets to the dashboard,B,../data/dataiku\dashboards\filters.md
Which mode allows you to add and remove filters as well as configure their default states?,View mode,Edit mode,Cross-filtering mode,Filter panel mode,B,../data/dataiku\dashboards\filters.md
What happens to changes made to filters in View mode when you exit the dashboard?,They are saved automatically,They are shared with all users,They are lost,They are saved as a new dashboard,C,../data/dataiku\dashboards\filters.md
Which of the following is used to describe a deactivated filter in Dataiku's filter query parameter syntax?,NO_VALUE,OFF,HOUR_OF_DAY,DAY_OF_WEEK,B,../data/dataiku\dashboards\url-filters-query-param.md
What operator is used in Dataiku's filter query parameter syntax to specify a list of values to include?,nin,range,in,daterange,C,../data/dataiku\dashboards\url-filters-query-param.md
"In Dataiku's filter query parameter syntax, how are special characters encoded?",Using their hexadecimal ASCII code prefixed by '_x' and suffixed by '_',Using their Unicode code point prefixed by '\u',Using their HTML entity names,Using their binary code prefixed by '0b',A,../data/dataiku\dashboards\url-filters-query-param.md
Which of the following is true about publishing a chart insight from a dataset?,You need at least 'Write project content' permission.,Modifications in the insight will be reflected in the dataset chart.,You need at least 'Read project content' permission.,Charts built in a visual analysis can be published to the Dashboard.,C,../data/dataiku\dashboards\insights\chart.md
What can you modify if you have write access to the chart insight?,Only the filters,"All settings including axis, legends, and filters",Only the axis and legends,Only the displayed data,B,../data/dataiku\dashboards\insights\chart.md
What happens when you click the '+' button from the dashboard to add tiles?,You are redirected to the 'View' display of the insight.,You can only see datasets that have been dashboard-authorized.,You can only add pre-existing charts.,You can modify the chart directly in the dashboard view.,B,../data/dataiku\dashboards\insights\chart.md
Which of the following is NOT a location from which you can publish a Data Quality insight?,From the Data Quality current status page,From the dashboard,From the Data Quality rules page,From the project Data Quality page,C,../data/dataiku\dashboards\insights\data-quality.md
What does the breakdown view provide for a non-partitioned dataset?,A per-rule breakdown,A per-partition breakdown,A per-dataset breakdown,A per-project breakdown,A,../data/dataiku\dashboards\insights\data-quality.md
Which of the following is NOT a source from which you can publish a dataset table insight?,From a dataset,From the dashboard,From the model version,From the managed folder,C,../data/dataiku\dashboards\insights\index.md
What is one way to publish a Jupyter notebook insight?,From the metrics view,From the notebook,From the model version,From the scenario,B,../data/dataiku\dashboards\insights\index.md
Which of the following insights can be published from the Data Quality current status page?,Chart insight,Dataset table insight,Data Quality insight,Webapp insight,C,../data/dataiku\dashboards\insights\index.md
Which of the following permissions are required to publish a Jupyter notebook insight?,Read project content and Write safe code,Write project content and Write safe code,Read project content and Create Jupyter notebook,Write project content and Create Jupyter notebook,B,../data/dataiku\dashboards\insights\jupyter-notebook.md
What does the 'Publish' action do when performed from a Jupyter notebook?,Creates a snapshot of the notebook and sends an email notification,Creates a snapshot of the notebook and archives the previous version,"Creates a snapshot of the notebook, creates an insight pointing to it, and adds the insight to a specified dashboard",Creates a snapshot of the notebook and deletes the previous version,C,../data/dataiku\dashboards\insights\jupyter-notebook.md
Where can you publish a Jupyter notebook insight from?,Only from the notebook,Only from the dashboard,From the notebook and the dashboard,"From the notebook, the dashboard, and the project settings",C,../data/dataiku\dashboards\insights\jupyter-notebook.md
What is required to publish a managed folder insight from the managed folder?,Write project content permission,Read project content permission,Admin access,Dashboard access,B,../data/dataiku\dashboards\insights\managed-folder.md
Which of the following is NOT a method to publish a managed folder insight?,From the managed folder,From the dashboard,From the Flow,From the API node,D,../data/dataiku\dashboards\insights\managed-folder.md
What additional options does the 'View' tab of the insight offer?,Edit the insight,Delete the insight,Download options,Change permissions,C,../data/dataiku\dashboards\insights\managed-folder.md
Which of the following is a method to publish a metric insight?,From the metrics view,From the dashboard,From the project settings,From the data preparation view,A,../data/dataiku\dashboards\insights\metric.md
What is required to publish a metric insight from the metrics view?,Write project content permission,Read project content permission,Admin access,Dashboard access,B,../data/dataiku\dashboards\insights\metric.md
What can the tile display in the dashboard?,Only the current value of the metric,Only the history values,Both the current value and the history values,Only a list of values,C,../data/dataiku\dashboards\insights\metric.md
Which of the following is NOT a way to publish a model report insight?,From the model version,From the dashboard,From the Flow,From the project settings,D,../data/dataiku\dashboards\insights\model-report.md
What does the tile display of a 'model report' show?,A single report page,Multiple report pages,Only the summary page,Only the confusion matrix,A,../data/dataiku\dashboards\insights\model-report.md
What must be done to show multiple pages of a model report insight in a dashboard?,Create several tiles based on the model report insight,Edit the original model,Change the tile settings to show all pages,Use the 'View insight' option,A,../data/dataiku\dashboards\insights\model-report.md
Which permission is required to publish a scenario runs report insight from the scenario’s action menu?,Manage dashboard authorizations,Read project content,Write project content,Run scenario,B,../data/dataiku\dashboards\insights\scenario.md
What does the 'Run scenario' insight allow dashboard-only users to do?,Edit the scenario,View the scenario history,Run a scenario,Download scenario files,C,../data/dataiku\dashboards\insights\scenario.md
Which of the following is required to publish a webapp insight from the webapp's view?,Write project content permission,Read project content permission,Dashboard access,Admin access,B,../data/dataiku\dashboards\insights\webapp.md
What must be done before a webapp can be displayed in the dashboard?,It must be published from the dashboard,It must be run at least once in the webapp editor,It must be authorized by an admin,It must be saved in the project folder,B,../data/dataiku\dashboards\insights\webapp.md
How can 'standard' web apps dynamically adjust their content based on dashboard filters?,By using a special API,By listening for messages sent from the dashboard,By accessing the project settings,By modifying the webapp editor,B,../data/dataiku\dashboards\insights\webapp.md
What can you do in the Datasets & Indexed Tables section of the Data Catalog if you have relevant permissions?,Delete datasets from the Dataiku instance.,Use a dataset in your own projects or publish it to a Data Collection or the Feature Store.,Modify the schema of external databases.,Change the data steward of a dataset.,B,../data/dataiku\data-catalog\datasets-indexed-tables.md
What additional feature is available if your instance admin has indexed your external database connections?,You can delete external tables.,You can toggle to search Indexed External Tables.,You can modify external database schemas directly.,You can change the data steward of external tables.,B,../data/dataiku\data-catalog\datasets-indexed-tables.md
Which of the following categories is NOT a way to search for data in the Data Catalog?,Data Collections,Datasets & Indexed Tables,Connections Explorer,Data Pipelines,D,../data/dataiku\data-catalog\index.md
Where can you find the 'Popular Datasets' in the Data Catalog?,At the top of the homepage,In the Data Collections section,At the bottom of the homepage,In the Connections Explorer,C,../data/dataiku\data-catalog\index.md
Which of the following is NOT a condition for a dataset to be considered popular in Dataiku?,It has a recent last build date.,It has been shared with multiple projects.,It is used in at least one downstream recipe in a project it is shared with.,It has been used in at least one Dataiku instance.,D,../data/dataiku\data-catalog\popular-datasets.md
What is the default maximum number of days since the last rebuild for a dataset to be considered popular?,15,30,45,60,B,../data/dataiku\data-catalog\popular-datasets.md
Which parameter ensures that only datasets part of at least one Data Collection are considered popular?,Max # days since last rebuild,Max # days since last used by a new recipe,Only from data collections,Only trending datasets,C,../data/dataiku\data-catalog\popular-datasets.md
What can users do with datasets in a Data Collection? (Choose two.),Explore and publish datasets,Delete datasets permanently,Mark datasets as favorite and preview content,Change the schema of datasets,"A, C",../data/dataiku\data-catalog\data-collections\index.md
Who can publish datasets to a Data Collection?,Any user,Users with relevant permissions,Only administrators,Only dataset owners,B,../data/dataiku\data-catalog\data-collections\index.md
Which of the following roles can edit the settings of a Data Collection?,Admins,Contributors,Readers,All of the above,A,../data/dataiku\data-catalog\data-collections\managing.md
What permission is required for a Contributor to publish a dataset from a DSS project to a Data Collection?,Permission from the Data Collection Admin,Permission from the project owner,Permission from the instance admin,No additional permission is required,B,../data/dataiku\data-catalog\data-collections\managing.md
Which of the following actions can a Reader perform within a Data Collection?,Edit the Data Collection settings,Publish datasets to the Data Collection,Access and interact with the content of the Data Collection,Add or remove DSS users and groups from the collection,C,../data/dataiku\data-catalog\data-collections\managing.md
What happens if a user has no other access and the dataset is not an authorized object?,The user will see the dataset but with limited details.,The user will be granted full access to the dataset.,The user will not be able to see the dataset in the Data Collection.,The user will be able to preview the dataset but not export it.,C,../data/dataiku\data-catalog\data-collections\permissions-and-dataset-visibility.md
Which of the following is NOT available for a discoverable dataset?,Project name,Dataset schema,Dataset tags,Data Steward,B,../data/dataiku\data-catalog\data-collections\permissions-and-dataset-visibility.md
Which of the following authorizations is NOT required for a user to publish datasets to a Data Collection?,Publish to Data Collections global group authorization,Publish to Data Collections project-level authorization,Contributor or Admin role on the Data Collection,Read-only access to the dataset,D,../data/dataiku\data-catalog\data-collections\publish-to-data-collection.md
Who can remove datasets from a Data Collection?,Any user with read access,Any Data Collection member with the Contributor or Admin role,Only the instance admin,Only the project admin,B,../data/dataiku\data-catalog\data-collections\publish-to-data-collection.md
What is the primary purpose of a bundle in Dataiku?,To move a project with all its contents from one node to another.,To transfer the metadata and data needed to replay tasks in a production environment.,To create a backup of the project.,To share the project with external collaborators.,B,../data/dataiku\deployment\creating-bundles.md
Which of the following is NOT included in the metadata snapshot of a bundle?,Project settings,Notebooks,Actual data,Recipes,C,../data/dataiku\deployment\creating-bundles.md
What can optionally be added to a bundle in Dataiku?,Global shared code,Static datasets,User-defined meanings,Project Deployer settings,B,../data/dataiku\deployment\creating-bundles.md
What is the first step in deploying a bundle using the Project Deployer?,Publishing the bundle on the Project Deployer,Creating the bundle,Deploying the bundle to the infrastructure,Modifying deployment settings,B,../data/dataiku\deployment\deploying-bundles.md
Which of the following is NOT a setting that can be modified from the Project Deployer's Settings tab?,Project's local variables,Connection remapping,Current active bundle,Creating new scenarios,D,../data/dataiku\deployment\deploying-bundles.md
What happens if you modify a deployment setting but do not click the 'Update' button?,The deployment will be deleted,The deployment will enter an 'Out of sync' state,The deployment will be automatically updated,Nothing will happen,B,../data/dataiku\deployment\deploying-bundles.md
What is the primary function of the DSS Automation Node?,To design and build data logic.,To manage all Automation nodes.,To separate development and production environments and deploy DSS projects in production.,To handle the deployment of API services.,C,../data/dataiku\deployment\index.md
Which of the following is NOT a component of the Project Deployer?,Managing all Automation nodes,Deploying bundles to Automation nodes,Monitoring the health and status of deployed projects,Handling the deployment of API services,D,../data/dataiku\deployment\index.md
What do bundles contain when deploying projects from the Design node to the Automation node?,A given version of the flow built in the Design node,Real-time API services,User credentials and permissions,Daily reports and dashboards,A,../data/dataiku\deployment\index.md
What is the purpose of activating a bundle in Dataiku?,To delete old project data,To extract metadata and additional data to make it the current state of the project on the Automation node,To create a new project from scratch,To export project data to an external database,B,../data/dataiku\deployment\manually-importing-bundles.md
What must be ensured before importing a bundle in Dataiku?,The Automation node is turned off,The required connections are available on the Automation node,All previous bundles are deleted,The Design node is deactivated,B,../data/dataiku\deployment\manually-importing-bundles.md
What happens if there is a conflict between the current User-defined meanings and the ones in your bundle during activation?,The bundle will be activated without any issues,A fatal error will be returned,A warning will be displayed,The bundle will be deleted,C,../data/dataiku\deployment\manually-importing-bundles.md
Which of the following stages is NOT preconfigured in DSS for deployment infrastructures?,Development,Test,Production,Staging,D,../data/dataiku\deployment\project-deployment-infrastructures.md
What is the default permission propagation setting for a new project deployed to an automation node?,None,Limited,Full,Custom,A,../data/dataiku\deployment\project-deployment-infrastructures.md
Which of the following is NOT a possible action for deployment hooks in Dataiku?,Creating a ticket in a change management tool,Publishing a message on Slack/Teams,Executing a specific scenario on the deployed project,Automatically creating new user accounts,D,../data/dataiku\deployment\project-deployment-infrastructures.md
What is the primary difference between a Local Deployer and a Standalone Deployer in Dataiku?,"Local Deployer is used for single Design or Automation nodes, while Standalone Deployer is used for multiple nodes.","Local Deployer requires additional setup, while Standalone Deployer does not.","Local Deployer is used for API services only, while Standalone Deployer is used for projects only.","Local Deployer is part of Dataiku Cloud, while Standalone Deployer is part of Dataiku Custom.",A,../data/dataiku\deployment\setup.md
What must be done on the Deployer to allow Design or Automation nodes to publish their projects or API services?,Generate a user API key with read-only privileges.,Generate an admin API key with global admin privileges.,Set the Deployer mode to 'Local'.,Create a new instance of type Deployer.,B,../data/dataiku\deployment\setup.md
What is critical for the security mechanism of the Deployer in Dataiku?,Ensuring that the same users with the same logins exist on the Deployer node.,Setting the Deployer mode to 'Remote'.,Using Dataiku Cloud Stacks for AWS.,Creating a new instance of type Deployer.,A,../data/dataiku\deployment\setup.md
"In the Explore tab of your dataset, what can you do by clicking on a column and choosing 'Analyze' from the dropdown menu?",Perform data cleaning operations.,Conduct exploratory analysis.,Export the dataset.,Merge datasets.,B,../data/dataiku\explore\analyze.md
Which of the following is NOT a feature provided by the 'Explore' component in Data Science Studio (DSS)?,Quick descriptive statistics,"Meanings, i.e., rich types",Ability to sort and filter the sample,Building machine learning models,D,../data/dataiku\explore\index.md
Which sampling method in DSS involves selecting records based on a fixed number of records from each class?,First records,Random sampling (fixed number of records),Stratified (fixed number of records),Class rebalancing (approximate ratio),C,../data/dataiku\explore\index.md
Which sampling method in DSS is the fastest but can provide a biased view of the dataset?,Random sampling (fixed number of records),Stratified (fixed number of records),First records,Class rebalancing (approximate ratio),C,../data/dataiku\explore\sampling.md
What is the recommended maximum number of records for best performance in interactive exploration in DSS?,"10,000 records","50,000 records","100,000 records","200,000 records",D,../data/dataiku\explore\sampling.md
Which sampling method in DSS ensures that the distribution of values in a column is respected and may return a few more than the specified number of rows?,Random sampling (approximate ratio),Stratified (fixed number of records),Class rebalancing (approximate number of records),Last records,B,../data/dataiku\explore\sampling.md
Which of the following build modes in Dataiku DSS is the most computationally intense?,Build only this (Non recursive),Build upstream (recursive) - Build only modified,Build upstream (recursive) - Build all upstream,Build downstream (recursive) - Find outputs and build recursively,C,../data/dataiku\flow\building-datasets.md
What happens when you select 'Build only this (Non recursive)' mode in Dataiku DSS?,It builds the selected dataset and all its upstream datasets.,It builds the selected dataset using its parent recipe.,It builds all downstream datasets from the selected dataset.,It rebuilds all dependencies of the selected dataset going back to the start of the flow.,B,../data/dataiku\flow\building-datasets.md
"In Dataiku DSS, what is the default setting for models regarding retraining?",Normal,Explicit,Write-protected,Manual,B,../data/dataiku\flow\building-datasets.md
Which of the following objects is NOT included in the documentation generated by the Flow Document Generator?,Datasets,Recipes,Managed Folders,Charts,D,../data/dataiku\flow\flow-document-generator.md
What must be activated on your DSS instance to use the Flow Document Generator feature?,Graphical export library,Dataiku API,Machine learning models,Dataiku Flow,A,../data/dataiku\flow\flow-document-generator.md
Which of the following steps is necessary to enable the AI Explain feature in Dataiku DSS?,Go to Administration > Settings > OTHER > AI Services and check 'Enable AI Explain',Go to the Flow screen and select 'Explain Flow',Select a zone on the Flow screen and click 'Explain',Go to the Launchpad > Extensions > AI Services and accept the 'Dataiku AI Services Terms of Use',A,../data/dataiku\flow\flow-explanations.md
What is one of the options you can adjust when generating explanations in Dataiku DSS?,Data Source,Language,Data Format,Algorithm,B,../data/dataiku\flow\flow-explanations.md
Which of the following file types can the Flow be exported to?,"PDF, PNG, JPEG","PDF, PNG, GIF","PDF, JPEG, BMP","PDF, PNG, TIFF",A,../data/dataiku\flow\graphics-export.md
What must be done before using the Dashboards and Flow export feature?,Install a third-party plugin,Set up the export feature on your DSS instance,Update the DSS software,Enable the export feature in the user settings,B,../data/dataiku\flow\graphics-export.md
What does a tiling value of 300% do when exporting the Flow?,Zooms the Flow three times and generates up to 9 images,Zooms the Flow twice and generates up to 4 images,Zooms the Flow four times and generates up to 16 images,Zooms the Flow five times and generates up to 25 images,A,../data/dataiku\flow\graphics-export.md
What does DSS use to dynamically rebuild datasets whenever one of their parent datasets or recipes has been modified?,Flow zones,Visual grammar,Lineage,Code recipes,C,../data/dataiku\flow\index.md
Which of the following is NOT a type of recipe mentioned in the context?,Visual Recipes,Code Recipes,Plugin Recipes,Data Recipes,D,../data/dataiku\flow\index.md
What feature in DSS allows users to quickly understand a data pipeline?,Flow zones,Visual grammar,Flow explanations,Flow document generator,B,../data/dataiku\flow\index.md
What is the default number of activities that can be executed simultaneously on a DSS instance?,3,5,7,10,B,../data/dataiku\flow\limits.md
Where can administrators change the maximum number of concurrent jobs and activities on a DSS instance?,Administration > Settings > Resource control > Concurrent jobs and activities,Administration > Settings > User management,Administration > Settings > Project settings,Administration > Settings > Security,A,../data/dataiku\flow\limits.md
Which of the following is NOT a way to limit the number of activities simultaneously executed in DSS?,For a given project,For a given recipe type,For a given user,For a given dataset,D,../data/dataiku\flow\limits.md
"In DSS, which color represents processes related to machine learning?",BLUE,YELLOW,GREEN,ORANGE,C,../data/dataiku\flow\visual-grammar.md
What icon represents a Prepare recipe in DSS?,FUNNEL,BROOM,PILE OF SQUARES,BARBELL,B,../data/dataiku\flow\visual-grammar.md
Which icon represents a dataset uploaded to DSS?,TWO CUBES,ELEPHANT,UPWARD POINTING ARROW,TROPHY,C,../data/dataiku\flow\visual-grammar.md
What is the primary purpose of using flow zones in Dataiku?,To define new security boundaries.,To manage large projects by dividing them into zones.,To automatically categorize datasets.,To enhance the speed of data processing.,B,../data/dataiku\flow\zones.md
Which of the following statements is true about the default zone in Dataiku?,You can delete the default zone.,The default zone can be nested within other zones.,All nodes are initially placed in the default zone.,The default zone cannot be renamed.,C,../data/dataiku\flow\zones.md
How can you add zones in Dataiku?,By using the + Zone button at the top of the screen.,By selecting nodes and choosing 'Move to a flow zone' from the right-click menu.,By nesting zones within each other.,Both A and B.,D,../data/dataiku\flow\zones.md
Which of the following functions in Dataiku's formula language returns the length of an array?,arrayContains,arrayLen,arraySort,arrayReverse,B,../data/dataiku\formula\index.md
"In Dataiku's formula language, which function would you use to convert a string to a date?",asDate,datePart,diff,inc,A,../data/dataiku\formula\index.md
What does the 'val' function do in Dataiku's formula language?,Returns the numerical value of a column,Returns the string value of a column,Returns the value of a column with auto-detected type,Returns the length of a string,C,../data/dataiku\formula\index.md
What is Dataiku Answers primarily designed for?,Creating financial reports.,Democratizing enterprise-ready large language model chat and retrieval-augmented generation usage.,Building e-commerce websites.,Managing customer databases.,B,../data/dataiku\generative-ai\answers.md
What is required to enable the AI Code Assistant feature in Dataiku Cloud?,Being a member of the space_administrators group,Having a premium subscription,Using a self-signed certificate,Installing additional plugins,A,../data/dataiku\generative-ai\code-assistant.md
Which of the following steps is necessary to enable AI Code Assistant in Jupyter notebooks?,Install a specific plugin,Copy and execute a specific code in a cell,Restart the Jupyter server,Update the Jupyter notebook version,B,../data/dataiku\generative-ai\code-assistant.md
What might prevent the AI Code Assistant feature from being fully functional on all browsers when using Visual Studio Code?,Using a self-signed certificate,Not having admin privileges,Not enabling the feature in the Code Studio Template,Not having the latest version of Visual Studio Code,A,../data/dataiku\generative-ai\code-assistant.md
Which of the following is a prerequisite for running local HuggingFace models on Dataiku?,A running Elastic AI Kubernetes cluster with NVIDIA GPUs,A Python 2.7 setup,A fully setup Elastic AI computation capability with Ubuntu base images,A HuggingFace account with no specific model approval,A,../data/dataiku\generative-ai\huggingface-models.md
What is recommended if your containers have good outgoing Internet connectivity when creating a HuggingFace connection?,Enable DSS-managed model cache,Disable DSS-managed model cache,Use a Python 2.7 code environment,Set memory and CPU requests or limits,B,../data/dataiku\generative-ai\huggingface-models.md
Which GPU is recommended for running models such as Falcon 7B or Llama2 7B?,A100 with 80 GB of VRAM,A10 with 40 GB of VRAM,A200 with 100 GB of VRAM,A50 with 20 GB of VRAM,B,../data/dataiku\generative-ai\huggingface-models.md
Which of the following is a method for running HuggingFace models locally?,Using Hosted LLM APIs,Using the Prompt Studio,Using Locally-running HuggingFace models,Using the LLM Mesh API,C,../data/dataiku\generative-ai\index.md
What is the purpose of the AI Code Assistant in Dataiku?,To generate daily reports,To assist in writing code in Jupyter notebooks and Visual Studio Code,To detect PII in datasets,To fine-tune machine learning models,B,../data/dataiku\generative-ai\index.md
Which of the following is NOT a component of Retrieval-Augmented Generation (RAG) in Dataiku?,Concepts,Initial setup,Embedding LLMs,Model cache,D,../data/dataiku\generative-ai\index.md
Which of the following features is NOT currently available on Dataiku Cloud?,Cost monitoring,Personally Identifiable Information (PII) detection and redaction,Toxicity detection,Caching,D,../data/dataiku\generative-ai\introduction.md
What is the purpose of the LLM Mesh in Dataiku?,To provide a full-featured development environment for Prompt Engineering,To serve as the common backbone for Enterprise Generative AI Applications,To expose rich chatbots with retrieval-augmented-generation,To classify and summarize text,B,../data/dataiku\generative-ai\introduction.md
Which of the following is a feature of the Prompt Studio in Dataiku?,Audit tracing,Cost monitoring,Testing and iterating on prompts,PII detection and redaction,C,../data/dataiku\generative-ai\introduction.md
Which of the following models is NOT supported by the AWS Bedrock connection?,Anthropic Claude models,AI21 Labs Jurassic 2 models,Google BERT models,Meta Llama2 models,C,../data/dataiku\generative-ai\llm-connections.md
What is required to set up a connection to Azure OpenAI?,An Azure account with Azure OpenAI enabled,A deployed Azure OpenAI service,An Azure OpenAI API key,All of the above,D,../data/dataiku\generative-ai\llm-connections.md
Which of the following models is supported by the Databricks Mosaic AI connection?,GPT-4,MPT 30B Instruct,Claude 3,Gemini Pro,B,../data/dataiku\generative-ai\llm-connections.md
What is the purpose of DSS's model cache?,To store models from HuggingFace.,To store user data securely.,To manage database connections.,To optimize data processing speed.,A,../data/dataiku\generative-ai\model-cache.md
Which of the following is NOT a required component when manually creating a model archive for DSS?,A root folder,A folder named 'model' containing the HuggingFace model repo content,A file named 'model_metadata.json',A file named 'model_config.json',D,../data/dataiku\generative-ai\model-cache.md
What is the recommended method for importing a model into DSS?,Manually creating an archive,Importing a model previously exported by DSS,Downloading directly from the internet,Using a third-party software,B,../data/dataiku\generative-ai\model-cache.md
Which of the following is required for fine-tuning a pre-trained model in the LLM Mesh?,A dataset with a prompt column and a completion column without missing values.,A dataset with a prompt column and a system message column without missing values.,A dataset with a completion column and a system message column without missing values.,A dataset with a prompt column and a completion column with missing values.,A,../data/dataiku\generative-ai\model-fine-tuning.md
What is strongly advised to use when fine-tuning Huggingface models?,A CPU,A GPU,A TPU,A VPU,B,../data/dataiku\generative-ai\model-fine-tuning.md
Which add-on is required to access the LLM fine-tuning recipe?,Basic LLM Mesh,Intermediate LLM Mesh,Advanced LLM Mesh,Premium LLM Mesh,C,../data/dataiku\generative-ai\model-fine-tuning.md
Which of the following providers support multimodal input with images in the LLM Mesh?,OpenAI (GPT-4o),Google Vertex (Imagen 1 and Imagen 2),Stability AI (Stable Diffusion 3.0),Bedrock Titan Image Generator,A,../data/dataiku\generative-ai\multimodal.md
Which of the following providers support image generation in the LLM Mesh?,OpenAI (DALL-E 3),Azure OpenAI (GPT-4o),Vertex Gemini Pro,Bedrock Claude 3,A,../data/dataiku\generative-ai\multimodal.md
In which phase is the image output feature currently available?,General Availability,Beta,Private Preview,Public Preview,C,../data/dataiku\generative-ai\multimodal.md
Which of the following packages is NOT required to set up the PII detection code environment?,presidio_anonymizer,presidio_analyzer,langdetect,numpy,D,../data/dataiku\generative-ai\pii-detection.md
Which of the following is NOT a recognized generic entity type for PII detection?,CREDIT_CARD,EMAIL_ADDRESS,US_SSN,IP_ADDRESS,C,../data/dataiku\generative-ai\pii-detection.md
What is the purpose of the 'Replace PII by a placeholder' option in PII detection?,To reject queries where PII is detected,To replace PII with a placeholder like '<PERSON>',To replace PII with a hash value,To remove PII entirely,B,../data/dataiku\generative-ai\pii-detection.md
What is the primary purpose of the Prompt Studio in Dataiku?,To develop and test machine learning models.,To create and iterate on prompts for large-scale batch generation.,To manage databases and data connections.,To visualize data through dashboards.,B,../data/dataiku\generative-ai\prompt-studio.md
Which feature is NOT mentioned as part of the Prompt Studio's capabilities?,Testing and iterating on prompts.,Comparing various LLMs.,Deploying prompts as Prompt Recipes.,Creating data visualizations.,D,../data/dataiku\generative-ai\prompt-studio.md
What is the primary purpose of Retrieval-Augmented Generation (RAG) in Dataiku?,To generate random text,To give standard LLMs the knowledge of a specific business problem,To create visualizations,To manage databases,B,../data/dataiku\generative-ai\rag.md
Which of the following is NOT a supported connection type for embedding LLMs in Dataiku?,OpenAI,Azure OpenAI,Google Cloud AI,AWS Bedrock,C,../data/dataiku\generative-ai\rag.md
What is the default Vector Store used for creating Knowledge Banks in Dataiku?,Chroma,FAISS,Pinecone,SQLite,B,../data/dataiku\generative-ai\rag.md
Which of the following geographic data types can Dataiku DSS natively read and write?,GeoJSON files,ESRI Shapefiles,Snowflake GEOGRAPHY data type,KML/KMZ files,C,../data/dataiku\geographic\data.md
What additional step is required to read KML/KMZ files in Dataiku DSS?,Install the KML Format extractor plugin,Convert KML/KMZ files to GeoJSON,Enable geographic data processing in settings,No additional steps are required,A,../data/dataiku\geographic\data.md
Which of the following providers is optimal for geocoding in China?,ArcGIS,Bing,Baidu,Google,C,../data/dataiku\geographic\geocoding.md
What is required for most online geocoding service providers?,API key,Local server,Offline access,No special requirements,A,../data/dataiku\geographic\geocoding.md
Which of the following is true about the bundled-data city-level reverse geocoder?,It requires an API key,It uses external providers,It is fast and does not need any external provider,It provides address-level resolution,C,../data/dataiku\geographic\geocoding.md
Which of the following is NOT a geography-related function in the DSS Formula Language?,geoBuffer,geoContains,geoWithin,geoTransform,D,../data/dataiku\geographic\geoformula.md
Which of the following join types is NOT available in the Geo join visual recipe?,INNER,LEFT,RIGHT,OUTER,D,../data/dataiku\geographic\geojoin.md
What is the primary purpose of the Geo join visual recipe?,To perform a join between datasets based on geospatial matching conditions.,To visualize geospatial data on a map.,To clean and preprocess geospatial data.,To perform statistical analysis on geospatial data.,A,../data/dataiku\geographic\geojoin.md
What is the primary function of the Geo Router plugin in Dataiku DSS?,To compute statistical tests.,To perform data cleaning.,To compute itineraries and isochrones.,To generate reports.,C,../data/dataiku\geographic\georouting.md
Which of the following is NOT a required input for computing isochrones using the Geo Router plugin?,A dataset with a geopoint column.,The travel time.,The transport mode.,The travel distance.,D,../data/dataiku\geographic\georouting.md
Where is the georouting cache stored in a UIF setup for DSS?,In the home directory of the UNIX user that runs DSS.,In the associated UNIX user’s home directory.,In the DSS installation directory.,In the Dataiku cloud storage.,B,../data/dataiku\geographic\georouting.md
Which of the following workshops is recommended for learning how to create maps in Dataiku DSS?,Introduction to Geospatial Analytics,Creating Maps,Geographic Data Preparation,Working with Shapefiles,B,../data/dataiku\geographic\index.md
What feature of Dataiku DSS allows you to combine datasets based on geographic location?,Geocoding,Georouting,Geo join,Geographic formula functions,C,../data/dataiku\geographic\index.md
Which processor in Dataiku DSS is used to convert an IP address to geographic coordinates?,Geopoint converters,Resolve GeoIP,Reverse geocoding,Zipcode geocoding,B,../data/dataiku\geographic\preparation.md
What is the default Coordinates Reference System (CRS) used by Dataiku DSS when processing geometries?,EPSG:3857,EPSG:4326,EPSG:32633,EPSG:27700,B,../data/dataiku\geographic\preparation.md
Which processor in Dataiku DSS can create polygons centered on input geopoints?,Compute distances between points,Change coordinates system,Create area around a geopoint,Extract from geo column,C,../data/dataiku\geographic\preparation.md
Which of the following geographic data types is supported by DSS?,Geopoint,Geoshape,Geoline,Geocircle,A,../data/dataiku\geographic\types.md
What does the 'Change CRS processor' in DSS allow you to do?,Change the data type of a column,Change the coordinate reference system of a geographic column,Convert geographic data to 3D,Merge multiple geographic columns,B,../data/dataiku\geographic\types.md
Which of the following methods can be used to preview geographic data in Dataiku DSS?,Right-clicking on any cell and clicking on the 'Preview' option,Using the keyboard shortcut 'shift + v' on geographic cells,Double-clicking on the cell,Both A and B,D,../data/dataiku\geographic\visualization.md
Which of the following licenses allows you to customize your Dataiku Govern instance?,Standard license,Advanced license,Basic license,Enterprise license,B,../data/dataiku\governance\definitions.md
What is an artifact in Dataiku Govern?,A type of license,A fixed structure object with properties,A user role,A type of data visualization,B,../data/dataiku\governance\definitions.md
"Which page in Dataiku Govern centralizes events such as updates, deletions, and creations linked to items?",Governable items,Model registry,Timeline page,Bundle registry,C,../data/dataiku\governance\definitions.md
Which of the following is NOT a governance policy option available in Dataiku Deployer?,Prevent the deployment of unapproved packages.,Warn and ask for confirmation before deploying unapproved packages.,Always deploy without checking.,Automatically approve all packages.,D,../data/dataiku\governance\governance-features.md
"In Dataiku Govern, what is the purpose of the Project Qualification step?","To define the level of value, risk, and feasibility of each project.",To assign reviewers for the project.,To set up email notifications for the project.,To customize the workflow steps for the project.,A,../data/dataiku\governance\governance-features.md
Which of the following is a feature available only with an Advanced license in Dataiku Govern?,Setting up email notifications.,Creating new workflows from scratch.,Attaching documentation for a project or a model version.,Specifying the person responsible for a specific project.,B,../data/dataiku\governance\governance-features.md
Which of the following is NOT a feature of the Dataiku Govern node?,Govern all your data and analytics initiatives,Manage deployment of projects or models with a built-in sign-off process,Monitor models that have been deployed and analyze their performance over time,Generate real-time predictions for ongoing processes,D,../data/dataiku\governance\index.md
What is required to make the Govern node aware of its external URL?,Setting up email notifications,Defining the Govern instance name,"Connecting your Govern and Design, Automation or Deployer instances",Making Govern aware of its external URL,D,../data/dataiku\governance\index.md
Which of the following is a type of Govern item in Dataiku?,Business initiatives,Data pipelines,User roles,API endpoints,A,../data/dataiku\governance\index.md
"Which section provides contextual information about a project, including its description, type, scope, related items, sign-off settings, and qualification snapshot?",Workflow,Attachments,Overview,Timeline,C,../data/dataiku\governance\navigation-item.md
"In which section can you find a history of important changes made to a Govern item, including the user who made the change, the description of the change, and a timestamp?",Overview,Timeline,Workflow,Attachments,B,../data/dataiku\governance\navigation-item.md
Which section displays deployment data specifically for model versions and bundles?,Dataiku Objects,Role assignments,Deployment,Timeline,C,../data/dataiku\governance\navigation-item.md
Which of the following is a recommended practice when setting up user logins on a Dataiku cluster?,Using different user logins for each node type.,Having the same user logins between the different nodes.,Using only admin logins for all nodes.,Not setting up user logins at all.,B,../data/dataiku\governance\setup.md
What must be done after modifying the 'DATADIR/install.ini' file to set the node ID for a DSS node?,Restart the DSS node in question.,Reinstall the DSS node.,Update the operating system.,Clear the cache.,A,../data/dataiku\governance\setup.md
"What is required for the API key generated on Govern to be used on Design, Automation, or Deployer nodes?",The API key must have global admin privileges.,The API key must be read-only.,The API key must be user-specific.,The API key must be temporary.,A,../data/dataiku\governance\setup.md
Which of the following is NOT a visualization provided in the Business initiatives page in Dataiku Govern?,Matrix view,Kanban view,Graph view,None of the above,C,../data/dataiku\governance\types-govern-items.md
Where can you govern a specific model version in Dataiku Govern?,Governable items page,Model registry page,Bundle registry page,Governed projects page,B,../data/dataiku\governance\types-govern-items.md
What happens if you choose to govern a model that is part of an ungoverned project in Dataiku Govern?,Only the model will be governed,The parent project will be automatically governed,The model cannot be governed,You will receive an error message,B,../data/dataiku\governance\types-govern-items.md
Which of the following operations can be performed using the Dataiku Govern public API?,List accessible blueprints and blueprint versions,"Create, edit, and delete artifacts",Manage users and groups,All of the above,D,../data/dataiku\governance\publicapi\features.md
Which of the following is the recommended way to interact with the Dataiku Govern public API?,Using the HTTP REST API,Using the Python API client,Using a JavaScript client,Using a command-line interface,B,../data/dataiku\governance\publicapi\index.md
What type of operations can you perform with the Dataiku Govern public API?,Only data retrieval operations,Only data insertion operations,A variety of administration and maintenance operations,Only user authentication operations,C,../data/dataiku\governance\publicapi\index.md
Which of the following is true about Global API keys in Dataiku Govern?,They can be created by any user.,They are equivalent to a DSS user.,They have their own access rights.,They can only be used for read-only operations.,C,../data/dataiku\governance\publicapi\keys.md
What special permission does a global admin key have in Dataiku Govern?,Manage users,Manage log files,Access the Blueprint Designer,All of the above,D,../data/dataiku\governance\publicapi\keys.md
What is the format of the request body for POST and PUT requests in the Dataiku Govern REST API?,XML,JSON,HTML,TEXT,B,../data/dataiku\governance\publicapi\rest.md
How is authentication handled in the Dataiku Govern REST API?,OAuth Tokens,API Keys,Session Cookies,JWT Tokens,B,../data/dataiku\governance\publicapi\rest.md
What indicates a successful request in the Dataiku Govern REST API?,3xx status code,4xx status code,2xx status code,5xx status code,C,../data/dataiku\governance\publicapi\rest.md
Which of the following is NOT a requirement to access data on a filesystem using Hadoop?,Libraries to handle the filesystem installed on the cluster and DSS node,A fully-qualified URI to the file,A specific Hadoop distribution,Hadoop configuration parameters,C,../data/dataiku\hadoop\hadoop-fs-connections.md
What is the primary method of connecting to S3 as a Hadoop filesystem?,Using S3A,Using EMRFS,Using VPC Endpoints,Using HDFS,A,../data/dataiku\hadoop\hadoop-fs-connections.md
What is the URI format to access blobs on Google Cloud Storage?,adl://<datalake_storage_name>.azuredatalakestore.net/<optional_path>,wasb://container_name@your_account.blob.core.windows.net/path/inside/container/,gs://bucket_name/path/inside/bucket/,s3a://bucket_name/path/inside/bucket/,C,../data/dataiku\hadoop\hadoop-fs-connections.md
Which of the following is NOT a valid use case for using a Hive dataset?,Reading data from existing Hive views,Reading data from ACID tables in ORC format,Writing data back to Hive tables,Reading tables containing DATE and DECIMAL data types,C,../data/dataiku\hadoop\hive-dataset.md
What must be true for a Spark recipe to work with Hive datasets?,The recipe must be in 'Hive CLI (global metastore)' mode,The recipe must be in 'Use global metastore' mode,You must have filesystem-level access to the underlying files,Both B and C,D,../data/dataiku\hadoop\hive-dataset.md
Which of the following statements is true about Hive datasets?,Hive datasets can be used for both reading and writing data,Hive datasets are pointers to Hive tables already defined in the Hive metastore,SQL recipes can be used with Hive datasets,Hive datasets do not require a connection to Hadoop,B,../data/dataiku\hadoop\hive-dataset.md
Which of the following formats is NOT supported by Hive for HDFS datasets?,CSV,Parquet,JSON,Avro,C,../data/dataiku\hadoop\hive.md
What is the recommended mode for the global metastore in a Hadoop cluster?,Isolated metastore,Shared metastore,Local metastore,Distributed metastore,B,../data/dataiku\hadoop\hive.md
Which Hive execution mode is deprecated and will be removed in a future DSS release?,Hiveserver 2,Hive CLI (global metastore),Hive CLI (isolated metastore),Both B and C,D,../data/dataiku\hadoop\hive.md
Which of the following formats is NOT supported by Impala for interacting with HDFS datasets?,CSV,Parquet,JSON,Avro,C,../data/dataiku\hadoop\impala.md
What type of authentication modes does DSS support for Impala?,"No authentication, Kerberos, LDAP","OAuth, Kerberos, LDAP","No authentication, OAuth, LDAP","OAuth, Kerberos, SAML",A,../data/dataiku\hadoop\impala.md
Which of the following is a requirement for Impala to write to directories corresponding to managed datasets?,The directory holding the managed datasets must give read permission to the 'impala' user,The directory holding the managed datasets must default to giving write permissions to other users,The directory holding the managed datasets must be set to permission 755,The directory holding the managed datasets must be owned by the 'impala' user,B,../data/dataiku\hadoop\impala.md
Which of the following is NOT a supported Hadoop filesystem connection in DSS?,HDFS,S3,Azure Blob Storage,Google Drive,D,../data/dataiku\hadoop\index.md
What is the purpose of configuring Kerberos credentials periodic renewal in DSS?,To ensure continuous secure access to Hadoop clusters,To improve the speed of data processing,To enable multi-user access,To reduce storage costs,A,../data/dataiku\hadoop\index.md
Which of the following is a Hive execution engine supported by DSS?,Hiveserver 2,Presto,BigQuery,Redshift,A,../data/dataiku\hadoop\index.md
Which of the following is NOT a prerequisite for setting up DSS Hadoop integration?,Having client access to the Hadoop cluster,Installing Hadoop client libraries,Setting up a writable HDFS home directory,Having a running HiveServer2 server,C,../data/dataiku\hadoop\installation.md
What should you do if your Hadoop cluster has Kerberos security enabled?,Follow the standard DSS Hadoop integration instructions,"Manually install libraries, binaries, and configuration",Refer to the 'Connecting to secure clusters' documentation,Disable Kerberos security,C,../data/dataiku\hadoop\installation.md
What is the purpose of the 'hdfs_root' connection in DSS Hadoop integration?,To provide read-only access to the entire HDFS filesystem,To store DSS-generated datasets,To configure Hive connectivity,To configure Impala connectivity,A,../data/dataiku\hadoop\installation.md
Which of the following is NOT supported when using multiple Hadoop clusters in DSS?,Running multiple Spark versions,Using different Hadoop distributions,Using the same Kerberos realms,User mappings must be similar between clusters,B,../data/dataiku\hadoop\multi-clusters.md
What is the purpose of 'managed dynamic clusters' in DSS?,To define static connection settings for additional clusters,"To create, configure, and shutdown dynamic clusters through DSS",To manage user permissions for clusters,To override runtime configurations for Spark,B,../data/dataiku\hadoop\multi-clusters.md
"In DSS, what does the 'builtin cluster' configuration include?",Only the YARN resource manager address,Only the HiveServer2 connection details,Only the Impala connection details,"Hadoop, Hive, Impala, and Spark configurations",D,../data/dataiku\hadoop\multi-clusters.md
What is the purpose of the Kerberos principal in the context of DSS connecting to a secure Hadoop cluster?,To store user data,To authenticate and authorize DSS to access Hadoop resources,To manage Hadoop cluster configurations,To monitor Hadoop cluster performance,B,../data/dataiku\hadoop\secure-clusters.md
Which of the following is NOT a step in setting up the DSS Kerberos account?,Creating a Kerberos principal for DSS,Creating a Kerberos keytab for the account,Configuring the Hadoop cluster to authorize the principal,Installing Hadoop on the DSS Unix server,D,../data/dataiku\hadoop\secure-clusters.md
What is the default renewal period for Kerberos credentials in DSS?,30 minutes,1 hour,2 hours,1 day,B,../data/dataiku\hadoop\secure-clusters.md
Which version of Spark is deprecated in Dataiku DSS?,Spark 1,Spark 2,Spark 3,Spark 4,B,../data/dataiku\hadoop\spark.md
What must be done to ensure SparkSQL fully works from DSS in some Hadoop installations?,Install additional Spark libraries,Enable DEBUG logging,Authorize the DSS user account to access the Hive metastore,Update the DSS software,C,../data/dataiku\hadoop\spark.md
What is the recommended logging level for the 'org.apache.http.wire' logger in Spark?,DEBUG,ERROR,WARN,INFO,D,../data/dataiku\hadoop\spark.md
Which of the following distribution methods is available for syncing data from Teradata to HDFS using the Teradata Connector for Hadoop (TDCH)?,batch.insert,internal.fastload,split.by.hash,split.by.value,C,../data/dataiku\hadoop\tdch.md
Which of the following is a limitation of the Teradata Connector for Hadoop (TDCH)?,Only the JSON format is supported for the HDFS dataset,Partitioned datasets are not supported,SQL 'query' datasets are supported,Properties defined at the HDFS connection level are taken into account,B,../data/dataiku\hadoop\tdch.md
Which version of Spark is supported on Cloudera Data Platform 7.1.8?,Spark 2,Spark 3.2,Spark 3.3,Spark 3.1,C,../data/dataiku\hadoop\distributions\cdp.md
Which of the following is not supported by Dataiku DSS?,Connecting to secure clusters,User isolation with Ranger,Using Knox,Deploying DSS within the zone protected by Knox,C,../data/dataiku\hadoop\distributions\cdp.md
What is the workaround for the Hive issue in CDP 7.1.9 that impacts Window recipes in DSS?,Upgrade to a newer version of Hive,Set hive.vectorized.execution.reduce.enabled=false,Use a bounded window frame,Downgrade to an older version of CDP,B,../data/dataiku\hadoop\distributions\cdp.md
Which of the following is NOT a way to set up Dataiku DSS?,Dataiku Cloud,Dataiku Cloud Stacks,Custom Dataiku,Dataiku On-Premises,D,../data/dataiku\installation\index.md
Which cloud providers are supported by Dataiku Cloud Stacks? (Choose all that apply),Amazon Web Services (AWS),Google Cloud Platform (GCP),Microsoft Azure,IBM Cloud,"A, B, C",../data/dataiku\installation\index.md
What is the main function of the Dataiku Fleet Manager (FM) in the Dataiku Cloud Stacks for AWS setup?,"To handle the entire lifecycle of Dataiku instances, including deployment, upgrade, backup, restore, and configuration.",To manage user access and permissions for Dataiku instances.,To provide data visualization tools for Dataiku instances.,To offer machine learning model training and evaluation services.,A,../data/dataiku\installation\cloudstacks-aws\concepts.md
What does provisioning an instance mean in the context of Dataiku Fleet Manager?,Creating a backup of the DSS node.,Creating the required cloud resources to host the DSS node.,Upgrading the DSS node to the latest version.,Deleting the existing DSS node.,B,../data/dataiku\installation\cloudstacks-aws\concepts.md
What is the role of the FM agent in Dataiku instances?,To provide data visualization tools.,To manage communication with the FM server and perform administrative tasks.,To train and evaluate machine learning models.,To handle user authentication and authorization.,B,../data/dataiku\installation\cloudstacks-aws\concepts.md
Which of the following statements is true about manual snapshots in DSS?,You need to stop the instance to make a manual snapshot.,Manual snapshots have an impact on instance users.,You can manually backup your instance at any time by clicking on 'New snapshot' in the 'Snapshots' tab.,Manual snapshots are automatically created at a specified frequency.,C,../data/dataiku\installation\cloudstacks-aws\dss-backup.md
How can you enable automated snapshots in DSS?,By clicking on 'New snapshot' in the 'Snapshots' tab.,By enabling it in the 'Settings' tab of the instance.,By stopping the instance and then enabling it.,By manually creating a snapshot every time.,B,../data/dataiku\installation\cloudstacks-aws\dss-backup.md
What is a necessary step before upgrading to Fleet Manager version 12.6.0 or higher?,Backup the Fleet Manager's data disk,Stop the virtual machine hosting Fleet Manager,Delete the existing stack,Create a new stack,B,../data/dataiku\installation\cloudstacks-aws\fm-upgrade-cf.md
Which of the following is NOT a step in the process of stopping the Fleet Manager server?,Find your already created stack under Cloud Formation,Copy the Instance ID and go to the EC2 console,Create a snapshot of the data volume,Click on 'Instance State > Stop instance',C,../data/dataiku\installation\cloudstacks-aws\fm-upgrade-cf.md
"If you encounter a PostgreSQL related error message after an upgrade, what is a likely cause?",The machine hosting Fleet Manager was not properly stopped before the upgrade,The data volume was not backed up,The existing stack was not deleted,The new stack was not created with the correct settings,A,../data/dataiku\installation\cloudstacks-aws\fm-upgrade-cf.md
What is the purpose of the guided setup described in the document?,"To set up a Dataiku Cloud Stacks for AWS, including Elastic Compute clusters.",To configure a local development environment for Dataiku.,To deploy a Dataiku instance on Google Cloud Platform.,To set up a Dataiku instance on Microsoft Azure.,A,../data/dataiku\installation\cloudstacks-aws\guided-setup-new-vpc-elastic-compute.md
Which AWS service is used to manage the Elastic Compute clusters in the guided setup?,Amazon EC2,Amazon S3,Amazon EKS,Amazon RDS,C,../data/dataiku\installation\cloudstacks-aws\guided-setup-new-vpc-elastic-compute.md
What is required to start the first DSS instance in the guided setup?,A valid Dataiku license,An Amazon RDS instance,A Google Cloud Platform account,A Microsoft Azure subscription,A,../data/dataiku\installation\cloudstacks-aws\guided-setup-new-vpc-elastic-compute.md
Which of the following is NOT a feature of the Dataiku setup on AWS?,Elastic AI,Advanced security,Auto-healing setup,Dataiku has access to your data,D,../data/dataiku\installation\cloudstacks-aws\index.md
What is the purpose of the Fleet Manager (FM) in the Dataiku Cloud Stacks for AWS?,To manage virtual networks,To handle instance templates,To manage and orchestrate Dataiku instances,To provide AWS credentials,C,../data/dataiku\installation\cloudstacks-aws\index.md
Which of the following is NOT a step in the guided setup for deploying in a new VPC with Elastic Compute?,VPC setup,IAM setup,Fleet Manager setup,Install a JDBC driver,D,../data/dataiku\installation\cloudstacks-aws\index.md
Which of the following is NOT a type of DSS instance managed by Fleet Manager?,Design nodes,Execution nodes,Deployer nodes,API nodes,D,../data/dataiku\installation\cloudstacks-aws\instances.md
What is the main purpose of the dashboard in Fleet Manager?,To manage SSL settings,To display basic network information and data disk usage,To provision cloud resources,To deprovision instances,B,../data/dataiku\installation\cloudstacks-aws\instances.md
Which SSL strategy involves generating a certificate that is trusted by default by most browsers?,None (HTTP only),Self-signed certificates,Certificate/key for each instance,Generate certificates using Let’s Encrypt,D,../data/dataiku\installation\cloudstacks-aws\instances.md
Which of the following protocols is NOT supported by FM for Single Sign-On (SSO)?,OpenID Connect,SAML,OAuth 2.0,None of the above,C,../data/dataiku\installation\cloudstacks-aws\sso.md
What is the purpose of the 'Authorization code' in the OIDC protocol?,To authenticate the end user directly,To exchange for an ID token,To store user credentials securely,To generate a new password for the user,B,../data/dataiku\installation\cloudstacks-aws\sso.md
Which of the following identity providers has NOT been tested for compatibility with FM's OIDC integration?,OKTA,Azure Active Directory,Google G.Suite,Facebook,D,../data/dataiku\installation\cloudstacks-aws\sso.md
Which of the following setup actions ensures DSS can use Kubernetes clusters and Spark integration?,Install system packages,Add authorized SSH keys,Setup Kubernetes and Spark-on-Kubernetes,Run Ansible tasks,C,../data/dataiku\installation\cloudstacks-aws\templates-actions.md
What is the recommended way to offer AWS credentials to DSS instances?,Using a keypair,Using an IAM instance profile,Storing AWS Secret Access Key in ASM,Encrypting and storing AWS Secret Access Key by FM,B,../data/dataiku\installation\cloudstacks-aws\templates-actions.md
Which setup action allows you to install additional system packages on the machine?,Install a JDBC driver,Set security related HTTP Headers,Install system packages,Add authorized SSH keys,C,../data/dataiku\installation\cloudstacks-aws\templates-actions.md
What is the recommended way to ensure Fleet Manager has the proper permissions to perform various calls to the AWS API?,Using AWS credentials for DSS,Using an IAM instance profile with the proper permissions,Using SSL certificates,Using a Dataiku License Token,B,../data/dataiku\installation\cloudstacks-aws\tenant-settings.md
Which AWS service is NOT mentioned as needing to be reachable from the Fleet Manager virtual machine?,EC2,STS,Route53,S3,D,../data/dataiku\installation\cloudstacks-aws\tenant-settings.md
What is required to benefit from most capabilities of Fleet Manager?,AWS KMS CMK,IAM instance profile,Dataiku License or Dataiku License Token,SSL certificates,C,../data/dataiku\installation\cloudstacks-aws\tenant-settings.md
Which of the following is a mandatory value required when creating a new virtual network in Fleet Manager?,Label,VPC id,Subnet id,All of the above,D,../data/dataiku\installation\cloudstacks-aws\virtual-networks.md
What is the default HTTPS configuration for instances deployed in Fleet Manager?,None,Self-signed certificate,Custom certificate for each instance,Let’s Encrypt,B,../data/dataiku\installation\cloudstacks-aws\virtual-networks.md
What permissions must the IAM Role used by the Fleet Manager have for DNS Strategy?,route53:GetHostedZone,route53:ChangeResourceRecordSets,Both A and B,None of the above,C,../data/dataiku\installation\cloudstacks-aws\virtual-networks.md
What is the primary role of the Dataiku Fleet Manager (FM) in the Dataiku Cloud Stacks for Azure setup?,"To handle the entire lifecycle of Dataiku instances, including deployment, upgrade, backup, restore, and configuration.",To manage the virtual networks used by the instances.,To provide a set of configuration information that can be reused to start several instances with common properties.,To run alongside DSS in your instances and manage communication with the FM server.,A,../data/dataiku\installation\cloudstacks-azure\concepts.md
What does provisioning an instance mean in the context of Dataiku Fleet Manager?,Creating a set of configuration information that can be reused to start several instances.,Creating the required cloud resources to host the DSS node.,Managing communication with the FM server and performing administrative tasks.,Representing the network context in which the instances will be launched.,B,../data/dataiku\installation\cloudstacks-azure\concepts.md
What is the function of an instance template in Dataiku Fleet Manager?,It represents the network context in which the instances will be launched.,"It is a single installation of a DSS design node, automation node, or deployer node.",It is a set of configuration information that can be reused to start several instances with common properties.,It runs alongside DSS in your instances and manages communication with the FM server.,C,../data/dataiku\installation\cloudstacks-azure\concepts.md
Which of the following statements is true about manual snapshots in DSS?,You need to stop the instance to make a manual snapshot.,Manual snapshots have an impact on the instance users.,You can manually backup your instance at any time by clicking on 'New snapshot' in the 'Snapshots' tab.,Manual snapshots are automatically created at a specified frequency.,C,../data/dataiku\installation\cloudstacks-azure\dss-backup.md
How can you enable automated snapshots in DSS?,By clicking on 'New snapshot' in the 'Snapshots' tab.,By enabling the feature in the 'Settings' tab of the instance.,By stopping the instance and then enabling the feature.,By manually creating a snapshot at a specified frequency.,B,../data/dataiku\installation\cloudstacks-azure\dss-backup.md
Which of the following steps is NOT involved in upgrading a DSS instance?,Go to the settings of the instance,Select the new DSS version in the 'Version' field,Reprovision the instance,Restart the existing virtual machine,D,../data/dataiku\installation\cloudstacks-azure\dss-upgrade.md
What should you do if your Fleet Manager instance does not have direct outgoing Internet access to update the list of available DSS versions?,Wait for an automatic update,Manually download the updates,Contact your Dataiku Technical Account Manager or Customer Success Manager,Restart the Fleet Manager instance,C,../data/dataiku\installation\cloudstacks-azure\dss-upgrade.md
What is a required step before upgrading to Fleet Manager version 12.6.0 or higher?,Backup the data disk,Stop the virtual machine hosting Fleet Manager,Delete the existing server,Create a new stack,B,../data/dataiku\installation\cloudstacks-azure\fm-upgrade.md
What should you do if you encounter a PostgreSQL related error message after upgrading Fleet Manager?,Restart the virtual machine,Reinstall PostgreSQL,Upgrade to an intermediate version first,Delete the data disk,C,../data/dataiku\installation\cloudstacks-azure\fm-upgrade.md
What is the purpose of creating a managed identity for your DSS instances in the Dataiku Cloud Stacks for Azure setup?,To provide a unique identifier for the DSS instances,To assign a public IP address to the DSS instances,To give permissions to the managed identities used by the software at runtime,To create a new resource group for the DSS instances,C,../data/dataiku\installation\cloudstacks-azure\guided-setup-new-vnet-elastic-compute.md
Which of the following is NOT a step in deploying the Fleet Manager in the Dataiku Cloud Stacks for Azure setup?,Choosing the target subscription,Entering a strong password for logging in to Fleet Manager,Creating a new Azure Container Registry,Selecting the size of the Virtual machine,C,../data/dataiku\installation\cloudstacks-azure\guided-setup-new-vnet-elastic-compute.md
What is the purpose of the 'Elastic Compute cluster' in the Dataiku Cloud Stacks for Azure setup?,To provide a fully-managed DSS design node,To enable containerized execution for any activity in a project,To assign a public IP address to the Fleet Manager,To create a new resource group for the DSS instances,B,../data/dataiku\installation\cloudstacks-azure\guided-setup-new-vnet-elastic-compute.md
Which of the following is NOT a step in the guided setup for deploying in a new VNet with Elastic Compute?,Create a managed identity for your Fleet Manager instance,Create a managed identity for your DSS instances,Deploy Fleet Manager,Configure SSL settings,D,../data/dataiku\installation\cloudstacks-azure\index.md
What is the purpose of the Fleet Manager (FM) in Dataiku Cloud Stacks for Azure?,To manage virtual networks,To deploy and manage DSS instances,To handle data backups,To configure SSL settings,B,../data/dataiku\installation\cloudstacks-azure\index.md
Which of the following SSL configurations is NOT supported for instances in Dataiku Cloud Stacks for Azure?,None (HTTP only),Self-signed certificates,Certificate/key for each instance,Wildcard certificates,D,../data/dataiku\installation\cloudstacks-azure\index.md
Which of the following is NOT a type of DSS instance managed by Fleet Manager?,Design nodes,Execution nodes,Deployer nodes,API nodes,D,../data/dataiku\installation\cloudstacks-azure\instances.md
What is the main purpose of the dashboard in DSS?,To manage SSL settings,To display basic network information and data disk usage,To provision cloud resources,To deprovision instances,B,../data/dataiku\installation\cloudstacks-azure\instances.md
Which SSL strategy involves generating a certificate that is not trusted by default and requires manual trust by users?,None (HTTP only),Self-signed certificates,Certificate/key for each instance,Generate certificates using Let’s Encrypt,B,../data/dataiku\installation\cloudstacks-azure\instances.md
Which of the following SSO protocols is supported by FM?,OAuth 2.0,OpenID Connect,LDAP,Kerberos,B,../data/dataiku\installation\cloudstacks-azure\sso.md
What is the role of the 'ID Token' in the OIDC protocol?,It is used to store user passwords.,It contains required attributes or claims about the user.,It is used to encrypt user data.,It is a backup authentication method.,B,../data/dataiku\installation\cloudstacks-azure\sso.md
Which of the following identity providers has FM's OIDC integration been successfully tested against?,Facebook,Twitter,Google G.Suite,LinkedIn,C,../data/dataiku\installation\cloudstacks-azure\sso.md
Which of the following setup actions ensures DSS adds security HTTP headers?,Install system packages,Add authorized SSH keys,Set security related HTTP Headers,Install a JDBC driver,C,../data/dataiku\installation\cloudstacks-azure\templates-actions.md
What is the recommended way to offer Azure credentials to DSS instances?,Using a User-Assigned Managed Identity,Using a Startup managed identity,Using a Runtime managed identity,Using a public SSH key,A,../data/dataiku\installation\cloudstacks-azure\templates-actions.md
Which setup action installs a code environment with the Visual Machine Learning and Visual Time series forecasting preset?,Install system packages,Install a code environment with a Visual ML preset,Run Ansible tasks,Add Certificate Authority to DSS truststore,B,../data/dataiku\installation\cloudstacks-azure\templates-actions.md
Which of the following is required for Fleet Manager to perform various calls to the Azure API?,A Dataiku License,A User Assigned Managed Identity with Contributor access to the Resource Group,An HTTP proxy,Service Tags,B,../data/dataiku\installation\cloudstacks-azure\tenant-settings.md
What is necessary to benefit from most capabilities in Fleet Manager?,An HTTP proxy,Service Tags,A Dataiku License or Dataiku License Token,A User Assigned Managed Identity,C,../data/dataiku\installation\cloudstacks-azure\tenant-settings.md
Which of the following is NOT a requirement for creating a virtual network in Fleet Manager?,Label,Virtual network id,Subnet name,Public IP address,D,../data/dataiku\installation\cloudstacks-azure\virtual-networks.md
What is the default HTTPS configuration for instances in Fleet Manager?,None,Self-signed certificate,Custom certificate for each instance,Let’s Encrypt,B,../data/dataiku\installation\cloudstacks-azure\virtual-networks.md
Which of the following HTTPS configurations requires specifying an email address?,None,Self-signed certificate,Custom certificate for each instance,Let’s Encrypt,D,../data/dataiku\installation\cloudstacks-azure\virtual-networks.md
What is the primary role of the Dataiku Fleet Manager (FM) in the Dataiku Cloud Stacks for GCP setup?,"To handle the entire lifecycle of Dataiku instances, including deployment, upgrade, backup, restore, and configuration.",To manage the virtual network configurations for Dataiku instances.,To provide a user interface for data analysis and visualization.,To create machine learning models automatically.,A,../data/dataiku\installation\cloudstacks-gcp\concepts.md
What does provisioning an instance in Dataiku Fleet Manager (FM) involve?,Creating a new machine learning model.,Creating the required cloud resources to host the DSS node.,Setting up a virtual network for the instance.,Launching the Dataiku user interface.,B,../data/dataiku\installation\cloudstacks-gcp\concepts.md
What is the function of the FM agent in Dataiku instances?,To create machine learning models.,To manage communication with the FM server and perform administrative tasks.,To handle data visualization tasks.,To configure virtual networks.,B,../data/dataiku\installation\cloudstacks-gcp\concepts.md
Which of the following statements is true about manual snapshots in DSS?,You need to stop the instance to make a manual snapshot.,Manual snapshots have an impact on instance users.,You can manually backup your instance at any time by clicking on 'New snapshot' in the 'Snapshots' tab.,Manual snapshots are only available in the 'Settings' tab.,C,../data/dataiku\installation\cloudstacks-gcp\dss-backup.md
What can you configure in the 'Settings' tab of the instance for automated snapshots?,The type of data to be backed up.,The maximum number of snapshots to retain.,The users who can access the snapshots.,The storage location for the snapshots.,B,../data/dataiku\installation\cloudstacks-gcp\dss-backup.md
What is a required step before upgrading to Fleet Manager version 12.6.0 or higher?,Create a snapshot of the data disk.,Stop the virtual machine hosting Fleet Manager.,Delete the existing deployment.,Reprovision the DSS machines.,B,../data/dataiku\installation\cloudstacks-gcp\fm-upgrade.md
Which command can help troubleshoot a non-responsive Fleet Manager after an upgrade?,sudo systemctl status fm-setup,sudo journalctl -u fm-setup,sudo service fm-setup restart,sudo fm-setup logs,B,../data/dataiku\installation\cloudstacks-gcp\fm-upgrade.md
Which of the following roles is NOT required when creating a service account for your DSS instance?,Service account user,Kubernetes engine admin,Compute viewer,DNS administrator,D,../data/dataiku\installation\cloudstacks-gcp\guided-setup-new-vpc-elastic-compute.md
What is the purpose of the 'internal-cidr' when creating a VPC to host the fleet manager?,To allow internal routing on the network between VMs and secondary IP ranges,To assign public IP addresses to the VMs,To set the firewall rules for external access,To define the region for the subnet,A,../data/dataiku\installation\cloudstacks-gcp\guided-setup-new-vpc-elastic-compute.md
Which of the following is a step in deploying the Fleet Manager?,Create a service account for the DSS instance,Ensure access to the container registry,Fetch the template files using gsutil,Create a VPC to host the DSS instance,C,../data/dataiku\installation\cloudstacks-gcp\guided-setup-new-vpc-elastic-compute.md
Which of the following is NOT a feature of the fully-managed Dataiku setup on Google Cloud?,Elastic AI,Advanced security,R support,Dataiku having access to your data,D,../data/dataiku\installation\cloudstacks-gcp\index.md
What is the first step in the guided setup for deploying Dataiku in a new VPC with Elastic Compute?,Create a service account for your DSS instance,Create a service account for your Fleet Manager instance,Ensure access to the container registry,Create a VPC to host the fleet manager,B,../data/dataiku\installation\cloudstacks-gcp\index.md
Which of the following is NOT an HTTPS configuration option for virtual networks in Dataiku?,None,Self-signed certificate,Custom certificate for each instance,Public certificate for all instances,D,../data/dataiku\installation\cloudstacks-gcp\index.md
Which of the following is NOT a type of DSS instance managed by Fleet Manager?,Design nodes,Execution nodes,Deployer nodes,API nodes,D,../data/dataiku\installation\cloudstacks-gcp\instances.md
What does deprovisioning an instance involve?,Terminating the cloud virtual machine while keeping the Persistent Disk,Deleting all data on the Persistent Disk,Upgrading DSS software,Creating a new virtual machine,A,../data/dataiku\installation\cloudstacks-gcp\instances.md
Which of the following SSO protocols is supported by FM?,OAuth 2.0,OpenID Connect,LDAP,Kerberos,B,../data/dataiku\installation\cloudstacks-gcp\sso.md
What is the purpose of the 'Identifier claim key' in OIDC configuration for FM?,To specify the URL of the identity provider,To define the user attribute used to map to the corresponding username in FM,To set the client ID for the OIDC client,To configure the token endpoint authentication method,B,../data/dataiku\installation\cloudstacks-gcp\sso.md
Which of the following identity providers has FM's OIDC integration been successfully tested against?,Facebook,Twitter,Google G.Suite,LinkedIn,C,../data/dataiku\installation\cloudstacks-gcp\sso.md
Which of the following setup actions ensures DSS adds security HTTP headers?,Install system packages,Add authorized SSH keys,Set security related HTTP Headers,Install a JDBC driver,C,../data/dataiku\installation\cloudstacks-gcp\templates-actions.md
What is the recommended way to offer GCP credentials to DSS instances?,Using a public SSH key,Using a service account,Using a JDBC driver,Using HTTP headers,B,../data/dataiku\installation\cloudstacks-gcp\templates-actions.md
Which setup action installs a code environment with the Visual Machine Learning and Visual Time series forecasting preset?,Install system packages,Run Ansible tasks,Install a code environment with a Visual ML preset,Add Certificate Authority to DSS truststore,C,../data/dataiku\installation\cloudstacks-gcp\templates-actions.md
What is required to benefit from most capabilities in Fleet Manager?,A GCP API Key,A Dataiku License or Dataiku License Token,An HTTP Proxy,A Private Service Connect,B,../data/dataiku\installation\cloudstacks-gcp\tenant-settings.md
Which of the following is NOT a function that Fleet Manager can perform behind an HTTP proxy?,Fetch new DSS image lists,Update or verify licenses,Log users in with the OpenID Connect protocol,Perform calls to GCP services,D,../data/dataiku\installation\cloudstacks-gcp\tenant-settings.md
Which of the following is a requirement for deploying DSS instances in a virtual network?,DSS instances must be able to reach FM on its main port.,DSS instances must have a public IP address.,DSS instances must use self-signed certificates.,DSS instances must be deployed in multiple networks.,A,../data/dataiku\installation\cloudstacks-gcp\virtual-networks.md
What is the default HTTPS configuration for instances in FM?,None,Self-signed certificate,Custom certificate for each instance,Let’s Encrypt,B,../data/dataiku\installation\cloudstacks-gcp\virtual-networks.md
Which DNS strategy requires reprovisioning the affected instances?,Assigning a GCP DNS domain name,Using self-signed certificates,Changing the public IP policy,None of the above,A,../data/dataiku\installation\cloudstacks-gcp\virtual-networks.md
Which of the following is NOT a mandatory entry in the initial installation configuration file for DSS?,nodetype,port,ssl,None of the above,C,../data/dataiku\installation\custom\advanced-customization.md
What is the default maximum file size for DSS log files before they are rotated?,10MB,50MB,100MB,500MB,B,../data/dataiku\installation\custom\advanced-customization.md
Which of the following log files is NOT rotated automatically by default?,backend.log,hproxy.log,nginx/access.log,supervisord.log,C,../data/dataiku\installation\custom\advanced-customization.md
Which of the following Java versions are supported by DSS?,Java 8 and 11,Java 11 and 17,Java 8 and 17,Java 7 and 11,B,../data/dataiku\installation\custom\advanced-java-customization.md
What is the purpose of the 'jek' process in DSS?,"Handles all interaction with users, the configuration, and the visual data preparation","Runs the jobs (e.g., what happens when you use 'Build')",Handles long-running background tasks and builds data samples,Handles interactions with Hive,B,../data/dataiku\installation\custom\advanced-java-customization.md
Which file should be edited to customize the maximum memory size (xmx) for DSS Java processes?,env-default.sh,env-site.sh,install.ini,dssadmin,C,../data/dataiku\installation\custom\advanced-java-customization.md
Which of the following is required to install a DSS API node?,A specific DSS license,A Kubernetes-based infrastructure,A pre-existing data directory with data,A different installer script than for a design node,A,../data/dataiku\installation\custom\api-node.md
What additional parameter must be added to the installer.sh command-line to install a DSS API node?,-t api,-d api,-p api,-l api,A,../data/dataiku\installation\custom\api-node.md
Which of the following is required to install a DSS automation node?,A specific DSS license,A pre-existing data directory with data,A different installer script than the design node,A different operating system than the design node,A,../data/dataiku\installation\custom\automation-node.md
What additional parameter must be added to the installer.sh command-line to install a DSS automation node?,-t automation,-d DATA_DIR,-p PORT,-l LICENSE_FILE,A,../data/dataiku\installation\custom\automation-node.md
What additional parameter must be added to the installer.sh command-line to install a deployer node?,-t deployer,-d DATA_DIR,-p PORT,-l LICENSE_FILE,A,../data/dataiku\installation\custom\deployer-node.md
Which of the following is required to use a deployer node?,A specific DSS license,A specific operating system,A specific hardware configuration,A specific network setup,A,../data/dataiku\installation\custom\deployer-node.md
Which of the following is a requirement for installing a Govern node in Dataiku?,MySQL 8+ database,Postgresql 12+ database,Oracle 11g database,SQLite database,B,../data/dataiku\installation\custom\govern-node.md
What additional flag must be added to the installer.sh command-line when installing a Govern node?,-t design,-t api,-t govern,-t admin,C,../data/dataiku\installation\custom\govern-node.md
Which file needs to be edited to set up the Postgresql database connection for Govern?,DATA_DIR/config/dip.properties,DATA_DIR/config/db.properties,DATA_DIR/config/connection.properties,DATA_DIR/config/settings.properties,A,../data/dataiku\installation\custom\govern-node.md
Which of the following operating systems is not compatible with the DSS export feature?,Centos 6,AmazonLinux,Ubuntu,Windows,A,../data/dataiku\installation\custom\graphics-export.md
What is the purpose of the embedded Chrome browser in the DSS export feature?,To provide a user interface for DSS,To perform the actual snapshotting for exports,To manage user accounts,To handle data storage,B,../data/dataiku\installation\custom\graphics-export.md
What command should be run to enable user namespaces if the result of 'sysctl user.max_user_namespaces' is 0?,sysctl user.max_user_namespaces=1000,sysctl user.max_user_namespaces=0,sysctl user.max_user_namespaces=500,sysctl user.max_user_namespaces=2000,A,../data/dataiku\installation\custom\graphics-export.md
Which of the following tasks can be performed by Data Science Studio when connected to a Hadoop cluster?,Read and write HDFS datasets,Run Hive queries and scripts,Run Impala queries,All of the above,D,../data/dataiku\installation\custom\hadoop-spark.md
What additional capabilities does Data Science Studio gain with Spark integration?,Run SparkSQL queries,"Run preparation, join, stack and group recipes on Spark",Train and use Spark MLLib models,All of the above,D,../data/dataiku\installation\custom\hadoop-spark.md
Which of the following nodes is used for real-time predictions in Dataiku DSS?,Automation node,Deployer node,API node,Govern node,C,../data/dataiku\installation\custom\index.md
Which of the following is NOT a required package for installing Dataiku DSS on a Debian/Ubuntu Linux distribution?,curl,git,nginx,apache2,D,../data/dataiku\installation\custom\initial-install.md
What is the minimum recommended space for the Dataiku DSS data directory?,50 GB,100 GB,150 GB,200 GB,B,../data/dataiku\installation\custom\initial-install.md
Which command is used to start Dataiku DSS directly?,sudo systemctl start dataiku,DATA_DIR/bin/dss start,dataiku-dss-VERSION/installer.sh -d /path/to/DATA_DIR -p PORT,tar xzf /PATH/TO/dataiku-dss-VERSION.tar.gz,B,../data/dataiku\installation\custom\initial-install.md
Which of the following databases does Data Science Studio come with bundled drivers for?,MySQL,PostgreSQL,Vertica,Oracle,B,../data/dataiku\installation\custom\jdbc.md
What is required for PostgreSQL script recipe support in Data Science Studio?,A compatible command-line psql client,PostgreSQL version 8,A My Vertica account,A JDBC driver for PostgreSQL,A,../data/dataiku\installation\custom\jdbc.md
Which of the following steps is NOT involved in migrating the base port of a Data Science Studio (DSS) instance?,Stopping DSS,Editing the installation configuration file,Running the installer in upgrade mode,Restarting DSS,C,../data/dataiku\installation\custom\migrations.md
What must be done after moving the data directory to a new location during a DSS migration?,Reinstall locally-installed Python packages,Edit the installation configuration file,Run the installer in upgrade mode,Stop DSS,A,../data/dataiku\installation\custom\migrations.md
Which of the following Python versions is NOT supported by DSS for its builtin Python environment?,3.6,3.7,3.8,3.10,C,../data/dataiku\installation\custom\python.md
What is the purpose of the '-P PYTHONBIN' option in the DSS installer?,To specify the data directory for DSS,To select the Python version for the builtin environment,To stop the DSS service,To remove the builtin virtual environment,B,../data/dataiku\installation\custom\python.md
Under what condition should the procedure to rebuild the builtin Python virtual environment be performed?,When upgrading DSS to a new version,When instructed by Dataiku Support,When adding new Python packages,When changing the default data directory,B,../data/dataiku\installation\custom\python.md
Which R versions are required for DSS integration?,R version 3.5 or R version 4,R version 3.6 or R version 4,R version 3.4 or R version 4,R version 3.7 or R version 4,B,../data/dataiku\installation\custom\r.md
What should you do if the DSS server has Internet access only through a web proxy during R integration?,Use the default repository without any changes,Configure the proxy using the http_proxy and/or https_proxy environment variables,Manually download the R packages on another system,Skip the installation of missing dependencies,B,../data/dataiku\installation\custom\r.md
What is the first step in the automatic installation procedure for R integration if your DSS server has Internet access?,Run the installation script,Stop DSS,Go to the DSS data directory,Start DSS,C,../data/dataiku\installation\custom\r.md
Which of the following Linux distributions is NOT fully supported for installing DSS?,"Red Hat Enterprise Linux, versions 8.4 and later 8.x","Ubuntu Server, versions 20.04 LTS and 22.04 LTS",Debian 10.x,Amazon Linux 2,C,../data/dataiku\installation\custom\requirements.md
What is the minimum amount of RAM required to run DSS?,16 GB,32 GB,64 GB,128 GB,B,../data/dataiku\installation\custom\requirements.md
Which filesystem is NOT recommended for installing DSS?,XFS,ext4,NFS,POSIX compliant filesystem,C,../data/dataiku\installation\custom\requirements.md
Which of the following is NOT a requirement for configuring a reverse proxy for Dataiku Data Science Studio?,nginx version 1.4 or above,Apache version 2.4.5 or above,A valid certificate for HTTPS connections,Superuser privileges for DSS,D,../data/dataiku\installation\custom\reverse-proxy.md
What is the purpose of setting 'proxy_http_version 1.1' in the nginx reverse proxy configuration for Dataiku Data Science Studio?,To allow long queries,To allow large uploads,To allow protocol upgrade to websocket,To allow large downloads,C,../data/dataiku\installation\custom\reverse-proxy.md
Which of the following is NOT a recommended way to upgrade a DSS instance?,Upgrading an instance in-place,Upgrading by project import/export,Upgrading by cloning the instance,Upgrading by creating a new instance and copying DATA_DIR,B,../data/dataiku\installation\custom\upgrade.md
What is a necessary step before starting the upgrade of a DSS instance?,Rebuild base images,Stop the old version of DSS,Reinstall standalone Hadoop and Spark,Rebuild code envs,B,../data/dataiku\installation\custom\upgrade.md
Which of the following tasks may need to be performed after a major DSS upgrade?,Rebuild code envs,Reinstall graphics exports,Reinstall standalone Hadoop and Spark,Unpack the new software,A,../data/dataiku\installation\custom\upgrade.md
Which of the following is NOT a recommended use case for the Dataiku AMI on AWS?,Quickly starting a DSS instance for testing purposes,Using as an edge node of an existing Hadoop cluster,Running a standalone DSS instance with default options,Setting up a DSS instance for production purposes,B,../data/dataiku\installation\other\aws.md
What is the default operating system for the Dataiku AMI on AWS?,Ubuntu,AmazonLinux,AlmaLinux 8,CentOS,C,../data/dataiku\installation\other\aws.md
Which user account does DSS run under by default on the Dataiku AMI?,ec2-user,root,dataiku,admin,C,../data/dataiku\installation\other\aws.md
Which of the following is NOT a recommended use case for the pre-built Dataiku image on Azure?,Testing purposes,Production purposes,Quick start of a DSS instance,Standalone DSS instance with default options,B,../data/dataiku\installation\other\azure.md
What is the default port for accessing DSS via HTTPS on the Azure VM instance?,80,443,8080,8443,B,../data/dataiku\installation\other\azure.md
Which user account does DSS run under by default on the Azure VM instance?,admin,root,dataiku,azureuser,C,../data/dataiku\installation\other\azure.md
Which of the following is NOT a recommended use case for the pre-built Dataiku image on GCP?,Quick starting a DSS instance for testing purposes,Using as an edge node of an existing Hadoop cluster,Running a standalone DSS instance with default options,Testing DSS with pre-made choices and installed packages,B,../data/dataiku\installation\other\gcp.md
What is the default HTTP port on which DSS is available?,22,80,443,8080,B,../data/dataiku\installation\other\gcp.md
Where is the DSS data directory located on the Compute Engine instance?,/var/lib/dss,/usr/local/dss,/home/dataiku/dss,/opt/dataiku/dss,C,../data/dataiku\installation\other\gcp.md
Which of the following is NOT a prerequisite for installing DSS on macOS?,macOS 10.9 'Mavericks' or later,At least 8Gb of RAM,Intel x86-64 or Apple Silicon,At least 16Gb of RAM,D,../data/dataiku\installation\other\osx.md
What is the default TCP base port for DSS installed via the Dataiku Launcher on macOS?,8080,11200,443,3306,B,../data/dataiku\installation\other\osx.md
How does DSS run on Apple Silicon?,Natively,Using Rosetta,Using a virtual machine,It does not run on Apple Silicon,B,../data/dataiku\installation\other\osx.md
What is the minimum recommended amount of RAM to run the DSS virtual machine?,2 GB,4 GB,6 GB,8 GB,D,../data/dataiku\installation\other\vm.md
Which of the following errors indicates that hardware virtualization is not enabled in the BIOS?,VT-x is not available (VERR_VMX_NO_VMX),Connection timeout,Connection refused,DSS is starting,A,../data/dataiku\installation\other\vm.md
Where is the DSS data directory located in the virtual machine?,/usr/local/dss,/opt/dataiku/dss,/home/dataiku/dss,/var/lib/dss,C,../data/dataiku\installation\other\vm.md
Which of the following is NOT true about DSS on Windows?,DSS on Windows is purely experimental and still in its early stages.,DSS on Windows is meant for production usage.,Dataiku will not provide support on this platform.,DSS can be installed on Windows 10 or later through the Dataiku Launcher.,B,../data/dataiku\installation\other\windows.md
What is the minimum amount of RAM required to install DSS on Windows?,4Gb,6Gb,8Gb,10Gb,C,../data/dataiku\installation\other\windows.md
Where are the logs stored for the Dataiku Launcher on Windows?,%LOCALAPPDATA%/Dataiku/DataScienceStudio/launcher.log,%LOCALAPPDATA%/Dataiku/DataScienceStudio/dss_home/logs,%LOCALAPPDATA%/Dataiku/DataScienceStudio/Python/logs,%LOCALAPPDATA%/Dataiku/DataScienceStudio/Java/logs,A,../data/dataiku\installation\other\windows.md
Which of the following search strategies starts like a Random search but uses a predictive model to refine the search?,Grid search,Random search,Bayesian search,Simple split cross-validation,C,../data/dataiku\machine-learning\advanced-optimization.md
"In K-Fold cross-validation, how is the training set split by default?",Into two equally sized portions,Into K equally sized portions,Randomly without any specific portions,According to a time variable,B,../data/dataiku\machine-learning\advanced-optimization.md
What is the main advantage of using a stratified group K-Fold over a group K-Fold?,It balances the folds by keeping the number of distinct groups approximately the same in each fold.,It balances the folds by keeping the target distribution approximately the same in each fold.,It increases the training time significantly.,It ignores the random seed parameter.,B,../data/dataiku\machine-learning\advanced-optimization.md
Which of the following prediction styles in DSS prioritizes variety and speed over pure performance?,Quick Prototypes,Interpretable Models,High Performance,Expert Mode,A,../data/dataiku\machine-learning\auto-ml.md
What type of models does DSS choose when the 'Interpretable Models' prediction style is selected?,Tree-based models with deep hyper-parameter optimization,A variety of models prioritizing speed,Decision trees and linear models,Custom models written by the user,C,../data/dataiku\machine-learning\auto-ml.md
What is a key feature of the 'High Performance' prediction style in DSS?,It focuses on giving 'white-box' models,It prioritizes variety and speed,It involves a very deep hyper-parameter optimization search,It allows for semi-automatic massive features generation,C,../data/dataiku\machine-learning\auto-ml.md
Which programming languages can be used to code custom models in DSS?,Java and Python,Python and Scala,R and Scala,Java and R,B,../data/dataiku\machine-learning\custom-models.md
What is a potential issue if the test dataset is too small?,The model may overfit.,The performance measurement may not be reliable.,The training speed will be slower.,The model will always predict the mean.,B,../data/dataiku\machine-learning\diagnostics.md
Which statistical test is used to assess if the target distribution for the test set is drawn from the same distribution as that of the training set for classification tasks?,Kolmogorov-Smirnov test,Chi-squared test,T-test,ANOVA,B,../data/dataiku\machine-learning\diagnostics.md
What is an indicator that data leakage might have occurred?,Low R2 score,High number of tree leaves,Extremely high performance metric such as > 98% AUC,Slow training speed,C,../data/dataiku\machine-learning\diagnostics.md
Which of the following is NOT a limitation of ensembling models in DSS?,Only models trained with the same preprocessing settings can be ensembled.,It is possible to ensemble models with different time ordering or weight parameters.,It is not possible to ensemble models trained using K-Fold cross-test.,It is not possible to ensemble computer vision models.,B,../data/dataiku\machine-learning\ensembles.md
What must be true for models to be ensembled in DSS?,They must be trained with the same preprocessing settings.,They must be trained with different time ordering.,They must include vector or image features.,They must be partitioned models.,A,../data/dataiku\machine-learning\ensembles.md
Which of the following is a type of machine learning mentioned in the context?,Supervised ML,Reinforcement Learning,Semi-supervised Learning,Transfer Learning,A,../data/dataiku\machine-learning\index.md
What is the purpose of the 'Model Document Generator' in DSS?,To generate predictions,To document the model,To optimize the model,To export the model,B,../data/dataiku\machine-learning\index.md
Which of the following is NOT listed as a feature or tool in DSS?,Automated machine learning,Model Settings Reusability,Reinforcement Learning,Time Series Forecasting,C,../data/dataiku\machine-learning\index.md
Which of the following is NOT a use case supported by Dataiku DSS for collaborative labeling?,Record labeling,Image labeling,Text labeling,Audio labeling,D,../data/dataiku\machine-learning\labeling.md
"In Dataiku DSS, what is the default output mode for the Labels Dataset?",All annotations,Only validated annotations,Annotations with comments,Annotations with conflicts,B,../data/dataiku\machine-learning\labeling.md
What is the role of a Reviewer in the Dataiku DSS labeling process?,To annotate data,To manage profiles and permissions,To validate annotations and resolve conflicts,To specify input data to be labeled,C,../data/dataiku\machine-learning\labeling.md
Which of the following is NOT a limitation of the Model Document Generator?,It is not compatible with ensemble models.,It is not compatible with Keras or computer vision models.,It is not compatible with models imported from MLflow.,It is not compatible with regression models.,D,../data/dataiku\machine-learning\model-document-generator.md
What must be activated on your DSS instance to use the Model Document Generator?,Graphical export library,Dataiku API,Machine learning module,Data preparation module,A,../data/dataiku\machine-learning\model-document-generator.md
Which placeholder would you use to display the name of the algorithm used to compute the model?,design.algorithm.name,design.target.name,design.features_count.value,design.model_type.name,A,../data/dataiku\machine-learning\model-document-generator.md
Which of the following export options is NOT available for models in Dataiku?,Export to Python,Export as a Snowflake function,Export to a CSV file,Export a PMML file,C,../data/dataiku\machine-learning\models-export.md
What is required for a model to be compatible with Java export in Dataiku?,The model must be trained using the 'In-memory (Python)' engine,The model must be compatible with Local (Optimized) scoring,The model must be a computer vision model,The model must have a preparation script,B,../data/dataiku\machine-learning\models-export.md
Which of the following algorithms is NOT compatible with PMML export in Dataiku?,Logistic Regression,Random Forest,Gradient tree boosting (excluding XGBoost),Support Vector Machine,D,../data/dataiku\machine-learning\models-export.md
"In Dataiku DSS, where are machine learning models designed, trained, explored, and selected?",Flow,Lab,API node,Scoring recipe,B,../data/dataiku\machine-learning\models-lifecycle.md
What is a Saved Model in Dataiku DSS used for?,Generating daily reports,Real-time APIs and Scoring recipes,Building dashboards,Data cleaning,B,../data/dataiku\machine-learning\models-lifecycle.md
Which of the following is NOT a limitation of partitioned models?,Only Visual Machine Learning with Python backend is supported.,Clustering models are supported.,All partitions must have enough data samples to properly train a model.,The SQL engine does not support the Partition dispatch mode of the scoring recipe.,B,../data/dataiku\machine-learning\partitioned.md
What must be included in the input dataset to the scoring recipe when using Partition dispatch mode?,Partitioning columns in the data.,Only the target variable.,Only the features without partitioning columns.,A mix of partitioned and unpartitioned data.,A,../data/dataiku\machine-learning\partitioned.md
Which of the following scoring engines provides the best performance and is automatically used whenever possible?,Python engine,Optimized scorer,Spark engine,SQL (Regular) engine,B,../data/dataiku\machine-learning\scoring-engines.md
Which preprocessing option is NOT available for SQL (Regular) scoring?,Rescaling,Binning,Datetime cyclical encoding,Imputation,C,../data/dataiku\machine-learning\scoring-engines.md
Which engine is used to score rows using the API node?,Spark engine,SQL (Regular) engine,Local engine,SQL (Snowflake with Java UDF) engine,C,../data/dataiku\machine-learning\scoring-engines.md
Which of the following actions allows you to reuse model settings in Dataiku?,Duplicating a Modeling Task,Changing the Prediction type,Re-detecting the settings,Deleting a Modeling Task,A,../data/dataiku\machine-learning\task-reuse.md
What happens when you change the target of a predictive Modeling Task in Dataiku?,All settings are reset to default values,You cannot change the target,You can choose to keep the current model settings,The project is deleted,C,../data/dataiku\machine-learning\task-reuse.md
In which sections of a Modeling Task can you copy settings from one task to another in Dataiku?,Target settings and Prediction type,Features handling and Algorithms,Data preparation and Visualization,Model evaluation and Deployment,B,../data/dataiku\machine-learning\task-reuse.md
Which of the following algorithms is NOT used for both regression and classification?,Random Forests,Gradient Boosted Trees,Logistic Regression,Support Vector Machine,C,../data/dataiku\machine-learning\algorithms\in-memory-python.md
"Which of the following is a time series forecasting algorithm that uses an additive model with yearly, weekly, and daily seasonality?",AutoARIMA,Prophet,DeepAR,Seasonal naive,B,../data/dataiku\machine-learning\algorithms\in-memory-python.md
Which clustering algorithm views clusters as areas of high density separated by areas of low density and can find clusters of any shape?,K-means,Gaussian Mixture,DBSCAN,Agglomerative Clustering,C,../data/dataiku\machine-learning\algorithms\in-memory-python.md
Which of the following machine learning engines are supported by DSS’s visual machine learning?,In-memory Python,MLLib (Spark) engine,TensorFlow,Scikit-learn,"A, B",../data/dataiku\machine-learning\algorithms\index.md
What is the process of applying trained models to new records to make predictions called?,Training,Scoring,Validation,Testing,B,../data/dataiku\machine-learning\algorithms\index.md
Which of the following algorithms is NOT supported by DSS 13 on MLLib for classification?,Logistic Regression,Naive Bayes,Gradient Boosted Trees,KMeans,D,../data/dataiku\machine-learning\algorithms\mllib.md
What is a specific limitation of Gradient Boosted Trees in MLLib?,It does not support categorical handling other than dummy encoding.,It does not output per-class probabilities.,It does not support K-fold cross-test.,It does not support containerized execution.,B,../data/dataiku\machine-learning\algorithms\mllib.md
Which of the following is NOT a supported feature preprocessing option in MLLib?,"Tokenize, hash & count for text handling",Dummy encoding for categorical handling,Feature combinations,Regular numerical handling,C,../data/dataiku\machine-learning\algorithms\mllib.md
Which of the following meta-learners in Dataiku trains a single model with the treatment variable as an input feature?,S-learner,T-learner,X-learner,Causal forest,A,../data/dataiku\machine-learning\causal-prediction\causal-prediction-algorithms.md
"In the context of Causal Forests in Dataiku, what does the 'honest' framework do?",Uses a single subset of the train set to compute the optimal split and the value of a node,Uses two separate subsets of the train set to compute the optimal split and the value of a node,Uses the entire dataset to compute the optimal split and the value of a node,Uses a random subset of the train set to compute the optimal split and the value of a node,B,../data/dataiku\machine-learning\causal-prediction\causal-prediction-algorithms.md
Which of the following is NOT a parameter that can be optimized in a Causal Forest model in Dataiku?,Number of trees,Feature sampling strategy,Maximum depth of tree,Learning rate,D,../data/dataiku\machine-learning\causal-prediction\causal-prediction-algorithms.md
What must the input dataset for the evaluation recipe contain?,The outcome column and the treatment column,The outcome column and the control column,The treatment column and the control column,The outcome column and the propensity column,A,../data/dataiku\machine-learning\causal-prediction\evaluation.md
What does the evaluation recipe compute based on the input data and the saved causal model?,The predicted effect,The control effect,The propensity score,The treatment size,A,../data/dataiku\machine-learning\causal-prediction\evaluation.md
Which of the following is a method used in causal prediction algorithms?,Meta-learning,Random forest,Support vector machines,K-means clustering,A,../data/dataiku\machine-learning\causal-prediction\index.md
What type of curves are used to evaluate the performance of a causal prediction model?,ROC and AUC curves,Precision-Recall curves,Uplift and Qini curves,Learning curves,C,../data/dataiku\machine-learning\causal-prediction\index.md
Which setting is NOT part of the Causal Prediction Settings?,Outcome & Treatment,Train / Test set,Metrics,Feature Engineering,D,../data/dataiku\machine-learning\causal-prediction\index.md
What is the main focus of the Causal Prediction modeling task?,Predicting the treatment effect,Predicting the outcome variable,Predicting the counterfactual outcome,Predicting the average treatment effect,A,../data/dataiku\machine-learning\causal-prediction\introduction.md
Which of the following is NOT compatible with causal prediction?,MLflow models,Binary treatments,Multi-valued treatments,Binary outcome variables,A,../data/dataiku\machine-learning\causal-prediction\introduction.md
What types of outcome variables are supported for classification tasks in causal prediction?,Numerical and categorical,Only numerical,Only binary categorical,Multi-valued categorical,C,../data/dataiku\machine-learning\causal-prediction\introduction.md
What does the 'feature importance' measure in a causal model with a binary treatment variable?,The accuracy of the model,The total reduction of the splitting criterion brought by that feature,The number of times a feature is selected for splitting,The overall performance of the model,B,../data/dataiku\machine-learning\causal-prediction\results.md
Which of the following is NOT a purpose of the 'Distribution of the predicted effect' histogram?,Identifying subpopulations reacting differently to the treatment,Determining the main range of the predicted effects,Assessing the accuracy of the model,Detecting the existence of extreme cases (outliers),C,../data/dataiku\machine-learning\causal-prediction\results.md
What does a p-value lower than 0.05 indicate in the Treatment Randomization test?,The treatment was perfectly randomized,The null hypothesis should be rejected with 95% confidence,The model's accuracy is below a dummy model,The treatment was not randomized,B,../data/dataiku\machine-learning\causal-prediction\results.md
What is the purpose of the causal scoring recipe?,To compute the predicted treatment effect based on the input data and the saved causal model.,To predict the probability of receiving the treatment on each row.,To categorize statistical tests.,To generate a daily report of the predicted risks of loan default.,A,../data/dataiku\machine-learning\causal-prediction\scoring.md
Which of the following is NOT required as input for the causal scoring recipe?,Outcome variable,Treatment variable,Input data,Saved causal model,A,../data/dataiku\machine-learning\causal-prediction\scoring.md
What does the propensity scoring recipe compute if a propensity model was trained?,Predicted treatment effect,Probability of receiving the treatment on each row,Exact ratio of the population to be scored,Causal model,B,../data/dataiku\machine-learning\causal-prediction\scoring.md
Which of the following metrics is used to evaluate the performance of causal prediction models in Dataiku?,Area under the uplift curve (AUUC),Mean Squared Error (MSE),F1 Score,Precision,A,../data/dataiku\machine-learning\causal-prediction\settings.md
What is the purpose of the propensity model in causal prediction settings?,To predict the outcome variable,To predict the treatment variable,To split the dataset into train and test sets,To optimize hyperparameters,B,../data/dataiku\machine-learning\causal-prediction\settings.md
Which of the following is NOT a hypothesis related to the treatment variable in causal predictions?,Unconfoundedness,Randomization,Positivity,Normality,D,../data/dataiku\machine-learning\causal-prediction\settings.md
Which of the following pre-trained models is used for object detection?,EfficientNet B0,EfficientNet B4,EfficientNet B7,Faster R-CNN with ResNet-50-FPN backbone,D,../data/dataiku\machine-learning\computer-vision\architecture.md
What is the purpose of using an optimizer during training?,To increase the number of retrained layers,To update the weights of the network based on the loss,To define the number of epochs,To set the batch size,B,../data/dataiku\machine-learning\computer-vision\architecture.md
Which learning rate schedule method is used during the first epoch to start with a smaller learning rate?,On plateau,Step,Exponential,Warmup learning rate schedule,D,../data/dataiku\machine-learning\computer-vision\architecture.md
Which of the following transformations is NOT a type of data augmentation mentioned in the context?,Color transformations,Affine transformations,Crop transformations,Noise transformations,D,../data/dataiku\machine-learning\computer-vision\data-augmentation.md
What happens to bounding box annotations in object detection when a crop transformation is applied?,They are always removed.,They are updated to reflect the transformations.,They remain unchanged.,They are duplicated.,B,../data/dataiku\machine-learning\computer-vision\data-augmentation.md
Which of the following is the supported input format for scoring computer vision models with API endpoints?,JSON with base64 encoded image,XML with binary image data,CSV with image URLs,Plain text with image file paths,A,../data/dataiku\machine-learning\computer-vision\endpoints-api.md
Which metric does the model maximize by default for object detection?,ROC AUC,F1 Score,Accuracy,Log Loss,B,../data/dataiku\machine-learning\computer-vision\evaluation-metrics.md
What is the default metric for image classification models?,Precision,Recall,ROC AUC,F1 Score,C,../data/dataiku\machine-learning\computer-vision\evaluation-metrics.md
When would you want to maximize recall in an object detection model?,When your classes of interest are well-represented in your training data,When your classes of interest are under-represented in your training data,When you want to predict only the most likely objects,When you want to minimize the log loss,B,../data/dataiku\machine-learning\computer-vision\evaluation-metrics.md
Which tab allows you to change the ratio of test samples compared to the training samples in a computer vision model?,Target tab,Training tab,Train / Test tab,Metrics tab,C,../data/dataiku\machine-learning\computer-vision\first-model.md
What is the first step to train an Object detection or Image classification model in Dataiku?,Create the analysis,Install the required packages,Review the design of your model,Monitor the performance of your model,B,../data/dataiku\machine-learning\computer-vision\first-model.md
Which tab allows you to introduce some diversity in your training dataset and visualize examples of those augmentations?,Target tab,Training tab,Train / Test tab,Data augmentation tab,D,../data/dataiku\machine-learning\computer-vision\first-model.md
Which of the following is a supported image format for computer vision analysis in Dataiku?,JPEG,TIFF,BMP,All of the above,D,../data/dataiku\machine-learning\computer-vision\index.md
What is the purpose of the Precision-Recall curve in performance assessment for object detection?,To measure the accuracy of the model,To evaluate the trade-off between precision and recall,To determine the model's training time,To visualize the distribution of data points,B,../data/dataiku\machine-learning\computer-vision\index.md
Which of the following is a pre-trained model available for computer vision tasks in Dataiku?,ResNet,VGG,Inception,All of the above,D,../data/dataiku\machine-learning\computer-vision\index.md
Which of the following is required to create a computer vision analysis in Dataiku?,A dataset with image paths and annotations or categories.,A dataset with only image paths.,A dataset with only annotations or categories.,A dataset with image paths and descriptions.,A,../data/dataiku\machine-learning\computer-vision\inputs.md
What format should the target column be in for an Object detection analysis in Dataiku?,A string representing the category.,An integer representing the category.,A JSON with bounding box coordinates and category.,A list of image paths.,C,../data/dataiku\machine-learning\computer-vision\inputs.md
Which image formats are supported for computer vision in Dataiku DSS?,"jpg, jpeg, png","jpg, jpeg, bmp","png, bmp, tiff","jpeg, bmp, gif",A,../data/dataiku\machine-learning\computer-vision\inputs.md
Which framework is DSS Computer Vision based on?,TensorFlow,PyTorch,Keras,Scikit-learn,B,../data/dataiku\machine-learning\computer-vision\introduction.md
What feature should you use if you want to create a model from scratch with a custom architecture in DSS?,Object Detection,Image Classification,Deep Learning,Interactive Data Augmentation,C,../data/dataiku\machine-learning\computer-vision\introduction.md
"In the context of object detection, what does the 'Not detected' column in the confusion matrix represent?",Predicted objects that have no corresponding ground truth,Ground truth objects that weren’t detected during prediction,Objects that were detected but assigned the wrong class label,Objects that were detected with low confidence scores,B,../data/dataiku\machine-learning\computer-vision\performance-assessment.md
What does the Precision-Recall curve in object detection show?,The average precision for different IOU values,The confidence scores of detected objects,Individual metrics for each detection,The ground truth vs predicted labels,C,../data/dataiku\machine-learning\computer-vision\performance-assessment.md
"In image classification, what information does the confusion matrix provide?",The predicted probabilities for each class,The ground truth vs what the model predicted,The confidence scores of detected objects,The average precision for different IOU values,B,../data/dataiku\machine-learning\computer-vision\performance-assessment.md
What is recommended to significantly reduce the training time of a model?,Using CPUs,Using GPUs,Using TPUs,Using FPGAs,B,../data/dataiku\machine-learning\computer-vision\runtime-gpu.md
What is required for DSS to train or score using GPUs?,At least one CUDA compatible NVIDIA GPU,AMD GPU with OpenCL support,Intel integrated graphics,Any GPU with DirectX support,A,../data/dataiku\machine-learning\computer-vision\runtime-gpu.md
What happens if multiple GPUs are used during training?,"One model will be trained by GPU, each GPU receiving batch_size images to compute the gradient across all models.",Each GPU trains a separate model independently.,Only one GPU is used while the others remain idle.,The training process is slower compared to using a single GPU.,A,../data/dataiku\machine-learning\computer-vision\runtime-gpu.md
Which of the following methods is used to retrieve previously saved models or weights in Keras?,load_model and load_weights,save_model and save_weights,get_model and get_weights,retrieve_model and retrieve_weights,A,../data/dataiku\machine-learning\deep-learning\advanced.md
"In DSS, what is the purpose of the 'build_sequence' method in advanced training mode?",To define custom Keras callbacks,To return sequences used to train the model,To preprocess data in batches,To monitor metrics during training,B,../data/dataiku\machine-learning\deep-learning\advanced.md
Which of the following metrics is NOT tracked by DSS for binary classification?,Accuracy,Precision,R2 Score,ROC AUC,C,../data/dataiku\machine-learning\deep-learning\advanced.md
Which of the following is NOT a necessary step when creating a Deep Learning model in Dataiku?,Writing the architecture of the neural network,Compiling the model,Defining the input shapes,Selecting the training dataset,D,../data/dataiku\machine-learning\deep-learning\architecture.md
What should be the dimension of the last layer for a binary classification model with sigmoid activation?,Equal to the number of target classes,Equal to 1,Equal to 2,Equal to the number of input features,B,../data/dataiku\machine-learning\deep-learning\architecture.md
Which function in Dataiku is responsible for returning an instance of the Keras Model class?,compile_model,build_model,train_model,evaluate_model,B,../data/dataiku\machine-learning\deep-learning\architecture.md
Which of the following steps is NOT part of creating a Deep Learning analysis to solve a Prediction problem in DSS?,Select the Lab,Choose your target variable and Expert Mode,Select 'Visual Deep Learning: Tensorflow' in the code environment,Choose Deep Learning and click Create,C,../data/dataiku\machine-learning\deep-learning\first-model.md
"In the default architecture created by DSS for a Deep Learning model, which API is used to define the model?",PyTorch,Keras,Scikit-learn,TensorFlow,B,../data/dataiku\machine-learning\deep-learning\first-model.md
What is the required shape of the input image when using DSS deep learning for image analysis?,"(197, 197, 1)","(197, 197, 3)","(200, 200, 3)","(197, 197, 4)",B,../data/dataiku\machine-learning\deep-learning\images.md
Which of the following is NOT a valid way to provide image data for scoring using a saved model in DSS?,Relative path to the image stored in the same managed folder used for training,Absolute path to the image stored in any folder,String that is the Base64 representation of the image file,Relative path to the image stored in a different managed folder,B,../data/dataiku\machine-learning\deep-learning\images.md
Which of the following steps is involved in creating your first deep learning model?,Create a code environment with the required packages,Build a Keras model,Using multiple GPUs for training,Using pre-trained models from Keras,A,../data/dataiku\machine-learning\deep-learning\index.md
What is one of the advanced topics covered in deep learning?,Scoring images,Using text features,Start with weights from a previously trained model,Monitor the performance of your model during the training,C,../data/dataiku\machine-learning\deep-learning\index.md
Which of the following is a method to handle multiple inputs in deep learning?,Regular multi-feature inputs,Compile the model,Selection of GPU,TensorFlow session,A,../data/dataiku\machine-learning\deep-learning\index.md
What type of input in DSS receives the preprocessed numerical array from the regular preprocessing of one or multiple input features?,Custom-processed single-feature inputs,Regular multi-feature inputs,Main deep learning input,Recurrent network input,B,../data/dataiku\machine-learning\deep-learning\inputs.md
Which type of deep learning model input in DSS allows a custom preprocessor to create a tensor from a single input feature?,Regular multi-feature inputs,Main deep learning input,Custom-processed single-feature inputs,Recurrent network input,C,../data/dataiku\machine-learning\deep-learning\inputs.md
Which of the following is NOT handled by DSS when building deep learning models?,Preprocessing data,Defining the architecture of the deep learning model,Handling the training process,Integrating with Tensorboard,B,../data/dataiku\machine-learning\deep-learning\introduction.md
What framework is primarily used to define deep learning models in DSS?,PyTorch,Keras,Scikit-learn,Caffe,B,../data/dataiku\machine-learning\deep-learning\introduction.md
Which feature should you check out if you want to create a model using a 'fully-visual' task?,Natural Language Processing,Time Series Analysis,Computer Vision,Reinforcement Learning,C,../data/dataiku\machine-learning\deep-learning\introduction.md
Which of the following statements is true about training Keras models on GPUs?,Training on GPUs is usually slower than on CPUs.,"Training on GPUs is usually faster, especially when images are involved.",Training on GPUs requires changing the model architecture.,Training on GPUs is not supported by DSS.,B,../data/dataiku\machine-learning\deep-learning\runtime-gpu.md
What must you do before training your first deep learning model in DSS?,Install the DSS built-in Python environment.,Create a code environment with the required packages.,Change the model architecture to support GPUs.,Deploy the model as a service endpoint.,B,../data/dataiku\machine-learning\deep-learning\runtime-gpu.md
What strategy does DSS use to manage training on multiple GPUs?,DataParallelStrategy,MirroredStrategy,DistributedStrategy,ParallelTrainingStrategy,B,../data/dataiku\machine-learning\deep-learning\runtime-gpu.md
What is the purpose of the TokenizerProcessor in DSS?,To compute a vocabulary on all the corpus and convert each text to a vector representing the sequence of words.,To generate a daily report of the predicted risks of loan default.,To build a monthly dashboard of the evolution of e-commerce sales.,To predict fraudulent transactions in real-time on a website.,A,../data/dataiku\machine-learning\deep-learning\text.md
Which of the following is a known issue when using pre-trained models from Keras without internet access?,The model will not load.,The code will throw an Exception: URL fetch failure.,The weights will be incorrect.,The architecture will change.,B,../data/dataiku\machine-learning\deep-learning\troubleshooting.md
What is the impact of modifying the 'Memory allocation rate per GPU' in the TensorFlow session settings in DSS?,It changes the number of GPUs used.,It impacts the config.gpu_options.per_process_gpu_memory_fraction of the session.,It disables GPU usage.,It changes the CPU allocation.,B,../data/dataiku\machine-learning\deep-learning\troubleshooting.md
Which type of problems can the Deep Learning in the Prediction part of Visual Machine Learning currently treat?,Object Detection,"Regression, Binary Classification, and Multiclass Classification",Clustering,Time Series Forecasting,B,../data/dataiku\machine-learning\deep-learning\troubleshooting.md
Which of the following encoding methods replaces each category by a numerical value computed based on the target values?,Dummy-encoding,Ordinal encoding,Target encoding,Frequency encoding,C,../data/dataiku\machine-learning\features-handling\categorical.md
"In Impact coding, what does the parameter 'm' control?",The number of categories,The global mean of the target variable,How much the global mean is taken into account when computing the target encoding,The number of rows in the category,C,../data/dataiku\machine-learning\features-handling\categorical.md
Which of the following methods should be used for handling structurally missing data in categorical features?,Impute,Drop rows,Treat as a regular value,Replace by 0/1 flag indicating presence,C,../data/dataiku\machine-learning\features-handling\categorical.md
Which of the following is NOT a valid return type for the transform method in a custom processor?,PANDAS DATAFRAME,2-D NUMPY ARRAY,SCIPY.SPARSE.CSR_MATRIX,PYTHON LIST,D,../data/dataiku\machine-learning\features-handling\custom.md
What must be done to use a custom processor in the visual ML UI?,DECLARE THE CLASS DIRECTLY IN THE MODELS > DESIGN TAB,PACKAGE THE CLASS IN A LIBRARY AND IMPORT IT,ASSIGN THE PROCESSOR TO THE 'PROCESSOR' VARIABLE IN THE CODE EDITOR,ENABLE THE 'PROC. WANTS MATRIX' OPTION,B,../data/dataiku\machine-learning\features-handling\custom.md
What is the purpose of the fit method in a custom processor?,TO RETURN A PANDAS DATAFRAME,TO MODIFY THE OBJECT IN-PLACE IF FITTING IS NECESSARY,TO RETURN A 2-D NUMPY ARRAY,TO ENABLE THE 'PROC. WANTS MATRIX' OPTION,B,../data/dataiku\machine-learning\features-handling\custom.md
Which of the following is NOT a supported method for handling missing values in an image feature?,Drop rows,Fail if missing values found,Impute,Replace with average image,D,../data/dataiku\machine-learning\features-handling\images.md
Which of the following statements about image embedding in DSS is TRUE?,Image embedding feature handling is supported on the API node.,Image embedding creates semantically meaningful vector representation of images.,Image embedding does not require a Local HuggingFace connection.,Image embedding models cannot be used for deep learning models.,B,../data/dataiku\machine-learning\features-handling\images.md
Which type of encoding is used for categorical variables to handle the impact of categories on the target variable?,Ordinal encoding,Frequency encoding,Target encoding,Datetime cyclical encoding,C,../data/dataiku\machine-learning\features-handling\index.md
What type of variables can most machine learning engines in DSS visual machine learning process?,Numerical features with missing values,Categorical features with missing values,Numerical features with no missing values,Text features with missing values,C,../data/dataiku\machine-learning\features-handling\index.md
Which of the following is a method for handling missing values in text variables?,Target encoding,Datetime cyclical encoding,Text embedding,Frequency encoding,C,../data/dataiku\machine-learning\features-handling\index.md
Which of the following methods is NOT a way to handle numerical variables in Dataiku?,Keep as a regular numerical feature,Replace by 0/1 flag indicating presence,Binarize based on a threshold,Replace by a random value,D,../data/dataiku\machine-learning\features-handling\numerical.md
What is the purpose of datetime cyclical encoding in Dataiku?,To convert datetime features into categorical features,To transform datetime features into numerical features while preserving cyclical significance,To remove datetime features from the dataset,To replace datetime features with a constant value,B,../data/dataiku\machine-learning\features-handling\numerical.md
Which rescaling method sets the minimum value of the feature to zero and the max to one?,Standard rescaling,Min-max rescaling,Logarithmic rescaling,Exponential rescaling,B,../data/dataiku\machine-learning\features-handling\numerical.md
Which of the following feature roles means that the feature is not used during machine learning?,Reject,Input,Use for display only,Categorical,A,../data/dataiku\machine-learning\features-handling\roles-and-types.md
What type of variable takes one of an enumerated list of values?,Numerical,Text,Categorical,Vector,C,../data/dataiku\machine-learning\features-handling\roles-and-types.md
Which variable type is specifically mentioned as being available for Deep Learning?,Numerical,Text,Vector,Image,D,../data/dataiku\machine-learning\features-handling\roles-and-types.md
Which of the following text handling methods in DSS produces sparse matrices?,Count vectorization,TF/IDF vectorization,Hashing trick,Text embedding,C,../data/dataiku\machine-learning\features-handling\text.md
What is required in the runtime environment for using text embedding in Visual ML?,sentence-transformers python package,scikit-learn python package,pandas python package,numpy python package,A,../data/dataiku\machine-learning\features-handling\text.md
How are missing values for Text embedding handled in DSS?,Ignored,Imputed with empty strings,Imputed with empty embeddings (zeros),Replaced with the mean value,C,../data/dataiku\machine-learning\features-handling\text.md
Which method should be used to handle randomly missing data in vector features?,Unfold,Impute,Drop rows,Fail if missing values found,B,../data/dataiku\machine-learning\features-handling\vectors.md
What does the 'Unfold' method do when handling vector variables?,Replaces missing values with a specified value,Discards rows with missing values,Creates one column per element in the vector values,Fails if missing values are found,C,../data/dataiku\machine-learning\features-handling\vectors.md
Which method should be avoided unless missing data is extremely rare?,Unfold,Impute,Drop rows,Fail if missing values found,C,../data/dataiku\machine-learning\features-handling\vectors.md
Which of the following methods is NOT used by Dataiku DSS to compute individual prediction explanations?,Shapley values,Individual Conditional Expectation (ICE),Keras/Tensorflow,Custom models and algorithms from plugins,C,../data/dataiku\machine-learning\supervised\explanations.md
What is the primary purpose of individual prediction explanations in Dataiku DSS?,To visualize the global feature importance values,To understand the prediction of an individual row and how certain features impact it,To compute the average prediction of a dataset,To train new machine learning models,B,../data/dataiku\machine-learning\supervised\explanations.md
Which of the following is a limitation of the individual prediction explanations feature in Dataiku DSS?,It can only be used with Keras/Tensorflow models,It is available for all types of machine learning models,It can be very memory consuming when using the Scoring Recipe,It provides explanations for all features in the dataset,C,../data/dataiku\machine-learning\supervised\explanations.md
"In the context of supervised machine learning, what is the 'target' variable?",The variable used to split the dataset,The variable that is predicted,The variable used for feature generation,The variable used for model optimization,B,../data/dataiku\machine-learning\supervised\index.md
Which of the following methods is used for splitting the dataset in supervised machine learning?,Subsampling,Feature reduction,Model optimization,Outcome optimization,A,../data/dataiku\machine-learning\supervised\index.md
Which feature importance method is based on the contribution of each feature to the prediction?,Gini feature importance,Shapley feature importance,Partial dependence,Regression coefficients,B,../data/dataiku\machine-learning\supervised\index.md
Which of the following is a feature of the interactive scoring simulator in Dataiku?,It allows you to tweak input features and see real-time predictions and explanations.,It can only be used with models trained using the R programming language.,It supports explanations for all types of models including Keras and computer vision models.,It does not allow you to compare multiple predictions side-by-side.,A,../data/dataiku\machine-learning\supervised\interactive-scoring.md
What are the default pre-set values for numerical and categorical features in the compute view of the interactive scoring simulator?,Means for numerical features and medians for categorical features.,Modes for numerical features and means for categorical features.,Medians for numerical features and modes for categorical features.,Modes for numerical features and medians for categorical features.,C,../data/dataiku\machine-learning\supervised\interactive-scoring.md
Which of the following is NOT a limitation of the interactive scoring feature in Dataiku?,It is available only for Visual ML models trained using Python or Keras backends.,Explanations are not supported on Keras or computer vision models.,Sorting features by importance is not available for ensembling models.,It can be used with models trained using any backend including R and Java.,D,../data/dataiku\machine-learning\supervised\interactive-scoring.md
Which of the following is NOT a component of the assertion result in ML assertions?,Name of the assertion,Number of matching rows,Number of rows dropped,Model accuracy,D,../data/dataiku\machine-learning\supervised\ml-assertions.md
"In the context of ML assertions, what happens if no row matches the assertion filter?",The valid ratio and assertion result will not be defined.,The model will automatically retrain.,The assertion will be considered as passed.,The assertion will be ignored.,A,../data/dataiku\machine-learning\supervised\ml-assertions.md
Which of the following tasks are ML assertions NOT available for?,Classification tasks,Regression tasks,Clustering tasks,In-memory python backend tasks,C,../data/dataiku\machine-learning\supervised\ml-assertions.md
What is the primary purpose of Model Error Analysis in machine learning?,To improve the speed of model training.,To investigate the model’s failures and build intuition around poorly performing subpopulations.,To increase the accuracy of the primary model.,To reduce the size of the dataset.,B,../data/dataiku\machine-learning\supervised\model-error-analysis.md
What is the Error Tree in the context of Model Error Analysis?,A secondary model trained to predict the primary model's success or failure.,A visualization tool for data distribution.,A method to increase model accuracy.,A technique to reduce data dimensionality.,A,../data/dataiku\machine-learning\supervised\model-error-analysis.md
"In regression tasks, how is a model failure defined in Model Error Analysis?",When the predicted value is exactly equal to the true value.,When the absolute difference between the predicted and true value is higher than a threshold ε.,When the predicted value is less than the true value.,When the predicted value is greater than the true value.,B,../data/dataiku\machine-learning\supervised\model-error-analysis.md
Which of the following criteria is NOT used to evaluate counterfactual explanations?,Proximity,Plausibility,Diversity,Accuracy,D,../data/dataiku\machine-learning\supervised\model-exploration.md
"For binary classifiers, what is the target in counterfactual explanations?",The same class as the reference example,Any class but the reference's,The opposing class to the one predicted for the reference example,A specific class chosen by the user,C,../data/dataiku\machine-learning\supervised\model-exploration.md
Which of the following is NOT a limitation of model exploration?,Not available for models trained before DSS 10,Not available for Keras or computer vision models,Not available for binary classification models,Not available for timeseries,C,../data/dataiku\machine-learning\supervised\model-exploration.md
Which of the following metrics is NOT computed in the Model Fairness Report?,Demographic Parity,Equalized Odds,Equality of Opportunity,Predictive Accuracy,D,../data/dataiku\machine-learning\supervised\model-fairness-report.md
What is the purpose of the 'Sensitive column' in the Model Fairness Report?,To identify the target value that is advantageous,To compute the metric discrepancies based on the reference group,To contain the sensitive group based on which fairness metrics are computed,To display the model's accuracy,C,../data/dataiku\machine-learning\supervised\model-fairness-report.md
Which of the following steps is necessary to use the Model Fairness Report?,Install the plugin and build the code-env for it,Train the model and click on 'Metrics',Select 'Model Accuracy Report' from the 'Views' menu,None of the above,A,../data/dataiku\machine-learning\supervised\model-fairness-report.md
What is the default coverage level for prediction intervals in regression tasks?,90%,95%,99%,85%,B,../data/dataiku\machine-learning\supervised\prediction-intervals.md
What is the purpose of the helper model in generating prediction intervals?,To improve the accuracy of the primary predictive model,To estimate the bounds of the prediction interval by fitting the residuals,To reduce the computational complexity of the primary model,To validate the primary model's predictions,B,../data/dataiku\machine-learning\supervised\prediction-intervals.md
What is the primary purpose of model overrides in machine learning models?,To improve the speed of model training,To add an extra layer of human control over the models’ predictions,To reduce the size of the dataset,To enhance the graphical representation of data,B,../data/dataiku\machine-learning\supervised\prediction-overrides.md
Which of the following is NOT a limitation of model overrides?,Overrides are not available for tasks other than AutoML Prediction tasks,Overrides are available for ensemble models,Variables are not supported when defining override conditions,Python scoring on an API node is not supported when the model is using overrides,B,../data/dataiku\machine-learning\supervised\prediction-overrides.md
What happens when multiple overrides match a single row in a dataset?,All matching overrides are applied,The last matching override applies,The first matching override applies,No override is applied,C,../data/dataiku\machine-learning\supervised\prediction-overrides.md
Which of the following statements about Shapley feature importance is true?,Shapley values are supported for Visual Deep Learning models using the Tensorflow/Keras backend.,Shapley feature importance is always computed automatically regardless of computational cost.,Shapley feature importance can be computed for models trained using the Python backend.,Shapley feature importance is only available for tree-based models.,C,../data/dataiku\machine-learning\supervised\results.md
What does the 'target classes proportions' describe in a classification tree?,The weighted proportion of classes at each node.,The raw distribution of classes at each node.,The probability of the target class at each node.,The log-odds of the target class at each node.,B,../data/dataiku\machine-learning\supervised\results.md
Which of the following is a limitation of Gini feature importance?,It can only be computed for linear models.,It tends to overemphasize numerical features with many different values.,It is not available for LightGBM models.,It always results in negative values.,B,../data/dataiku\machine-learning\supervised\results.md
Which of the following prediction types is used when the target is numeric?,Regression,Two-class classification,Multi-class classification,Clustering,A,../data/dataiku\machine-learning\supervised\settings.md
What is the default ratio for splitting the dataset into training and test sets in DSS?,70% training and 30% testing,80% training and 20% testing,60% training and 40% testing,90% training and 10% testing,B,../data/dataiku\machine-learning\supervised\settings.md
Which feature reduction method uses Principal Component Analysis to reduce the dimension of the feature space?,Correlation with target,Tree-based,Principal Component Analysis,Lasso regression,C,../data/dataiku\machine-learning\supervised\settings.md
Which of the following is NOT a required column in the input dataset for the evaluation recipe?,The time column,The time series identifiers columns,The target column,The metrics column,D,../data/dataiku\machine-learning\time-series-forecasting\evaluation.md
What does the evaluation recipe compute by moving the forecast/evaluation window from the end of the input dataset to the beginning?,The input dataset,The evaluation dataset,The metrics dataset,The time series identifiers,B,../data/dataiku\machine-learning\time-series-forecasting\evaluation.md
Which statistical model requires refitting to be enabled for evaluation to work?,ARIMA,Seasonal LOESS,Linear Regression,Random Forest,B,../data/dataiku\machine-learning\time-series-forecasting\evaluation.md
Which section would you refer to for understanding how to split your data into training and testing sets for time series forecasting?,Introduction,Settings: Train / Test set,Runtime and GPU support,Scoring recipe,B,../data/dataiku\machine-learning\time-series-forecasting\index.md
Where can you find information about the performance metrics used in time series forecasting?,Time Series Forecasting Settings,Runtime and GPU support,Time Series Forecasting Results,Evaluation recipe,C,../data/dataiku\machine-learning\time-series-forecasting\index.md
Which section would you consult to learn about using external features in your time series forecasting model?,Settings: External features,Scoring recipe,Runtime and GPU support,Evaluation recipe,A,../data/dataiku\machine-learning\time-series-forecasting\index.md
Which of the following is NOT compatible with time series forecasting in Dataiku?,MLflow models,Model export to Java,Model Document Generator,All of the above,D,../data/dataiku\machine-learning\time-series-forecasting\introduction.md
What is required in your dataset for time series forecasting models in Dataiku?,A uniform time step,A time variable with meaning 'Date',Multiple identifier columns,A GPU with CUDA,B,../data/dataiku\machine-learning\time-series-forecasting\introduction.md
Which of the following metrics is aggregated by first computing the mean of each quantile loss across time series and then computing the mean across all quantiles?,Mean Absolute Scaled Error (MASE),Mean Absolute Quantile Loss (MAQL),Mean Squared Error (MSE),Root Mean Squared Error (RMSE),B,../data/dataiku\machine-learning\time-series-forecasting\results.md
What does the model report contain when a time series forecasting model finishes training?,A list of hyperparameters used,A visualization of the time series forecast vs. the ground truth of the target variable,A summary of the training data,A list of all the algorithms tested,B,../data/dataiku\machine-learning\time-series-forecasting\results.md
"If K-Fold cross-test is used for evaluation, how are the aggregated metrics averaged?",By taking the median of all folds,By ignoring folds that yield undefined metric values,By summing the metrics of all folds,By taking the maximum value across all folds,B,../data/dataiku\machine-learning\time-series-forecasting\results.md
Which of the following is required to train or score a time series forecasting model on a GPU in DSS?,At least one CUDA compatible NVIDIA GPU.,The GPU drivers installed with a compatible CUDA version.,A compatible version of NVIDIA NCCL and CuDNN installed.,All of the above.,D,../data/dataiku\machine-learning\time-series-forecasting\runtime-gpu.md
Which Python libraries are specifically mentioned as being used for time series forecasting in DSS?,TensorFlow and Keras,GluonTS and MxNet,PyTorch and Scikit-learn,Pandas and NumPy,B,../data/dataiku\machine-learning\time-series-forecasting\runtime-gpu.md
What must the input dataset contain if the model was trained with external features?,Only the past time steps for the time column and target column.,"Past time steps for the time column, time series identifiers columns, target column, and external features columns.",Only the future time steps for the time column and external features columns.,Only the past time steps for the time column and external features columns.,B,../data/dataiku\machine-learning\time-series-forecasting\scoring.md
What is required for scoring with Seasonal LOESS?,Refitting must be disabled.,Refitting must be enabled.,External features must be included.,The target column must be empty.,B,../data/dataiku\machine-learning\time-series-forecasting\scoring.md
Which of the following is NOT a type of forecasting algorithm supported by DSS?,Baseline algorithms,Statistical algorithms,Deep learning algorithms,Genetic algorithms,D,../data/dataiku\machine-learning\time-series-forecasting\settings.md
What is the purpose of the 'gap' parameter in time series forecasting?,To define the time step used for resampling,To specify the number of skipped time steps for model evaluation,To determine the forecasting horizon,To select external features for the model,B,../data/dataiku\machine-learning\time-series-forecasting\settings.md
What is the main advantage of using K-Fold cross-test in time series forecasting?,It reduces the training time significantly,It allows for a more accurate estimation of model performance,It eliminates the need for a test set,It simplifies the model training process,B,../data/dataiku\machine-learning\time-series-forecasting\settings.md
Which of the following steps is NOT part of accessing unsupervised machine learning in DSS?,Go to the Flow for your project,Click on the dataset you want to use,Select the *Lab*,Select *AutoML Regression*,D,../data/dataiku\machine-learning\unsupervised\index.md
What is a key difference between supervised and unsupervised machine learning?,Supervised machine learning requires a target variable.,Unsupervised machine learning requires a target variable.,Supervised machine learning does not require a target variable.,Both require a target variable.,A,../data/dataiku\machine-learning\unsupervised\index.md
Which of the following is a method for detecting outliers in clustering according to DSS?,Performing a pre-clustering with a large number of clusters and considering the smallest 'mini-clusters' as outliers.,Using a decision tree to identify outliers.,Applying a linear regression model to detect outliers.,Using a neural network to identify outliers.,A,../data/dataiku\machine-learning\unsupervised\settings.md
What is the main interest of using PCA for clustering?,To improve the accuracy of the clustering algorithm.,To reduce the number of variables by arranging them into principal components.,To increase the number of dimensions in the dataset.,To eliminate the need for sampling.,B,../data/dataiku\machine-learning\unsupervised\settings.md
Which of the following is NOT a sampling method mentioned for clustering in DSS?,Sampling from the beginning of a dataset.,Randomly sampling from the entire dataset.,Sampling based on the median value of the dataset.,Extracting a sample if the dataset does not fit in RAM.,C,../data/dataiku\machine-learning\unsupervised\settings.md
Which of the following is NOT a component of a 'table' in the metastore?,A location of the files making up the data,A schema (column names and types),A storage format indicating the file format of the data files,A list of users with access permissions,D,../data/dataiku\metastore\index.md
Which of the following engines and features in DSS does NOT leverage the metastore to perform computations?,Hive recipes and notebooks,Impala recipes and notebooks,SparkSQL notebooks,Python scripts,D,../data/dataiku\metastore\index.md
Which type of metastore can DSS leverage if you run on AWS?,Hive metastore,Glue metastore,DSS virtual metastore,Azure metastore,B,../data/dataiku\metastore\index.md
Which of the following is NOT an outcome of a check in DSS?,OK,ERROR,WARNING,INVALID,D,../data/dataiku\metrics-check-data-quality\checks.md
What type of check asserts that the value of a metric is within a given range?,Value in set check,Python check,Numeric range check,Status check,C,../data/dataiku\metrics-check-data-quality\checks.md
Where are checks configured in DSS?,In the 'Settings' tab of managed folders,"In the 'Status' tab of managed folders, saved models, and evaluation stores",In the 'Metrics' tab of managed folders,In the 'Scenarios' tab of managed folders,B,../data/dataiku\metrics-check-data-quality\checks.md
What is the primary purpose of a custom probe in Dataiku?,To visualize data,To perform complex computations on datasets or folders,To manage user permissions,To create dashboards,B,../data/dataiku\metrics-check-data-quality\custom_metrics_and_checks.md
Which of the following is NOT a valid return type for a custom probe in Dataiku?,DOUBLE,BOOLEAN,BIGINT,FLOAT,D,../data/dataiku\metrics-check-data-quality\custom_metrics_and_checks.md
"In the context of custom checks in Dataiku, what is advised to be included to distinguish the values they produce?",A unique identifier,A timestamp,A meaningful name,A version number,C,../data/dataiku\metrics-check-data-quality\custom_metrics_and_checks.md
Which of the following is NOT a possible status outcome of a Data Quality rule in Dataiku?,OK,ERROR,WARNING,INVALID,D,../data/dataiku\metrics-check-data-quality\data-quality-rules.md
What does the 'Auto-run after build' option do in Dataiku's Data Quality rules?,It disables the rule after dataset build.,It automatically computes the rule after each build of the dataset.,It deletes the rule after dataset build.,It sends an email notification after dataset build.,B,../data/dataiku\metrics-check-data-quality\data-quality-rules.md
Which of the following Data Quality rule types ensures that a column's values are unique?,Column values are not empty,Column values are empty,Column values are unique,Column values are valid according to meaning,C,../data/dataiku\metrics-check-data-quality\data-quality-rules.md
Which of the following actions can be performed from the Data Quality Rule edition panel of any dataset?,Creating a new template from scratch,Creating a template from the dataset's rules,Deleting a template,Managing template security settings,B,../data/dataiku\metrics-check-data-quality\data-quality-templates.md
What happens when you use a template on a dataset from the Data Quality rule edition page?,It creates a new dataset,It adds all rules from the template to the dataset,It deletes the existing rules in the dataset,It merges the dataset with the template,B,../data/dataiku\metrics-check-data-quality\data-quality-templates.md
"Who can discover, create, update, delete, and use any template in the instance?",Only administrators,Only the template creator,Any authenticated user,Only users with special permissions,C,../data/dataiku\metrics-check-data-quality\data-quality-templates.md
Which of the following is an improvement over the check mechanism for datasets in Dataiku?,Metrics,Data Quality rules,Custom probes,SQL probes,B,../data/dataiku\metrics-check-data-quality\index.md
What type of Data Quality rule would you use to ensure that a column's values are not empty?,Column min in range,Column values are unique,Column values are not empty,Column top N values in set,C,../data/dataiku\metrics-check-data-quality\index.md
Which of the following is NOT a type of Data Quality monitoring view in Dataiku?,Dataset level view,Project level view,Instance level view,User level view,D,../data/dataiku\metrics-check-data-quality\index.md
Which of the following probes computes the number and ratio of invalid data in a column?,Basic column statistics,Advanced column statistics,Column data validity statistics,Records,C,../data/dataiku\metrics-check-data-quality\metrics.md
What is the purpose of the 'tile' view in the Metrics display UI?,To display the history of all selected metrics,To display the latest value of each selected metric,To show the last values of several metrics on all partitions,To display the last values of each metric on all partitions as a chart,B,../data/dataiku\metrics-check-data-quality\metrics.md
Which execution engine is generally much slower because it needs to move all of the data?,Hive,Impala,SQL database,Streaming data engine,D,../data/dataiku\metrics-check-data-quality\metrics.md
Which of the following is NOT a capability offered by Dataiku for implementing a complete MLOps practice?,Curating features in a Feature Store,Analyzing drift,Building a monthly dashboard of the evolution of e-commerce sales,Tracking code experiments,C,../data/dataiku\mlops\index.md
Which of the following capabilities is specifically focused on evaluating and comparing models and model versions in Dataiku?,Feature Store,Model Comparisons,Drift analysis,Unified Monitoring,B,../data/dataiku\mlops\index.md
What is Model Drift?,A change in the statistic distribution of features over time.,A modification of the relationship between features and the target.,A negative impact on model behavior due to changes in real-life conditions compared to training conditions.,A process of gathering new data from production environments.,C,../data/dataiku\mlops\drift-analysis\index.md
Which of the following is NOT a type of drift analysis available in the 'Drift' section of a Model Evaluation Store?,Data Drift,Prediction Drift,Performance Drift,Feature Drift,D,../data/dataiku\mlops\drift-analysis\index.md
What is necessary to monitor model drift effectively?,Using the same data from the training environment.,Gathering new data from production environments and possibly having ground truth associated with it.,Only monitoring the model's performance metrics.,Ignoring changes in the statistic distribution of features.,B,../data/dataiku\mlops\drift-analysis\index.md
Which of the following recipes is used to compute pure data drift without involving a model?,Evaluate Recipe,Standalone Evaluate Recipe,Model Evaluation Recipe,Drift Detection Recipe,B,../data/dataiku\mlops\drift-analysis\input-data-drift.md
What type of drift metrics are computed for text features in Dataiku?,"Euclidean distance, Cosine similarity, and a dedicated Classifier","Mean Squared Error, R-squared, and a dedicated Classifier","Pearson Correlation, Spearman Correlation, and a dedicated Classifier","Jaccard Index, Hamming Distance, and a dedicated Classifier",A,../data/dataiku\mlops\drift-analysis\input-data-drift.md
"In the context of Dataiku, which recipe is most commonly used for model monitoring?",Standalone Evaluate Recipe,Evaluate Recipe,Model Evaluation Recipe,Drift Detection Recipe,B,../data/dataiku\mlops\drift-analysis\input-data-drift.md
What is required for Performance Drift analysis?,Input Data Drift,Prediction Drift,Ground truth / labels,Model evaluations,C,../data/dataiku\mlops\drift-analysis\performance-drift.md
What does Prediction Drift analyze?,The accuracy of the model on training data.,The distribution of predictions on the evaluated data.,The computational efficiency of the model.,The correlation between features in the dataset.,B,../data/dataiku\mlops\drift-analysis\prediction-drift.md
What could a significant change in the distribution of predictions indicate?,An improvement in model performance.,A concept drift in the underlying data.,A reduction in computational cost.,An increase in data quality.,B,../data/dataiku\mlops\drift-analysis\prediction-drift.md
Is having ground truth/labels required for Prediction Drift analysis?,"Yes, it is always required.","No, it is not required.",Only for supervised learning models.,Only for unsupervised learning models.,B,../data/dataiku\mlops\drift-analysis\prediction-drift.md
Which of the following is the default reference for drift comparison in a Model Evaluation Store?,Another Model Evaluation in the same store,Another Model Evaluation in another store,The evaluation created automatically when training the Saved Model used for the current Model Evaluation,The evaluation created automatically when training a model in a Visual Analysis,C,../data/dataiku\mlops\drift-analysis\reference.md
What is the reference dataset used for model monitoring in Dataiku?,The entire training dataset,The test part of the training dataset,The validation dataset,The input dataset content when the Evaluate recipe runs,B,../data/dataiku\mlops\drift-analysis\sampling.md
Which of the following statements is true about data drift in Dataiku?,Data drift considers only the current dataset.,Data drift considers only the reference dataset.,Data drift considers both the reference and current datasets.,Data drift does not consider any datasets.,C,../data/dataiku\mlops\drift-analysis\sampling.md
What is the purpose of the global score in data drift analysis?,To differentiate the reference and current datasets using balanced data,To differentiate the reference and current datasets using imbalanced data,To compute the model performance metrics,To compute prediction drift,A,../data/dataiku\mlops\drift-analysis\sampling.md
Which API does DSS use for experiment tracking?,TensorFlow,PyTorch,MLflow Tracking,Keras,C,../data/dataiku\mlops\experiment-tracking\concepts.md
What does a 'Run' in an experiment typically store? (Choose all that apply),Parameters,Metrics,Artefacts,Logs,"A, B, C",../data/dataiku\mlops\experiment-tracking\concepts.md
What is one of the DSS specific benefits of integrating MLflow Tracking?,Run artefacts are stored in a DSS managed folder,Experiments can be run without any security,Only supports TensorFlow models,Does not support model deployment,A,../data/dataiku\mlops\experiment-tracking\concepts.md
Which of the following methods can be used to deploy an MLflow model in Dataiku?,Through the 'Run details' screen by clicking the 'Deploy' button,Using the API with dataikuapi.dss.mlflow.DSSMLflowExtension.deploy_run_model(),By setting the prediction type to 'Other',All of the above,D,../data/dataiku\mlops\experiment-tracking\deploying.md
What must be specified in the same order as learned by the model when deploying a multi-class model?,The dataset,The classes,The experiment ID,The run ID,B,../data/dataiku\mlops\experiment-tracking\deploying.md
Which method would you use to list all models in Dataiku's MLflow extension?,list_experiments(),list_models(),rename_experiment(),restore_run(),B,../data/dataiku\mlops\experiment-tracking\extensions.md
What is the purpose of the method 'garbage_collect()' in Dataiku's MLflow extension?,To list all models,To rename experiments,To clear experiments marked for deletion,To deploy a model from a run,C,../data/dataiku\mlops\experiment-tracking\extensions.md
Which method is used to create a DSS dataset of the experiment tracking runs of a project?,clean_experiment_tracking_db(),set_run_inference_info(),create_experiment_tracking_dataset(),deploy_run_model(),C,../data/dataiku\mlops\experiment-tracking\extensions.md
Which API does Dataiku DSS use for code-based experiment tracking?,TensorFlow,PyTorch,MLflow Tracking,Scikit-learn,C,../data/dataiku\mlops\experiment-tracking\index.md
What is the primary purpose of experiment tracking in Dataiku?,To visualize data,To save all experiment-related information,To deploy models,To clean data,B,../data/dataiku\mlops\experiment-tracking\index.md
What is the purpose of creating a Managed Folder in a Dataiku project before tracking experiments?,To store artefacts,To manage user permissions,To organize datasets,To schedule tasks,A,../data/dataiku\mlops\experiment-tracking\tracking.md
"Which feature of MLflow Tracking automatically logs metrics, parameters, and models for common machine-learning packages?",Manual Logging,Autologging,Context Manager,Experiment Tracking,B,../data/dataiku\mlops\experiment-tracking\tracking.md
What is the recommended way to use MLflow Tracking integration in Dataiku?,Using the context manager ('with' statement),Using manual logging,Using a separate logging script,Using a third-party tool,A,../data/dataiku\mlops\experiment-tracking\tracking.md
What does the main screen of the Experiment Tracking UI display by default?,All experiments,Active experiments,Experiments marked for deletion,Completed experiments,B,../data/dataiku\mlops\experiment-tracking\viewing.md
Which of the following is NOT a format available for creating a DSS dataset from experiment tracking runs?,Long format,JSON format,CSV format,None of the above,C,../data/dataiku\mlops\experiment-tracking\viewing.md
What is the purpose of the 'Clear Deleted Experiments' project macro?,To restore deleted experiments,To permanently delete items marked for deletion,To archive experiments,To create a backup of experiments,B,../data/dataiku\mlops\experiment-tracking\viewing.md
Which of the following cloud vendors are supported for deploying External Models in DSS?,"Amazon SageMaker, Azure Machine Learning, Google Vertex AI, Databricks","Amazon SageMaker, IBM Watson, Google Vertex AI, Databricks","Amazon SageMaker, Azure Machine Learning, IBM Watson, Databricks","Amazon SageMaker, Azure Machine Learning, Google Vertex AI, IBM Watson",A,../data/dataiku\mlops\external-models\index.md
Which of the following is NOT a capability provided by DSS for managing External Models?,Scoring datasets using a scoring recipe,Managing multiple versions of the models,Exporting External Models in an API package,Analyzing drift on the External Model,C,../data/dataiku\mlops\external-models\index.md
What is the first step an Administrator must take before creating an External Model in DSS?,Create a new project,Create the 'External Models' code environment,Deploy the model on a cloud vendor,Evaluate the model's performance,B,../data/dataiku\mlops\external-models\index.md
Which of the following is a supported input format for Amazon SageMaker?,INPUT_SAGEMAKER_CSV,INPUT_AZUREML_JSON_INPUTDATA,INPUT_VERTEX_DEFAULT,INPUT_DATABRICKS_CSV,A,../data/dataiku\mlops\external-models\input-output-formats.md
Which of the following is a supported output format for Azure Machine Learning?,OUTPUT_SAGEMAKER_JSON,OUTPUT_VERTEX_DEFAULT,OUTPUT_AZUREML_JSON_OBJECT,OUTPUT_DATABRICKS_JSON,C,../data/dataiku\mlops\external-models\input-output-formats.md
Which of the following is a supported input format for Databricks?,INPUT_SAGEMAKER_JSON,INPUT_TF_INSTANCES_JSON,INPUT_AZUREML_JSON_WRITER,INPUT_VERTEX_DEFAULT,B,../data/dataiku\mlops\external-models\input-output-formats.md
Which of the following is NOT a capability used to implement a Feature Store approach in DSS?,Feature Storage handled by Dataiku extensive Connections Library,Data Ingestion and Curation using Recipes in the Flow,Offline serving for batch processing using Join Recipes,Data visualization using built-in charting tools,D,../data/dataiku\mlops\feature-store\index.md
What permission is required to define Feature Groups in Dataiku?,Manage Datasets,Manage Feature Store,Manage Projects,Manage Connections,B,../data/dataiku\mlops\feature-store\index.md
What happens when you remove a Feature Group in Dataiku?,The underlying dataset is deleted,All existing sharings of the underlying dataset are removed,The Feature Group will not be available in the Feature Store for future users,The dataset is archived,C,../data/dataiku\mlops\feature-store\index.md
Which of the following methods is NOT mentioned as a way to import an MLflow model into DSS?,Through the Python API,Using the 'Deploy' action available for logged models in Experiment Tracking's runs,Importing from a Databricks registry,Uploading directly from a local CSV file,D,../data/dataiku\mlops\mlflow-models\importing.md
What is the minimum Python version required for the code environment when importing an MLflow model through the Python API?,2.7,3.5,3.6,3.7,D,../data/dataiku\mlops\mlflow-models\importing.md
"When importing an MLflow model from a Databricks registry, what is the first step to ensure?",Creating a new project in DSS,Ensuring a valid connection to your Databricks workspace,Uploading the model to a local folder,Setting up a new code environment,B,../data/dataiku\mlops\mlflow-models\importing.md
Which of the following is NOT a capability of DSS when managing MLflow models?,Scoring datasets using a scoring recipe,Deploying the model for real-time scoring using the API node,Generating synthetic datasets for training,Analyzing drift on the MLflow model,C,../data/dataiku\mlops\mlflow-models\index.md
What is one of the advanced capabilities of DSS when importing MLflow models?,Generating new features automatically,Evaluating the performance of a classification or regression model on a labeled dataset,Creating new machine learning algorithms,Automatically tuning hyperparameters,B,../data/dataiku\mlops\mlflow-models\index.md
Which of the following ML packages is not supported for binary classification in Dataiku DSS?,CatBoost,fast.ai 1,LightGBM,ONNX,B,../data/dataiku\mlops\mlflow-models\limitations.md
Which MLflow version is recommended for use with TensorFlow 2 / Keras 2.10 in Dataiku DSS?,1.20.2,1.30.0,2.13.0,2.20.0,C,../data/dataiku\mlops\mlflow-models\limitations.md
Which of the following is NOT a supported framework by MLflow?,PyTorch,TensorFlow,Scikit-learn,MATLAB,D,../data/dataiku\mlops\mlflow-models\training.md
What is the purpose of the 'mlflow.keras.save_model' function in the context of Dataiku?,To train a Keras model,To save a Keras model in a format compatible with DSS,To evaluate a Keras model,To delete a Keras model,B,../data/dataiku\mlops\mlflow-models\training.md
Which method is used to retrieve the path of a local managed folder in Dataiku?,dataiku.Folder.get_path(),dataiku.Folder.retrieve_path(),dataiku.Folder.fetch_path(),dataiku.Folder.obtain_path(),A,../data/dataiku\mlops\mlflow-models\training.md
Which of the following modes is available for scoring with an MLflow Saved Model in DSS?,Direct output mode,Indirect output mode,Restructure mode,Both A and C,D,../data/dataiku\mlops\mlflow-models\using.md
What functionalities are available for evaluating versions of MLflow Saved Models on any valid DSS dataset?,Performance analysis,Model Evaluation Store,Model Comparison,All of the above,D,../data/dataiku\mlops\mlflow-models\using.md
Which of the following types of models are NOT supported for use in Dataiku Model Comparisons?,Binary classification models,Clustering models,Regression models,Multi-class classification models,B,../data/dataiku\mlops\model-comparisons\index.md
What is the purpose of setting a 'champion' model in Dataiku's Model Comparison?,To delete all other models,To facilitate the comparison of a specific model against all others,To automatically deploy the model in production,To rename the model,B,../data/dataiku\mlops\model-comparisons\index.md
Which of the following is NOT a type of model evaluation that can be compared in Dataiku Model Comparisons?,Visual analysis model,Saved Model version,Evaluations from Evaluation Stores,Partitioned models,D,../data/dataiku\mlops\model-comparisons\index.md
What is the key difference between the display of a Model Evaluation and the display of a Saved Model Version?,Performance is computed against the training set for both.,Performance is computed against the test set for a Saved Model Version and against the evaluation dataset for a Model Evaluation.,Performance is computed against the evaluation dataset for both.,Performance is computed against the validation set for both.,B,../data/dataiku\mlops\model-evaluations\analyzing-evaluations.md
How are Model Evaluations displayed in the Model Evaluation Store when there are 3 or more of them?,As a bar graph.,As a pie chart.,As a line graph.,As a scatter plot.,C,../data/dataiku\mlops\model-evaluations\analyzing-evaluations.md
"In the context of Model Evaluations, what can labels be used for?",To add semantic to evaluations.,To change the algorithm used for training.,To modify the dataset size.,To adjust the learning rate.,A,../data/dataiku\mlops\model-evaluations\analyzing-evaluations.md
Which of the following is necessary for continuous evaluation and drift analysis of a model?,Using the Audit Dispatching and Event Server to retrieve logs from API nodes.,Manually checking the model performance every month.,Using only the initial training data for evaluation.,Ignoring the ground truth data.,A,../data/dataiku\mlops\model-evaluations\automating.md
What can be done if a warning or an error is raised by a check during the model evaluation process?,Ignore the warning and continue.,Send a message to your team or retrain and redeploy your model.,Stop using the model entirely.,Manually adjust the model parameters.,B,../data/dataiku\mlops\model-evaluations\automating.md
What is the purpose of holding out a portion of the data when training a machine learning model?,To increase the size of the training set,To evaluate the performance of the machine learning model,To reduce the computational cost,To simplify the model,B,../data/dataiku\mlops\model-evaluations\concepts.md
Which of the following is NOT a type of result screen available for evaluating machine learning models in DSS?,Confusion matrix,Decision charts,Error deciles,Hyperparameter tuning,D,../data/dataiku\mlops\model-evaluations\concepts.md
What is the purpose of creating subsequent evaluations of a machine learning model in DSS?,To improve the model's accuracy,To evaluate the model on a different dataset at a later time,To reduce the size of the dataset,To simplify the model,B,../data/dataiku\mlops\model-evaluations\concepts.md
Which of the following is NOT a valid output of an Evaluation Recipe in DSS?,An Evaluation Store,An output dataset,A metrics dataset,A training dataset,D,../data/dataiku\mlops\model-evaluations\dss-models.md
Which type of models are NOT supported by the Evaluation Recipe in DSS?,Non-partitioned classification models,Regression models,Computer vision models,Tabular MLflow models,C,../data/dataiku\mlops\model-evaluations\dss-models.md
What is the primary purpose of a Standalone Evaluation Recipe (SER) in Dataiku?,To train new machine learning models.,To evaluate custom models using the Model Evaluation framework.,To import models from MLflow.,To generate daily reports.,B,../data/dataiku\mlops\model-evaluations\external-models.md
Which of the following is NOT a benefit of using the Model Evaluation framework for custom models in Dataiku?,Models results screens,Drift analysis,Training new models,Standalone Evaluation Recipe,C,../data/dataiku\mlops\model-evaluations\external-models.md
What is required in the evaluation dataset for a Standalone Evaluation Recipe to perform results screens?,A column containing the predicted values only,A column containing the ground truth only,Both a column containing the predicted values and a column containing the ground truth,No specific columns are required,C,../data/dataiku\mlops\model-evaluations\external-models.md
What is the primary purpose of evaluating a machine learning model?,To determine the model's performance and behavior on a set of data called the Evaluation set.,To train the model on new data.,To deploy the model into production.,To visualize the data used in the model.,A,../data/dataiku\mlops\model-evaluations\index.md
Which of the following is NOT a component of automating model evaluations and drift analysis?,Metrics and Checks,Scenarios and feedback loop,Model deployment,Feedback loop,C,../data/dataiku\mlops\model-evaluations\index.md
What is one of the uses of model evaluations in MLOps?,Data visualization,Drift analysis,Data cleaning,Feature engineering,B,../data/dataiku\mlops\model-evaluations\index.md
Which panel in the Unified Monitoring overview displays a categorized count of detected issues?,Dataiku Projects,API Endpoints,Overview,Health Status,C,../data/dataiku\mlops\unified-monitoring\accessing-unified-monitoring.md
What does the 'Dataiku Projects' panel list in the Unified Monitoring overview?,Healthy endpoints,Unhealthy deployed projects,Categorized count of detected issues,Supported cloud platforms,B,../data/dataiku\mlops\unified-monitoring\accessing-unified-monitoring.md
Which of the following statuses is NOT part of the API Endpoint's status in Unified Monitoring?,Global Status,Deployment Status,Model Status,Service Status,D,../data/dataiku\mlops\unified-monitoring\api-endpoints.md
What does the Global Status of an API Endpoint represent?,The average of Deployment Status and Model Status,The worst of Deployment Status and Model Status,The best of Deployment Status and Model Status,The sum of Deployment Status and Model Status,B,../data/dataiku\mlops\unified-monitoring\api-endpoints.md
Which of the following infrastructures is NOT mentioned as being supported for API Services deployment in Unified Monitoring?,Amazon SageMaker,Google Vertex AI,Microsoft Azure Machine Learning,IBM Watson,D,../data/dataiku\mlops\unified-monitoring\api-endpoints.md
What does the Global Status of a Dataiku project represent?,"The average of Deployment Status, Model Status, and Execution Status.","The worst of Deployment Status, Model Status, and Execution Status.","The best of Deployment Status, Model Status, and Execution Status.","The sum of Deployment Status, Model Status, and Execution Status.",B,../data/dataiku\mlops\unified-monitoring\dataiku-projects.md
Which status represents the health status of the deployment as reported by the Deployer?,Model Status,Execution Status,Deployment Status,Global Status,C,../data/dataiku\mlops\unified-monitoring\dataiku-projects.md
What does the Execution Status of a Dataiku project represent?,The average execution status of all scenarios in the project.,The worst execution status of all enabled scenarios in the project.,The best execution status of all enabled scenarios in the project.,The execution status of the most recent scenario in the project.,B,../data/dataiku\mlops\unified-monitoring\dataiku-projects.md
What does Unified Monitoring in Dataiku provide?,Customizable dashboards for monitoring project health.,Out-of-the-box dashboards for monitoring the health and status of Dataiku projects.,Real-time data processing capabilities.,Automated data cleaning and preprocessing.,B,../data/dataiku\mlops\unified-monitoring\index.md
What does Unified Monitoring use to compute the model status of a given Saved Model?,All Model Evaluation Stores where the Saved Model serves as the input of an Evaluation Recipe.,The average computation result of all checks.,The best computation result of all checks.,The number of times the model has been evaluated.,A,../data/dataiku\mlops\unified-monitoring\model-status.md
"For 'API Endpoint' model status, what does Unified Monitoring need to query the node properly?",The project name and version.,The URL and an API key.,The model's training data.,The model's evaluation metrics.,B,../data/dataiku\mlops\unified-monitoring\model-status.md
Which AWS API integration in Dataiku provides speech-to-text extraction?,AWS Comprehend,AWS Translation,AWS Transcribe,AWS Comprehend Medical,C,../data/dataiku\nlp\aws-apis.md
How many languages does AWS Comprehend support for language detection?,12,40,71,100,D,../data/dataiku\nlp\aws-apis.md
Which AWS API integration in Dataiku is specifically noted for having significantly higher costs?,AWS Transcribe,AWS Comprehend,AWS Translation,AWS Comprehend Medical,D,../data/dataiku\nlp\aws-apis.md
Which of the following NLP capabilities is NOT provided by the Azure Cognitive Services API in Dataiku?,Language Detection,Sentiment Analysis,Named Entities Extraction,Translation,D,../data/dataiku\nlp\azure-apis.md
How many languages does the Azure Translation API support for translation in Dataiku?,13,16,23,90,D,../data/dataiku\nlp\azure-apis.md
Which of the following tasks can be performed using the Crowlingo Multilingual NLP API in Dataiku?,Language detection,Sentiment analysis,Summarization,All of the above,D,../data/dataiku\nlp\crowlingo-apis.md
What is the purpose of text embedding in machine learning?,To compute a numerical representation of a piece of text.,To generate a summary of a text document.,To translate text from one language to another.,To correct grammatical errors in a text.,A,../data/dataiku\nlp\embedding.md
Which plugin provides a recipe for retrieving text embeddings directly as a vector column?,Text-embedding plugin,Sentence-embedding plugin,Text-similarity plugin,Vector-retrieval plugin,B,../data/dataiku\nlp\embedding.md
Which of the following is true about the 'Sentence Embedding' plugin?,It is supported in multiple languages.,It is a native feature in Visual ML.,It is only available in English.,It does not require installation.,C,../data/dataiku\nlp\embedding.md
Which of the following NLP capabilities is NOT provided by the Google Cloud NLP integration?,Sentiment Analysis,Named Entities Extraction,Text classification in English,Translation between 109 languages,D,../data/dataiku\nlp\google-apis.md
How many languages does the Google Cloud Translation integration support?,16,11,109,50,C,../data/dataiku\nlp\google-apis.md
Which of the following is NOT a third-party API mentioned for language detection in DSS?,AWS Comprehend,Azure Cognitive Services,Google Cloud NLP,Crowlingo,C,../data/dataiku\nlp\index.md
Which of the following applications is NOT listed under text analysis in DSS?,Sentiment Analysis,Named Entity Recognition,Text Summarization,Image Classification,D,../data/dataiku\nlp\index.md
Which of the following services is NOT used for sentiment analysis in DSS?,AWS Comprehend,Azure Cognitive Services,Google Cloud NLP,Deepl Translation,D,../data/dataiku\nlp\index.md
Which of the following services provides key phrase extraction in 13 languages?,AWS Comprehend,Azure Cognitive Services,Google Cloud Natural Language,IBM Watson,A,../data/dataiku\nlp\key-phrase-extraction.md
How many languages does Azure Cognitive Services support for key phrase extraction?,10,13,16,20,C,../data/dataiku\nlp\key-phrase-extraction.md
Which of the following language detection capabilities provided by Dataiku supports the highest number of languages?,Native language detection,AWS Comprehend,Azure Cognitive Services,MeaningCloud,D,../data/dataiku\nlp\language-detection.md
Which Dataiku language detection capability does not leverage a 3rd party API?,AWS Comprehend,Azure Cognitive Services,Crowlingo,Native language detection,D,../data/dataiku\nlp\language-detection.md
Which of the following NLP tasks can Dataiku perform using the MeaningCloud API?,Language Detection in 180 languages,Sentiment Analysis in 10 languages,Text summarization,All of the above,D,../data/dataiku\nlp\meaningcloud-apis.md
What is required to use the MeaningCloud API capabilities in Dataiku?,A premium subscription,Installing the MeaningCloud plugin,A dedicated server,None of the above,B,../data/dataiku\nlp\meaningcloud-apis.md
Which of the following named entity extraction capabilities is provided by Dataiku's native entity extraction?,It leverages a 3rd party API.,It is an offline capability.,It supports 23 languages.,It is supported by Dataiku.,B,../data/dataiku\nlp\named-entities.md
How many languages does the AWS Comprehend integration support for named entity recognition?,7,11,12,23,C,../data/dataiku\nlp\named-entities.md
Which named entity recognition integration supports the highest number of languages?,Dataiku Native Entity Extraction,AWS Comprehend,Azure Cognitive Services,Google Cloud NLP,C,../data/dataiku\nlp\named-entities.md
Which of the following OCR engines does Dataiku leverage?,Google Cloud Vision,Tesseract,Amazon Textract,EasyOCR,"B, D",../data/dataiku\nlp\ocr.md
What is required to use the OCR capability in Dataiku?,A 3rd party API,An internet connection,The 'Text extraction and OCR' plugin,A paid subscription,C,../data/dataiku\nlp\ocr.md
What is the primary purpose of Ontology Tagging in Dataiku?,To generate daily reports.,To extract explicitly defined content and entities from text.,To build monthly dashboards.,To predict fraudulent transactions.,B,../data/dataiku\nlp\ontology-tagging.md
How many languages does Dataiku's Ontology Tagging support?,10,25,59,100,C,../data/dataiku\nlp\ontology-tagging.md
Which plugin provides the Ontology Tagging capability in Dataiku?,Data Analysis,Machine Learning,Text Analysis,Visualization,C,../data/dataiku\nlp\ontology-tagging.md
Which of the following recipes is NOT included in the OpenAI GPT plugin for Dataiku?,Text Generation,Classification,Question Answering,Data Visualization,D,../data/dataiku\nlp\openai-gpt.md
What must you do to use the OpenAI GPT plugin in Dataiku?,Install the plugin,Update Dataiku to the latest version,Enable the plugin in the settings,Contact Dataiku support,A,../data/dataiku\nlp\openai-gpt.md
Which of the following integrations provides sentiment analysis in the most languages?,AWS Comprehend,Azure Cognitive Services,Google Cloud NLP,MeaningCloud,C,../data/dataiku\nlp\sentiment-analysis.md
Which sentiment analysis capability does not leverage a 3rd party API?,AWS Comprehend,Azure Cognitive Services,Crowlingo,Offline sentiment analysis,D,../data/dataiku\nlp\sentiment-analysis.md
Which of the following is true about Dataiku's native speech-to-text capability?,It supports multiple languages.,It leverages a 3rd party API.,It is an offline capability.,It is supported by Dataiku.,C,../data/dataiku\nlp\speech-to-text.md
How many languages does AWS Transcribe integration support for speech-to-text extraction?,10,20,30,40,D,../data/dataiku\nlp\speech-to-text.md
How many languages does Dataiku's native spell checking capability support?,10,20,37,50,C,../data/dataiku\nlp\spell-checking.md
Which plugin provides the spell checking capability in Dataiku?,Data Preparation,Text Preparation,NLP Analysis,Language Processing,B,../data/dataiku\nlp\spell-checking.md
Which of the following statements is true about Dataiku's native text summarization capability?,It uses a 3rd party API for summarization.,It provides language-specific summarization.,It uses open-source models for extractive summarization.,It is supported by Dataiku.,C,../data/dataiku\nlp\summarization.md
What is the primary difference between extractive and abstractive summarization?,Extractive summarization generates new sentences.,Abstractive summarization extracts sentences from the document.,Extractive summarization extracts the most relevant sentences from the document.,Abstractive summarization is language-specific.,C,../data/dataiku\nlp\summarization.md
Which integration provides summarization detection in 102 languages?,Dataiku's native text summarization,Crowlingo,MeaningCloud,None of the above,B,../data/dataiku\nlp\summarization.md
Which of the following is NOT a feature provided by Dataiku's native text cleaning capability?,Tokenization,Filtering of punctuation,Lemmatization,Sentiment Analysis,D,../data/dataiku\nlp\text-cleaning.md
How many languages does Dataiku's native text cleaning capability support?,10,25,59,100,C,../data/dataiku\nlp\text-cleaning.md
What is required to use Dataiku's native text cleaning capability?,A third-party API,The 'Text Preparation' plugin,A premium subscription,A special license,B,../data/dataiku\nlp\text-cleaning.md
Which of the following document types can Dataiku extract text from?,PDF,DOCX,HTML,All of the above,D,../data/dataiku\nlp\text-extraction.md
What are the three columns in the dataset output by the Text extraction recipe?,"Filename, extracted text, error messages","Filename, extracted text, metadata","Filename, metadata, error messages","Extracted text, metadata, error messages",A,../data/dataiku\nlp\text-extraction.md
What plugin provides the text extraction capability in Dataiku?,Text extraction and OCR plugin,Data extraction plugin,Document processing plugin,Text analysis plugin,A,../data/dataiku\nlp\text-extraction.md
"In Dataiku's Visual Machine Learning, how can text be utilized?",As regular features,Only for data visualization,For database management,Only for statistical analysis,A,../data/dataiku\nlp\text-in-ml.md
Which translation capability provided by Dataiku supports the highest number of languages?,Native translation,AWS Translation,Azure Translation,Google Cloud Translation,D,../data/dataiku\nlp\translation.md
Which translation capability in Dataiku is an offline feature and does not leverage a 3rd party API?,AWS Translation,Azure Translation,Google Cloud Translation,Native translation,D,../data/dataiku\nlp\translation.md
Which translation capability provided by Dataiku supports the fewest number of languages?,Native translation,AWS Translation,Azure Translation,Deepl Translation,D,../data/dataiku\nlp\translation.md
Which of the following is NOT a benefit of running notebook kernels in containers?,Resource monitoring from Docker,Freeing the machine hosting the notebook server from executing notebooks,Automatic saving of files to the current working directory,Running kernels in remote containers in a Kubernetes cluster,C,../data/dataiku\notebooks\containerized-notebooks.md
What must be ensured in the code environment settings for running a notebook in a container?,Jupyter notebook support is selected in Packages to install,The environment is built for the desired container configurations,The kernel is installed in the container,Both A and B,D,../data/dataiku\notebooks\containerized-notebooks.md
Where can you install or remove the notebook kernels for the builtin environment?,Administration > Code Envs,Administration > Settings > Containerized execution,Administration > Notebook Kernels,Administration > Docker Settings,B,../data/dataiku\notebooks\containerized-notebooks.md
Which of the following types of notebooks is specifically designed for exploratory and experimental work using code in DSS?,SQL notebook,Python notebook,Predefined notebook,Containerized notebook,B,../data/dataiku\notebooks\index.md
What is the purpose of the 'Search notebook' in DSS?,To perform data preparation,To run machine learning models,To perform queries and manage search scope,To create visual recipes,C,../data/dataiku\notebooks\index.md
Which of the following is NOT a step in configuring containers for notebooks in DSS?,Builtin environment,Code envs,Running a notebook in a container,Creating a SQL notebook,D,../data/dataiku\notebooks\index.md
Which of the following commands lists all available Jupyter extensions in Dataiku DSS?,DATA_DIR/bin/dssadmin jupyter-nbextensions enable EXTENSION_NAME,DATA_DIR/bin/dssadmin jupyter-nbextensions list,DATA_DIR/bin/dssadmin jupyter-nbextensions available,DATA_DIR/bin/dssadmin jupyter-nbextensions disable EXTENSION_NAME,C,../data/dataiku\notebooks\jupyter-nbextensions.md
What must you always refer to when installing a Jupyter extension that requires a specific package in Dataiku DSS?,The Dataiku DSS user manual,The documentation of the extension,The Dataiku DSS support team,The Jupyter Notebook official website,B,../data/dataiku\notebooks\jupyter-nbextensions.md
Which of the following statements is true about enabling Jupyter extensions in Dataiku DSS?,You can enable an extension for specific users only.,You need to stop DSS to enable an extension.,Enabling an extension affects all users on the platform.,You can enable extensions that require a server extension.,C,../data/dataiku\notebooks\jupyter-nbextensions.md
Which of the following is NOT one of the prebuilt notebooks available in DSS?,Simple statistical analysis,Dimensionality reduction,Time series forecasting,Regression analysis,D,../data/dataiku\notebooks\predefined-notebooks.md
What is the first step to create a notebook from a prebuilt template in DSS?,Click on the Lab icon from a dataset’s page or in the Flow,Write your notebook in Jupyter,Download a copy as .ipynb of your notebook,Create a plugin,A,../data/dataiku\notebooks\predefined-notebooks.md
Which of the following analyses can be performed using the prebuilt notebooks in DSS?,Distribution analysis and statistical tests on a single numerical population,Regression analysis,Clustering analysis,Classification analysis,A,../data/dataiku\notebooks\predefined-notebooks.md
Which of the following is a recommended method for creating a Python notebook if the main goal is to analyze and study a dataset?,From the notebooks list,From a dataset's Lab modal,From the Jupyter interface,From the Administration menu,B,../data/dataiku\notebooks\python.md
What is a core concept of the Jupyter notebook regarding the process running the code?,The process is unloaded immediately after execution,The process remains loaded in memory even if you leave the interface,The process is automatically saved to disk,The process is shared among multiple users,B,../data/dataiku\notebooks\python.md
Which of the following is NOT a way to unload a Jupyter notebook in DSS?,"From the notebook UI interface, choose File > Close and Halt","From the notebooks list in the project, click on the yellow cross","From your personal activity drawer, click on the cross","From the dataset's Lab modal, click on unload",D,../data/dataiku\notebooks\python.md
Which of the following search scopes can be configured in a Search notebook in DSS?,Indices-based,Index-pattern-based,Datasets-based,All of the above,D,../data/dataiku\notebooks\search-notebook.md
What is the purpose of the Search notebook in DSS?,To perform search queries leveraging the native search capabilities of Elasticsearch,To create visualizations of data,To manage user permissions,To perform data cleaning operations,A,../data/dataiku\notebooks\search-notebook.md
Which Elasticsearch dialect versions are compatible with the Search notebook in DSS?,v5 and higher,v6 and higher,v7 and higher,v8 and higher,C,../data/dataiku\notebooks\search-notebook.md
What is the primary function of a SQL notebook in DSS?,To perform queries on all SQL databases supported by DSS.,To create visualizations from data.,To manage user permissions.,To deploy machine learning models.,A,../data/dataiku\notebooks\sql-notebook.md
How can you create a SQL notebook in DSS?,From the 'Datasets' tab of your project.,From the 'Notebooks' tab of your project.,From the 'Models' tab of your project.,From the 'Dashboards' tab of your project.,B,../data/dataiku\notebooks\sql-notebook.md
What is the recommended way to work with a SQL notebook?,To keep multiple cells for each query.,To keep a single cell for each query that you might want to rerun.,To avoid using cells and write all queries in one block.,To use cells only for final queries.,B,../data/dataiku\notebooks\sql-notebook.md
Which of the following folders can be safely excluded from a DSS backup because they only contain temporary data?,tmp,caches,data-catalog,All of the above,D,../data/dataiku\operations\backups.md
What is the recommended procedure for ensuring full consistency of a backup when DSS is running?,"Create the snapshot, mount it as read-only, execute an incremental backup tool, unmount the snapshot, destroy the snapshot","Stop DSS, create the snapshot, execute an incremental backup tool, restart DSS","Create the snapshot, execute an incremental backup tool, destroy the snapshot","Stop DSS, create the snapshot, mount it as read-only, execute an incremental backup tool, unmount the snapshot",A,../data/dataiku\operations\backups.md
What must be backed up in addition to the DATA_DIR for Govern nodes?,Temporary / Cache data,Logs,PostgreSQL database,Exports,C,../data/dataiku\operations\backups.md
Which of the following is NOT a type of Compute Resource Usage (CRU) context in DSS?,JOB_ACTIVITY,ANALYSIS_ML_TRAIN,SQL_NOTEBOOK,API_DEPLOYER_SERVICE,D,../data/dataiku\operations\compute-resource-usage-reporting.md
What is the purpose of the 'dataiku.com/dku-exec-submitter' label in Kubernetes pods created by DSS?,To identify the type of execution,To provide the login of the user who submitted the Kubernetes object,To specify the installation identifier of the DSS node,To indicate the unique identifier of the submission,B,../data/dataiku\operations\compute-resource-usage-reporting.md
Which of the following CRU types represents the execution of a Spark job on Kubernetes?,SINGLE_K8S_JOB,SPARK_K8S_JOB,LOCAL_PROCESS,SQL_QUERY,B,../data/dataiku\operations\compute-resource-usage-reporting.md
Which folder in the DSS data directory contains the data for the models trained in the Lab part of DSS?,analysis-data,saved-models,databases,notebook-results,A,../data/dataiku\operations\datadir.md
Which folder in the DSS data directory is not strictly required to be included in backups?,config,data-catalog,lib,managed-datasets,B,../data/dataiku\operations\datadir.md
Where can you find the path to the data directory if you did not install DSS yourself?,Administration > Maintenance > System info,Administration > Settings > Data directory,Settings > Maintenance > System info,Settings > Data directory > System info,A,../data/dataiku\operations\datadir.md
Which of the following is NOT a location in the DSS data directory?,jobs,scenarios,saved_models,logs,D,../data/dataiku\operations\disk-usage.md
What is the recommended method for cleaning up job logs in DSS?,Manually deleting job folders,Using the 'Clear job logs' macro,Running a Python script,Restarting DSS,B,../data/dataiku\operations\disk-usage.md
What kind of data is stored in the 'analysis-data' folder?,Temporary data,Machine learning models and staging data,Historical logs,Cached data,B,../data/dataiku\operations\disk-usage.md
Which of the following commands is used to list all jobs for a given project in DSS?,dsscli jobs-list,dsscli job-status,dsscli job-abort,dsscli job-clear,A,../data/dataiku\operations\dsscli.md
What is the primary difference between the dsscli and dssadmin command-line tools?,"dsscli is for installation tasks, while dssadmin is for day-to-day operations.","dsscli is for day-to-day operations, while dssadmin is for installation tasks.","dsscli is used for security-related commands, while dssadmin is used for project-related commands.","dsscli is used for managing datasets, while dssadmin is used for managing users.",B,../data/dataiku\operations\dsscli.md
Which of the following commands would you use to create a new user in DSS?,dsscli user-create,dsscli user-edit,dsscli users-list,dsscli user-delete,A,../data/dataiku\operations\dsscli.md
Which of the following commands is related to managing datasets in DSS?,Security-related commands,Jobs-related commands,Datasets related commands,Connections-related commands,C,../data/dataiku\operations\index.md
What is the purpose of the 'audit trail' in DSS?,To manage temporary files,To track and log events and actions within DSS,To configure log file rotation,To manage disk usage,B,../data/dataiku\operations\index.md
Which of the following is a method for controlling memory usage in DSS?,Using cgroups,Managing temporary files,Running an automatic backup,Viewing the audit trail,A,../data/dataiku\operations\index.md
Which of the following is true about the Free Edition of Dataiku DSS?,The license needs to be updated annually.,The license is automatically generated and never needs updating.,The license file must be manually copied into DATA_DIR/config/license.json.,The license is provided by Dataiku and needs to be renewed periodically.,B,../data/dataiku\operations\license.md
How can you update the license for the Enterprise Edition of Dataiku DSS?,By filling out a form on the Dataiku website.,By copying the new license file into DATA_DIR/config/license.json.,By logging into Dataiku DSS and entering the contents of the new license file.,Both B and C.,D,../data/dataiku\operations\license.md
"Which log file contains logs of the main DSS process, including all user operations directly performed in DSS?",ipython.log,backend.log,nginx.log,frontend.log,B,../data/dataiku\operations\logging.md
What is the default maximum file size for log rotation in DSS?,10 MB,50 MB,100 MB,200 MB,B,../data/dataiku\operations\logging.md
Which log file is rotated daily regardless of its size?,backend.log,ipython.log,nginx/access.log,eventserver.log,C,../data/dataiku\operations\logging.md
Which of the following is NOT a way to run a macro in DSS?,Manually from a Project’s 'Macros' screen,Automatically from a scenario step,By adding them on a dashboard for dashboard users,By scheduling them in the DSS calendar,D,../data/dataiku\operations\macros.md
Which of the following tasks can be automated using DSS macros?,Generating an audit report of which connections are used,List and mass-delete datasets by tag filters,Clear internal DSS databases,All of the above,D,../data/dataiku\operations\macros.md
Who can use data-import-related macros in DSS?,Only administrators,All DSS users,Only project managers,Only data scientists,B,../data/dataiku\operations\macros.md
What is the recommended memory allocation for the backend in large production instances of DSS?,4 to 8 GB,8 to 12 GB,12 to 20 GB,20 to 32 GB,C,../data/dataiku\operations\memory.md
Which of the following is NOT a factor that scales the backend memory requirement in DSS?,Number of users,"Number of projects, datasets, recipes",Overall activity,Number of Jupyter notebooks,D,../data/dataiku\operations\memory.md
What should you do if you see 'OutOfMemoryError: Java heap space' in the backend log?,Decrease backend.xmx,Increase backend.xmx,Restart DSS,Check the number of users,B,../data/dataiku\operations\memory.md
Which of the following is NOT a main topic for monitoring DSS?,Raising alerts when some services are not working properly,Storing and plotting immediate and historical data of host-level statistics,Providing built-in alerting mechanisms,Storing and plotting immediate and historical data of application-level statistics,C,../data/dataiku\operations\monitoring.md
What is the purpose of the dkumonitor tool provided by Dataiku?,To provide a built-in alerting mechanism for DSS,To bundle together the common 'Graphite / Grafana' stack for easy setup,To replace the need for any external monitoring software,To automatically fix any issues detected in DSS,B,../data/dataiku\operations\monitoring.md
"Which process is responsible for starting, restarting, and monitoring the top-level DSS processes?",The backend,The JEKs,supervisord,The governserver,C,../data/dataiku\operations\processes.md
"What is the main process of DSS that handles the API and UI, and schedules scenarios?",The JEKs,The backend,The FEKs,The governserver,B,../data/dataiku\operations\processes.md
"Which process is created for each model being trained using the 'In-memory (scikit-learn, LightGBM, XGBoost)' or 'Deep Learning' engines?",A Jupyter kernel,A Spark driver process,A Python process identified by 'dataiku.doctor.server',A webapp backend process,C,../data/dataiku\operations\processes.md
Which of the following HTTP proxies is NOT supported by DSS for WebSocket protocol?,nginx version 1.3.13 and above,Apache 2.4.5 and above,Amazon Web Services Application Load Balancer,SOCKS proxies,D,../data/dataiku\operations\proxies.md
What is required for DSS to trust a certificate authority in a network using a cloud-based proxy as a man in the middle (MITM) proxy?,Copy the certificates to /etc/pki/ca-trust/source/anchors and run update-ca-trust,Configure the proxy using standard configuration directives for Python or R,Add the proxy settings in the Administration menu under HTTP proxy,Use the REQUESTSCABUNDLE environment variable,A,../data/dataiku\operations\proxies.md
What is one of the main advantages of externally hosting runtime databases for bigger production DSS instances?,The internal H2 engine scales better than an external PostgreSQL server.,The external PostgreSQL server is more resilient to various failures.,It is easier to back up an H2 engine without any downtime.,Externally hosting runtime databases reduces the need for backups.,B,../data/dataiku\operations\runtime-databases.md
Which of the following is NOT stored in the runtime databases of DSS?,The history of run jobs and scenarios,The history and latest values of metrics and checks,The configuration of projects and datasets,The contents of discussions,C,../data/dataiku\operations\runtime-databases.md
What is the first step in setting up an externally-hosted runtime database for DSS?,Edit config/general-settings.json,Stop DSS,Run the command to copy databases to the external server,Start DSS,B,../data/dataiku\operations\runtime-databases.md
What is a major use case for audit centralization in API nodes?,Centralizing logs of API node queries for ML Ops activities.,Improving the speed of API responses.,Reducing the cost of API deployments.,Enhancing the user interface of the API.,A,../data/dataiku\operations\audit-trail\apinode.md
"In a high-security API node centralization setup, what is required for each service to ensure security?",A unique routing key for each service.,A service-specific authentication key.,A dedicated server for each service.,A separate API deployer for each service.,B,../data/dataiku\operations\audit-trail\apinode.md
What is the main advantage of the sample setup for easy centralization of API node logs?,No admin intervention is required when new API services are created and deployed.,It provides the highest level of security for the logs.,It allows for real-time monitoring of API performance.,It reduces the storage space required for logs.,A,../data/dataiku\operations\audit-trail\apinode.md
Which of the following topics is used for the audit of API node queries and replies?,generic,authfail,apinode-query,compute-resource-usage,C,../data/dataiku\operations\audit-trail\centralization-and-dispatch.md
Which target is experimental and covered by Tier 2 support?,Log4J target,EventServer target,Kafka target,None of the above,C,../data/dataiku\operations\audit-trail\centralization-and-dispatch.md
Which of the following fields in a standard audit log file indicates the duration of a query?,severity,logger,callTime,timestamp,C,../data/dataiku\operations\audit-trail\data.md
"In Event Server data files, what does the 'origAddress' field represent?",The IP of the DSS node that sent the event to the Event Server,The IP of the client that initiated the event,The IP of the server that processed the event,The IP of the user who performed the request,A,../data/dataiku\operations\audit-trail\data.md
Which of the following msgTypes is associated with the 'apinode-query' topic?,application-open,dataset-read-data-sample,prediction-query,scenario-run,C,../data/dataiku\operations\audit-trail\data.md
Which of the following steps is NOT required to import audit logs as a dataset in Dataiku's instance?,Add a Dataset > Cloud storages & Social > Amazon S3,Select the corresponding S3 connection and the path in bucket,Activate the partitioning in the 'Partitioning' tab,Manually add the API node logs to the instance,D,../data/dataiku\operations\audit-trail\dataikucloud.md
What is the name of the Amazon S3 connection that hosts the API Nodes logs queries in Dataiku Cloud?,customer-audit-log,dku-audit-log,api-node-log,instance-audit-log,A,../data/dataiku\operations\audit-trail\dataikucloud.md
What is the primary use case of the DSS Event Server?,To handle real-time data processing.,To centralize audit trail messages from multiple DSS nodes.,To manage user authentication.,To create datasets automatically.,B,../data/dataiku\operations\audit-trail\eventserver.md
Which of the following is NOT a step in installing the DSS Event Server?,Stop DSS,Run './bin/dssadmin install-event-server',Start DSS,Configure the event server in Administration > Settings > Event Server,D,../data/dataiku\operations\audit-trail\eventserver.md
Which of the following is a limitation of the DSS Event Server?,It cannot handle more than a few events per second.,It is not highly-available nor horizontally scalable.,It does not support authentication.,It can only write events to local files.,B,../data/dataiku\operations\audit-trail\eventserver.md
Which of the following is NOT a target for audit dispatching in DSS?,Log4J target,EventServer target,Kafka target,SQL target,D,../data/dataiku\operations\audit-trail\index.md
What information is included in the DSS audit trail?,"User ID, timestamp, IP address, authentication method","User ID, timestamp, IP address, browser type","User ID, timestamp, IP address, operating system","User ID, timestamp, IP address, location",A,../data/dataiku\operations\audit-trail\index.md
Which of the following is a method for configuring API nodes in DSS?,Manual configuration through API deployer,Automatic configuration through API deployer,Semi-automatic configuration through API deployer,Dynamic configuration through API deployer,B,../data/dataiku\operations\audit-trail\index.md
What is the primary function of the 'distinct' recipe in Dataiku DSS?,To merge multiple datasets into one.,To deduplicate rows in a dataset by retrieving unique rows.,To filter rows based on specific conditions.,To perform statistical analysis on a dataset.,B,../data/dataiku\other_recipes\distinct.md
Which of the following engines is NOT mentioned as an option for executing recipes in Dataiku DSS?,Hive,Impala,SparkSQL,TensorFlow,D,../data/dataiku\other_recipes\distinct.md
Which of the following is NOT a files-based connection supported by the download recipe?,HDFS,S3,MySQL,Azure Blob Storage,C,../data/dataiku\other_recipes\download.md
What is the primary function of the download recipe in Dataiku?,To interpret and analyze files,To create a directly usable dataset,To download files and store them in a DSS managed folder,To generate visual reports,C,../data/dataiku\other_recipes\download.md
What happens to remote files that disappear from the remote host during the next output folder build operation?,They are ignored,They are downloaded again,Their local mirror is deleted,They are renamed,C,../data/dataiku\other_recipes\download.md
Which of the following distances is NOT used for text columns in a fuzzy join?,Damerau–Levenshtein,Hamming,Euclidean,Cosine,C,../data/dataiku\other_recipes\fuzzy-join.md
What additional option in the fuzzy join recipe adds a meta column with details about joined keys?,Debug mode,Output matching details,Selected columns,Join conditions,B,../data/dataiku\other_recipes\fuzzy-join.md
Which join type is NOT supported by the fuzzy join recipe?,Inner,Left,Full,Right,C,../data/dataiku\other_recipes\fuzzy-join.md
Which of the following is NOT a supported connection for the 'generate features' recipe?,PostgreSQL,MySQL,MongoDB,Snowflake,C,../data/dataiku\other_recipes\generate-features.md
What is the purpose of setting a 'cutoff time' in the 'generate features' recipe?,To specify the point in time at which data from enrichment datasets should no longer be used in feature generation,To determine the primary key of the dataset,To set the time zone for the dataset,To define the format of the date columns,A,../data/dataiku\other_recipes\generate-features.md
Which of the following is a row-by-row computation available in the 'generate features' recipe?,Average,Count distinct,Day of week,Sum,C,../data/dataiku\other_recipes\generate-features.md
Which of the following is NOT a geometry storage type in DSS?,GEOMETRY,GEOPOINTS,WKT,SRID,D,../data/dataiku\other_recipes\geojoin.md
Which of the following geospatial matching operators is NOT symmetrical?,CONTAINS,INTERSECTS,TOUCHES,DISJOINT,A,../data/dataiku\other_recipes\geojoin.md
What is the SRID that all geometries must be projected in before manipulating geospatial data in DSS?,3857,4326,32633,27700,B,../data/dataiku\other_recipes\geojoin.md
Which of the following engines is NOT mentioned as an option for executing a recipe in DSS?,Hive,Impala,SparkSQL,PostgreSQL,D,../data/dataiku\other_recipes\grouping.md
What is the equivalent of a SQL 'group by' statement in DSS?,Filtering recipe,Grouping recipe,Joining recipe,Sampling recipe,B,../data/dataiku\other_recipes\grouping.md
Which of the following visual recipes in Dataiku is used for aggregating data?,Prepare,Sync,Grouping,Join,C,../data/dataiku\other_recipes\index.md
"In Dataiku, which recipe would you use to retrieve the first N rows of a dataset?",Top N,Sort,Stack,Split,A,../data/dataiku\other_recipes\index.md
Which visual recipe in Dataiku allows you to join datasets based on geospatial features?,Join,Fuzzy join,Geo join,Window,C,../data/dataiku\other_recipes\index.md
Which of the following join types is NOT supported by the 'join' recipe in DSS?,Left join,Right join,Self join,Cross join,C,../data/dataiku\other_recipes\join.md
What is the purpose of adding output datasets for unmatched rows in a join recipe?,To capture rows that do not meet the join conditions,To filter rows before the join operation,To rename columns with identical names,To select the engine for executing the recipe,A,../data/dataiku\other_recipes\join.md
Which of the following engines is NOT mentioned as an option for executing the join recipe in DSS?,Hive,Impala,SparkSQL,TensorFlow,D,../data/dataiku\other_recipes\join.md
Which of the following information can be retrieved about each file using the 'List Folder Contents' recipe?,File permissions,File size in bytes,File owner,File creation date,B,../data/dataiku\other_recipes\list-folder-contents.md
What is the purpose of the 'List Folder Contents' recipe in Dataiku?,To list all files in a folder into a dataset with additional file information,To delete all files in a folder,To move files from one folder to another,To compress files in a folder,A,../data/dataiku\other_recipes\list-folder-contents.md
What is the primary advantage of using the 'pivot' recipe over the 'pivot processor' in Dataiku?,The pivot recipe allows for dynamic output schema computation.,The pivot processor can handle larger datasets.,The pivot recipe requires pre-sorting of data.,The pivot processor offers more aggregation options.,A,../data/dataiku\other_recipes\pivot.md
Which of the following is NOT a method for handling modality names in the pivot recipe?,Soft slugification,Hard slugification,Numbering,Alphabetization,D,../data/dataiku\other_recipes\pivot.md
"In the context of the pivot recipe, what does the 'most frequent' option do when computing the list of modalities?",Keeps only the combinations appearing the most in the input data.,Keeps only the combinations appearing at least N times in the input data.,Specifies the combinations explicitly.,Ignores the original name of the modality and uses numbers instead.,A,../data/dataiku\other_recipes\pivot.md
What is the primary function of the Prepare recipe in Dataiku?,"To create data cleansing, normalization, and enrichment scripts.",To build machine learning models.,To visualize data in dashboards.,To manage user permissions.,A,../data/dataiku\other_recipes\prepare.md
How can you create a Prepare recipe in Dataiku?,Only from scratch.,Only by deploying it from a Visual Analysis in the Lab.,Both from scratch and by deploying it from a Visual Analysis in the Lab.,Only by importing it from external sources.,C,../data/dataiku\other_recipes\prepare.md
What is the main use case for a Push to Editable recipe?,To create a backup of a dataset.,To make manual corrections to a dataset.,To merge multiple datasets.,To visualize data trends.,B,../data/dataiku\other_recipes\push-to-editable.md
What is the row limit for datasets used in Push to Editable recipes?,50K rows,100K rows,200K rows,500K rows,B,../data/dataiku\other_recipes\push-to-editable.md
What must be selected to identify new or modified data in a Push to Editable recipe?,A unique identifier column,A timestamp column,A data type column,A filter column,A,../data/dataiku\other_recipes\push-to-editable.md
Which of the following types of filtering is only available when the input dataset is on ElasticSearch v7 and above?,Rules based,Formula based,SQL expression based,ElasticSearch query string,D,../data/dataiku\other_recipes\sampling.md
"In a rules-based filter, what are the two boolean operators that can be used to bind conditions and groups?","AND, OR","AND, NOT","OR, NOT","XOR, AND",A,../data/dataiku\other_recipes\sampling.md
Which type of filter is best suited for more complex filtering options or specific functions that do not appear in the rules-based filter view?,Rules based,Formula based,SQL expression based,ElasticSearch query string,B,../data/dataiku\other_recipes\sampling.md
Which of the following engines is NOT mentioned as an option for executing a recipe in DSS?,Hive,Impala,SparkSQL,PostgreSQL,D,../data/dataiku\other_recipes\sort.md
"In DSS, how are null values handled in ascending order since version 4.1?",Placed at the end,Placed at the beginning,Randomly placed,Ignored,B,../data/dataiku\other_recipes\sort.md
What is the primary function of the 'split' recipe in Dataiku?,To merge multiple datasets into one.,To dispatch rows of one dataset into several other datasets based on rules.,To clean and preprocess data.,To visualize data trends.,B,../data/dataiku\other_recipes\split.md
Which of the following is NOT a use-case of the sync recipe in Dataiku?,Copying a SQL dataset to HDFS to perform Hive recipes.,Copying a HDFS dataset to ElasticSearch to query it.,Copying a file dataset to a SQL database for efficient querying.,Copying a dataset to a different format without any schema handling.,D,../data/dataiku\other_recipes\sync.md
What should you do if you modify the schema of the input dataset in a sync recipe?,"Nothing, the schema will automatically update.",Go to the edition screen for the sync recipe and click the 'Resync Schema' button.,Manually update the schema of the output dataset.,Restart the Dataiku server.,B,../data/dataiku\other_recipes\sync.md
Which engine is always available but may not be optimal for running the sync recipe?,Spark,SQL,Hive,DSS streaming,D,../data/dataiku\other_recipes\sync.md
Which of the following engines is NOT mentioned as a possible choice for executing the 'top N' recipe in DSS?,Hive,Impala,MySQL,SparkSQL,C,../data/dataiku\other_recipes\topn.md
What is required for the 'top N' recipe to function correctly when no grouping key is provided?,At least one order column,At least one filter,At least one alias,At least one pre-filter,A,../data/dataiku\other_recipes\topn.md
What is the purpose of the remaining rows output in the 'top N' recipe?,To retrieve rows that match the Top N recipe definitions,To retrieve rows that do not match the Top N recipe definitions,To retrieve rows with null values,To retrieve rows with duplicate values,B,../data/dataiku\other_recipes\topn.md
Which of the following engines is NOT mentioned as an option for executing the 'window' recipe in DSS?,Hive,Impala,PostgreSQL,SparkSQL,C,../data/dataiku\other_recipes\window.md
What is the default window behavior when using the DSS engine if no window is specified?,It defaults to the first row,It defaults to the last row,It defaults to the whole frame,It defaults to the middle frame,C,../data/dataiku\other_recipes\window.md
Which of the following dependency functions is typically used when the output dataset is not partitioned?,Equals,Time range,Since beginning of week,Latest available,D,../data/dataiku\partitions\dependencies.md
What type of dependency function returns input values based on output values?,Relative,Absolute,Explicit values,All available,A,../data/dataiku\partitions\dependencies.md
Which dependency function yields all time periods in a time range relative to the output?,Equals,Time range,Since beginning of week,Explicit values,B,../data/dataiku\partitions\dependencies.md
Which of the following storage options is NOT mentioned as a type of files-based dataset that can be partitioned?,Filesystem,Amazon S3,Google Cloud Storage,Microsoft OneDrive,D,../data/dataiku\partitions\fs_datasets.md
What is the purpose of the partition redispatch feature in Dataiku DSS?,To duplicate columns before partitioning,To transform a non-partitioned dataset to a partitioned dataset,To remove partition columns from the schema,To create a prepare recipe,B,../data/dataiku\partitions\fs_datasets.md
Which of the following statements is true about Hive recipes in Dataiku DSS?,Hive recipes act on SQL databases using column-based partitioning.,Hive recipes act on HDFS datasets using files-based partitioning.,Hive recipes do not support partitioning.,Hive recipes require a WHERE clause to restrict selected partitions.,B,../data/dataiku\partitions\hive.md
What must be included in the PARTITION clause when writing data in a Hive recipe?,Dynamic values computed using the query itself.,Static values that do not change.,Values that are computed at runtime.,Values that are fetched from an external source.,B,../data/dataiku\partitions\hive.md
What is the format for identifying a partition with a granularity of 'Hour'?,YYYY,YYYY-MM,YYYY-MM-DD,YYYY-MM-DD-HH,D,../data/dataiku\partitions\identifiers.md
Which of the following is a valid example of a partition identifier with multiple dimensions?,2020-01-01|France,2020-01-01,France|Italy,2020-01-01-13,A,../data/dataiku\partitions\identifiers.md
Which of the following special keywords can be used to refer to the current month in a time dimension identifier?,CURRENT_DAY,CURRENT_MONTH,CURRENT_YEAR,CURRENT_HOUR,B,../data/dataiku\partitions\identifiers.md
Which of the following storage options is NOT used in files-based partitioning?,HDFS,Amazon S3,SQL databases,Azure Blob Storage,C,../data/dataiku\partitions\index.md
"In column-based partitioning, which of the following is NOT a type of database used?",SQL databases,NoSQL databases,Filesystem,Elasticsearch,C,../data/dataiku\partitions\index.md
What can you train from partitioned datasets?,Partitioned prediction models,Unsupervised learning models,Reinforcement learning models,Clustering models,A,../data/dataiku\partitions\models.md
"When a recipe is used to compute a partitioned dataset in Dataiku DSS, what does it process?",The entire dataset,Only the involved partitions,Random partitions,None of the above,B,../data/dataiku\partitions\recipes.md
What must be true for all output datasets when a recipe computes several datasets in Dataiku DSS?,They must have different partitioning schemas,They must have the same partitioning schema,They must be stored in the same location,They must be of the same size,B,../data/dataiku\partitions\recipes.md
What does a single invocation of a recipe in Dataiku DSS do?,Reads the entire input dataset and writes the entire output dataset,Reads one or several partitions of the input datasets and writes exactly one partition for each output dataset,Reads random partitions of the input datasets and writes random partitions for each output dataset,Reads and writes the entire dataset without partitioning,B,../data/dataiku\partitions\recipes.md
Which of the following is NOT a requirement for partitioning an external SQL table dataset in Dataiku DSS?,Configuring which columns provide the partitioning values,Specifying the type and parameters of each partitioning dimension,Ensuring the partitioning columns are of Date type,Providing a way to list the partitions,C,../data/dataiku\partitions\sql_datasets.md
"When using time-based partitioning on an SQL table in Dataiku DSS, what type should the partitioning columns NOT be?",String,Integer,Date,Varchar,C,../data/dataiku\partitions\sql_datasets.md
"In SQL recipes, how should you handle reading from partitioned datasets?",Manually restrict what is being read in your query.,Use a predefined template for all queries.,Hard-code the partition in your recipe.,Use a different SQL recipe for each partition.,A,../data/dataiku\partitions\sql_recipes.md
"When writing into partitioned datasets using an SQL Query recipe, what must appear in the output data?",Only the columns specified in the input dataset.,The partitioning columns at the correct position with the correct name.,A unique identifier for each row.,A timestamp for each record.,B,../data/dataiku\partitions\sql_recipes.md
What is required when writing into partitioned datasets using an SQL script recipe?,Ensuring idempotence and inserting records with the correct partitioning values.,Using a predefined template for all scripts.,Hard-coding the partition values in the script.,Using a different script for each partition.,A,../data/dataiku\partitions\sql_recipes.md
Which of the following can be added through plugins in Dataiku DSS?,Datasets,Recipes,Visual Machine Learning algorithms,All of the above,D,../data/dataiku\plugins\index.md
Which of the following methods is NOT a way to install plugins on a Dataiku DSS instance?,Installing from the Store,Installing from a ZIP file,Installing from a Git repository,Installing from a Docker container,D,../data/dataiku\plugins\installing.md
What permissions are required to install plugins on a Dataiku DSS instance?,Read permissions,Write permissions,Admin permissions,Execute permissions,C,../data/dataiku\plugins\installing.md
What action should you take if a pull fails due to a conflict in Dataiku?,Create a new local branch and push it,Directly resolve conflicts in DSS,Delete the remote branch,Ignore the conflict and continue working,A,../data/dataiku\plugins\reference\git-editor.md
Which of the following is NOT a feature available when associating a Git remote repository to your plugin in Dataiku?,Fetch,Push,Merge branches directly in DSS,Pull,C,../data/dataiku\plugins\reference\git-editor.md
What is the first step to revert your plugin to a specific revision in Dataiku?,Click on the branch indicator,Click on 'History' and select the revision,Push the current branch to the remote,Fetch the latest changes from the remote,B,../data/dataiku\plugins\reference\git-editor.md
Which of the following is NOT a type of parameter mentioned in the context?,Basic types,Complex types,DSS object parameters,Advanced types,D,../data/dataiku\plugins\reference\index.md
What is one of the actions you can perform with Git integration in the plugin editor?,Viewing history,Creating plugins,Running tests,Deploying models,A,../data/dataiku\plugins\reference\index.md
Which file is used to declare the required packages for a Dataiku DSS plugin?,plugin.json,requirements.json,desc.json,environment.spec,B,../data/dataiku\plugins\reference\other.md
"In a Dataiku DSS plugin, where should you place shared code that needs to be accessible by multiple datasets and recipes?",resource folder,python-lib folder,code-env folder,spec folder,B,../data/dataiku\plugins\reference\other.md
What is the purpose of the 'resources_init.py' file in a Dataiku DSS plugin?,To declare required packages,To initialize the code environment resources directory,To define the environment characteristics,To categorize recipes and datasets,B,../data/dataiku\plugins\reference\other.md
Which of the following is NOT a basic type of parameter in Dataiku?,BOOLEAN,DATE,MAP,DOUBLE,C,../data/dataiku\plugins\reference\params.md
What type of parameter would you use to select multiple values among possible choices in Dataiku?,SELECT,MULTISELECT,ARRAY,OBJECT_LIST,B,../data/dataiku\plugins\reference\params.md
"In Dataiku, which parameter type is used to display a parameter based on the value of other parameters?",visibilityCondition,conditionalParameter,dynamicParameter,dependentParameter,A,../data/dataiku\plugins\reference\params.md
Which of the following is NOT a component that can be part of a Dataiku DSS plugin?,Dataset,Recipe,API Gateway,Webapps,C,../data/dataiku\plugins\reference\plugins-components.md
What is the recommended practice for the 'id' element in the plugin.json file?,It should be a random string,It should be the same as the name of the top-level directory,It should be a numeric value,It should be left empty,B,../data/dataiku\plugins\reference\plugins-components.md
Which of the following is a correct description of the 'meta' field in a component's JSON file?,It contains the code that defines what the component does,"It is used for display purposes, including the name, description, and icon of the component",It specifies the version of the component,It lists the dependencies of the component,B,../data/dataiku\plugins\reference\plugins-components.md
Which of the following date formats is recognized as valid by Dataiku DSS?,ISO-8601 with timezone indicator,RFC 822,MM/DD/YYYY,DD-MM-YYYY,"A, B",../data/dataiku\preparation\dates.md
What is the purpose of the 'Smart Date Parser' in Dataiku DSS?,To detect probable formats of unparsed date columns and assist in generating the correct format,To convert UNIX timestamps to a date format,"To extract date components like year, month, and day",To filter rows based on date ranges,A,../data/dataiku\preparation\dates.md
Which of the following methods can be used to determine the timezone for a given row in Dataiku DSS?,Using a static value,Using a timezone column,Using an IP address column,Using a date format column,"A, B, C",../data/dataiku\preparation\dates.md
Which of the following dataset types is recommended for fast read and write on Spark?,S3,Azure Blob Storage,Google Cloud Storage,All of the above,D,../data/dataiku\preparation\engines.md
Which of the following processors is NOT available with SQL processing?,Keep/Delete columns,Rename columns,Extract numbers,Concatenate columns,C,../data/dataiku\preparation\engines.md
When does execution on the whole dataset happen during analysis?,When designing a data preparation,When exporting the prepared data,When running a machine learning model,Both B and C,D,../data/dataiku\preparation\engines.md
Which of the following is NOT a type of filter provided by DSS for filtering data?,Filter on value,Filter on numerical range,Filter on text length,Filter on date range,C,../data/dataiku\preparation\filter-flag.md
What does the 'Normalize' mode do in the context of filtering on value?,Performs a case-sensitive search,Performs a case-insensitive search,Performs an accents-insensitive search,Performs a search using regular expressions,C,../data/dataiku\preparation\filter-flag.md
Which of the following is NOT a valid match condition for filtering rows/cells with a formula?,A true boolean,A number which is not 0,The string 'true',The string 'false',D,../data/dataiku\preparation\filter-flag.md
Which processor in DSS uses the GeoIP database to resolve an IP address to geographic coordinates?,Geopoint converters,Resolve GeoIP,Reverse geocoding,Zipcode geocoding,B,../data/dataiku\preparation\geographic.md
What type of data does the 'Extract from geo column' processor extract from a geometry column?,"Centroid point, length, and area",Latitude and longitude,Country and city,Rectangular and circular polygons,A,../data/dataiku\preparation\geographic.md
Which processor would you use to convert data projected in a different CRS to the WGS84 (EPSG:4326) coordinates system?,Create GeoPoint from lat/lon,Change coordinates system,Compute distances between geospatial objects,Create area around a geopoint,B,../data/dataiku\preparation\geographic.md
Which of the following is NOT a feature of visual data preparation in Dataiku DSS?,Creating data cleansing scripts,Normalizing data,Enriching data,Building machine learning models,D,../data/dataiku\preparation\index.md
Which of the following processors can be used to handle date-related operations in Dataiku DSS?,Extract date elements,Compute distance between geospatial objects,Normalize measure,Tokenize text,A,../data/dataiku\preparation\index.md
Which of the following is a geographic processor available in Dataiku DSS?,Extract ngrams,Resolve GeoIP,Split column,Formula,B,../data/dataiku\preparation\index.md
Which processor in DSS is used to create new lines by splitting the values of a column?,Fold multiple columns,Unfold,Split and Fold,Triggered Unfold,C,../data/dataiku\preparation\reshaping.md
"What is the result of applying the 'Unfold' processor on a column with values 'A', 'B', and 'C'?",Transforms cell values into binary columns,Transforms array values into occurrence columns,Creates new lines by splitting the values of a column,Takes values from multiple columns and transforms them to one line per column,A,../data/dataiku\preparation\reshaping.md
Which processor is used to transform array values into occurrence columns?,Split and Fold,Unfold an array,Fold multiple columns by pattern,Split and Unfold,B,../data/dataiku\preparation\reshaping.md
Which of the following statements is true about the 'Fixed size intervals' binning mode?,The upper bound is included and the lower bound is excluded.,The lower bound is included and the upper bound is excluded.,Both bounds are included.,Both bounds are excluded.,B,../data/dataiku\preparation\processors\binner.md
Which of the following hashing algorithms is supported by PostgreSQL for pseudonymization?,MD5,SHA-256,SHA-512,None of the above,A,../data/dataiku\preparation\processors\column-pseudonymization.md
What is the main difference between pseudonymization and anonymization?,"Pseudonymization replaces unique attributes with pseudonyms, while anonymization irreversibly destroys any way of identifying the data subject.","Pseudonymization is reversible, while anonymization is not.","Pseudonymization is used for data encryption, while anonymization is used for data masking.","Pseudonymization is a type of data compression, while anonymization is a type of data encryption.",A,../data/dataiku\preparation\processors\column-pseudonymization.md
Which of the following is an optional step in the pseudonymization process?,Selecting a column for Salt,Selecting a hashing algorithm,Adding a new step of type 'Pseudonymize text',Selecting an option for Column,A,../data/dataiku\preparation\processors\column-pseudonymization.md
Which of the following matching modes counts complete cell values?,Complete value,Substring,Regular expression,Case-sensitive,A,../data/dataiku\preparation\processors\count-matches.md
Which normalization mode is only available for 'complete value' matching?,Case-sensitive matches,Case-insensitive matches,Accents-insensitive matches,Regular expression matches,C,../data/dataiku\preparation\processors\count-matches.md
"Which of the following is NOT a component of a condition in an if, then, else statement?",Column,Operator,Type of comparison,Output column,D,../data/dataiku\preparation\processors\create-if-then-else.md
"What type of operators would you use for numerical columns in an if, then, else statement?",Contains,Equals,Matches the regex,Greater than,D,../data/dataiku\preparation\processors\create-if-then-else.md
"Which of the following is NOT a type of action in an if, then, else statement?",Output column,Type of output,The item that is assigned,Condition,D,../data/dataiku\preparation\processors\create-if-then-else.md
Which of the following features is supported by the currency conversion processor?,It supports around 20 currencies with historical data.,It can only use a constant input currency.,It can output all data in the same currency.,It does not support historical data.,C,../data/dataiku\preparation\processors\currency-converter.md
Which of the following date elements can be extracted from an ISO-8601 formatted date in Dataiku DSS?,"Year, month, day","Hour, minutes, seconds","Week of year, timestamp",All of the above,D,../data/dataiku\preparation\processors\date-components-extract.md
What is the default timezone used when extracting date elements in Dataiku DSS?,UTC,PST,EST,GMT,A,../data/dataiku\preparation\processors\date-components-extract.md
Which of the following is NOT a common pattern used in the Java syntax for date specifier?,y (year),M (month in year),E (day name in week),P (period in day),D,../data/dataiku\preparation\processors\date-formatter.md
What is the default timezone used in Dataiku DSS for date formatting?,PST,EST,UTC,CET,C,../data/dataiku\preparation\processors\date-formatter.md
Which of the following is NOT an option for applying date parsing in Dataiku DSS?,A single column,An explicit list of columns,All columns matching a regex pattern,All columns except those matching a regex pattern,D,../data/dataiku\preparation\processors\date-parser.md
What is the default timezone used in Dataiku DSS for date parsing?,UTC,PST,EST,GMT,A,../data/dataiku\preparation\processors\date-parser.md
Which of the following types of data is NOT provided by the 'Enrich from French postcode' processor?,Basic demography data,Housing data,Weather data,Employment data,C,../data/dataiku\preparation\processors\enrich-french-postcode.md
The demographic data provided by the 'Enrich from French postcode' processor is based on which census years?,2005 and 2006,2007 and 2008,2009 and 2010,2011 and 2012,C,../data/dataiku\preparation\processors\enrich-french-postcode.md
Which of the following is NOT a simplification option offered by the NGram extractor?,Normalize text,Clear stop words,Stem words,Remove punctuation,D,../data/dataiku\preparation\processors\extract-ngrams.md
What is the primary purpose of the 'One ngram per row' output mode in the NGram extractor?,To generate a JSON array containing the ngrams,"To create a new row for each ngram, useful for grouping by ngram",To generate a new column for each ngram,To retain the structure of the original text,B,../data/dataiku\preparation\processors\extract-ngrams.md
What is a potential consequence of enabling skip-grams computation in the NGram extractor?,It simplifies the text by removing stop words,It normalizes the text to lowercase,It dramatically increases output size and computation requirements,It sorts words alphabetically,C,../data/dataiku\preparation\processors\extract-ngrams.md
Which option allows the processor to output each detected number into a separate column?,Extract several values,Extract values into a JSON array,Expand 'k' to '1000' and 'm' to '1000000',Decimal separator,A,../data/dataiku\preparation\processors\extract-numbers.md
What does the option 'Expand ‘k’ to ‘1000’ and ‘m’ to ‘1000000’' do?,It extracts several values and outputs each detected number into a separate column.,It outputs the found number(s) in a single column as a JSON-array.,It automatically expands notations like ‘10K’ and ‘5M’.,It uses the program’s best guess or chooses from between comma and dot separators.,C,../data/dataiku\preparation\processors\extract-numbers.md
Which of the following definitions is correct for the median?,The sum of values of a data set divided by the number of values.,The middle value separating the greater and lesser halves of a set of values.,The most frequent value in a set of values.,The value that appears most often in a data set.,B,../data/dataiku\preparation\processors\fill-empty-with-computed-value.md
"In the data set '1, 2, 2, 3, 4, 7, 9', what is the mode?",1,2,3,4,B,../data/dataiku\preparation\processors\fill-empty-with-computed-value.md
Which of the following options is NOT a way to apply the fill operation to columns?,A single column,An explicit list of columns,All columns matching a regex pattern,A random selection of columns,D,../data/dataiku\preparation\processors\fill-empty.md
Which of the following actions can be performed on matching rows or cells when filtering by date?,Keep matching rows only,Remove matching rows,Clear content of matching cells,All of the above,D,../data/dataiku\preparation\processors\filter-on-date.md
Which of the following actions can be performed on matching rows or cells when filtering invalid rows/cells in Dataiku?,Keep matching rows only,Remove matching rows,Clear content of matching cells,All of the above,D,../data/dataiku\preparation\processors\filter-on-meaning.md
"When applying the match condition to several columns in Dataiku, which options are available to determine if a row is considered as matching?",ALL,OR,AND,XOR,"A, B",../data/dataiku\preparation\processors\filter-on-meaning.md
Which of the following actions can be performed on matching rows or cells when filtering on a numerical range?,Keep matching rows only,Remove matching rows,Clear content of matching cells,All of the above,D,../data/dataiku\preparation\processors\filter-on-range.md
Which of the following actions can be performed on matching rows or cells in Dataiku?,Keep matching rows only,Remove matching rows,Clear content of matching cells,All of the above,D,../data/dataiku\preparation\processors\filter-on-value.md
"When applying the match condition to several columns, which option considers a row as matching if at least one column matches?",ALL,OR,AND,NONE,B,../data/dataiku\preparation\processors\filter-on-value.md
Which normalization mode in Dataiku uses case-sensitive search?,Exact (no transformation),Ignore case,Normalize (ignore accents),Substring,A,../data/dataiku\preparation\processors\filter-on-value.md
Which of the following matching modes is NOT supported by the find and replace feature?,Complete value,Substring,Regular expression,Partial match,D,../data/dataiku\preparation\processors\find-replace.md
What does the 'Normalization mode' option specify in the find and replace feature?,How to find the match,The type of replacement to perform,The columns to apply find and replace to,The output column,A,../data/dataiku\preparation\processors\find-replace.md
Which of the following actions can be performed on matching rows when using the date range flagging processor?,Remove matching rows,Keep matching rows only,Clear the content of the matching cells,All of the above,D,../data/dataiku\preparation\processors\flag-on-date.md
"When flagging rows based on a relative range, which timezone is used to interpret the dates in the dataset?",Local timezone,UTC timezone,GMT timezone,EST timezone,B,../data/dataiku\preparation\processors\flag-on-date.md
What type of value will flag a row as a match when using a formula in Dataiku?,A false boolean,A number that is 0,The string 'false',A true boolean,D,../data/dataiku\preparation\processors\flag-on-formula.md
What should be the content of the column created to flag rows based on a formula in Dataiku?,"1 for matching rows, empty for unmatched rows","0 for matching rows, 1 for unmatched rows","True for matching rows, false for unmatched rows","Empty for matching rows, 1 for unmatched rows",A,../data/dataiku\preparation\processors\flag-on-formula.md
What does the 'Flag invalid rows' processor do?,It deletes rows with invalid values.,It flags rows with invalid values by creating a column containing '1' if the row is invalid.,It replaces invalid values with default values.,It duplicates rows with invalid values.,B,../data/dataiku\preparation\processors\flag-on-meaning.md
What does the processor create for rows that match the numerical range condition?,A column containing '0',A column containing '1',A column containing 'True',A column containing 'Yes',B,../data/dataiku\preparation\processors\flag-on-range.md
Which of the following is NOT a way to select columns for the numerical range processor?,A single column,An explicit list of columns,All columns matching a given pattern,All columns containing numerical values,D,../data/dataiku\preparation\processors\flag-on-range.md
"When using the numerical range processor, how can a row be considered as matching?",If all columns are matching,If at least one column is matching,If the row contains a valid numerical value,Both A and B,D,../data/dataiku\preparation\processors\flag-on-range.md
What is the result of applying the 'Fold multiple columns' processor to a dataset?,It transforms a narrow dataset into a wide one.,It removes duplicate rows from the dataset.,It transforms a wide dataset into a narrow one.,It sorts the dataset based on a specified column.,C,../data/dataiku\preparation\processors\fold-columns-by-name.md
What is the purpose of the 'Fold multiple columns by pattern' processor in Dataiku?,To transform values from multiple columns into one line per column.,To split a single column into multiple columns based on a pattern.,To aggregate data from multiple rows into a single row.,To delete columns that match a specific pattern.,A,../data/dataiku\preparation\processors\fold-columns-by-pattern.md
What happens if the pattern used in 'Fold multiple columns by pattern' has a capture group?,The processor ignores the capture group.,The processor uses the captured portion of the column name instead of the full column name.,The processor creates a new column for each capture group.,The processor deletes the columns that match the capture group.,B,../data/dataiku\preparation\processors\fold-columns-by-pattern.md
Which option should be checked to delete folded columns after running the 'Fold multiple columns by pattern' processor?,Columns to fold pattern,Column for fold name,Column for fold value,Remove folded columns,D,../data/dataiku\preparation\processors\fold-columns-by-pattern.md
Which of the following is NOT a feature provided by the formula language in Dataiku?,Math functions,String manipulation functions,Graph plotting functions,Boolean and conditional expressions for rules creation,C,../data/dataiku\preparation\processors\formula.md
Which of the following is an example of a rule-based column creation in Dataiku?,(col1 + col2) / 2 * log(col3),"toLowercase(substring(strip(mytext), 0, 7))","if (width > height, 'wide', 'tall')",col1 * col2 + col3,C,../data/dataiku\preparation\processors\formula.md
What type of support does the formula editor in Dataiku provide?,Live syntax checking,Code auto-completion,Graphical user interface for formula creation,Automated data cleaning,A,../data/dataiku\preparation\processors\formula.md
What is the main limitation of performing a fuzzy join in memory?,The dataset size,The number of columns,The type of data,The number of rows,A,../data/dataiku\preparation\processors\fuzzy-join.md
Which of the following is NOT a built-in simplification option for the fuzzy join processor?,Normalize text,Clear stop words,Sort words alphabetically,Remove punctuation,D,../data/dataiku\preparation\processors\fuzzy-join.md
What is the recommended maximum number of rows for the 'other' dataset in a memory-based fuzzy join?,"100,000 rows","500,000 rows","1,000,000 rows","2,000,000 rows",B,../data/dataiku\preparation\processors\fuzzy-join.md
Which of the following formats is not supported with the SQL Engine when using PostgreSQL?,WGS-84,SRID 4326,EPSG:4326,GeoJSON,D,../data/dataiku\preparation\processors\geo-distance.md
What does the processor assume about the data when computing the distance between geospatial objects?,The data uses Cartesian coordinates,The data uses GPS coordinates,The data uses UTM coordinates,The data uses Mercator projection,B,../data/dataiku\preparation\processors\geo-distance.md
Which of the following is NOT a valid comparison for computing geodesic distance in the processor?,A fixed geopoint,A fixed geospatial object,Another geospatial column,A GeoJSON object,D,../data/dataiku\preparation\processors\geo-distance.md
Which of the following data can be extracted from a geometry column using the 'Extract from geo column' processor?,Centroid point,Length,Area,All of the above,D,../data/dataiku\preparation\processors\geo-info-extractor.md
What is the primary function of the Geo-join processor?,To perform a geographic nearest-neighbour join between two datasets with geo coordinates.,To calculate the average distance between geo-tagged points.,To filter out geo-tagged events based on specific criteria.,To visualize geo-tagged data on a map.,A,../data/dataiku\preparation\processors\geo-join.md
Which of the following is a requirement for the dataset being processed by the Geo-join processor?,It must contain a timestamp column.,It must contain two columns with latitude and longitude.,It must be sorted by event type.,It must be in CSV format.,B,../data/dataiku\preparation\processors\geo-join.md
What additional column does the Geo-join processor output?,join_distance,geo_tag,event_type,timestamp,A,../data/dataiku\preparation\processors\geo-join.md
Which of the following shapes can be selected for creating buffer polygons around geopoints?,Triangular,Rectangular,Hexagonal,Circular,"B, D",../data/dataiku\preparation\processors\geopoint-buffer.md
What units can be selected for distances when creating buffer polygons around geopoints?,Meters,Kilometers,Miles,Feet,"B, C",../data/dataiku\preparation\processors\geopoint-buffer.md
What format does the processor use to represent GeoPoints?,JSON,WKT,XML,CSV,B,../data/dataiku\preparation\processors\geopoint-create.md
What does the GeoPoint format represent in Dataiku?,A point on Earth,A point in space,A point in a 3D model,A point in a graph,A,../data/dataiku\preparation\processors\geopoint-extract.md
Which of the following countries has the default timezone 'Europe/Berlin'?,FR,DE,ES,IN,B,../data/dataiku\preparation\processors\holidays-computer.md
What does the 'Flag holidays' processor output for a given date?,A single boolean column indicating if it is a holiday,"Three boolean columns indicating if it is a school holiday, bank holiday, or weekend",A timestamp of the given date,A timezone-less tuple of the given date,B,../data/dataiku\preparation\processors\holidays-computer.md
Which of the following processors in DSS is used to create a new column by copying an existing one?,Rename columns,Copy column,Concatenate columns,Delete/Keep columns by name,B,../data/dataiku\preparation\processors\index.md
What is the purpose of the 'Discretize (bin) numerical values' processor in DSS?,To sort numerical values,To convert numerical values to a different format,To group numerical values into bins,To normalize numerical values,C,../data/dataiku\preparation\processors\index.md
Which processor would you use to extract specific elements from a date column in DSS?,Format date with custom format,Parse to standard date format,Extract date elements,Compute difference between dates,C,../data/dataiku\preparation\processors\index.md
What does the 'Split invalid cells into another column' processor do?,It deletes all invalid values in a column.,It moves all values of a column that are invalid for a specific meaning to another column.,It merges invalid values with valid ones in the same column.,It duplicates the column with invalid values.,B,../data/dataiku\preparation\processors\invalid-split.md
"In the given example, what is the meaning checked for the column 'icol'?",Text,Date,Number,Boolean,C,../data/dataiku\preparation\processors\invalid-split.md
What is a requirement for the 'other' dataset when performing a memory-based join?,It must be in the same project.,"It must contain less than 1,000,000 rows.",It must be stored in a SQL database.,It must be in CSV format.,A,../data/dataiku\preparation\processors\join.md
What happens if multiple rows match in the 'other' dataset during a memory-based join?,The first row is selected.,All matching rows are selected.,"The last row is selected, but ordering is not guaranteed.",An error is thrown.,C,../data/dataiku\preparation\processors\join.md
Which parameter is NOT required for the memory-based join processor?,Column containing the join key in the current dataset.,Name of the dataset to join with.,Columns from the joined dataset to copy.,"The type of join to perform (inner, outer, etc.).",D,../data/dataiku\preparation\processors\join.md
What is the main use case of the 'Group long-tail values' processor?,To merge all values of a column except the most frequent ones.,To split a column into multiple columns based on value frequency.,To delete all values of a column except the most frequent ones.,To sort a column based on value frequency.,A,../data/dataiku\preparation\processors\long-tail.md
"What will be the result of computing the mean of the values [1, 2, ''] using the processor described?",1,1.5,2,An error,B,../data/dataiku\preparation\processors\mean.md
What happens if all columns are empty when computing the mean using the processor?,The result will be zero,The result will be an empty cell or a default value,The result will be an error,The result will be the mean of the non-empty columns,B,../data/dataiku\preparation\processors\mean.md
Which of the following operations are performed by the numerical combinations processor in Dataiku?,Addition and Subtraction,Multiplication and Division,All of the above,None of the above,C,../data/dataiku\preparation\processors\numerical-combinations.md
Which option is NOT a feature of the 'Unnest object (flatten JSON)' processor?,Input column,Maximum depth,Convert null to empty cell,Generate summary statistics,D,../data/dataiku\preparation\processors\object-unnest-json.md
What happens by default when the 'Unnest object (flatten JSON)' processor encounters a JSON array nested within a JSON object?,The JSON array is flattened into multiple columns.,The JSON array is preserved as a single column.,The processor stops unnesting.,The processor converts the array to a string.,B,../data/dataiku\preparation\processors\object-unnest-json.md
Which of the following options is used to create a column indicating whether or not a pattern matched in Dataiku?,Regular expression,Capture groups,Found column,Extract all occurrences,C,../data/dataiku\preparation\processors\pattern-extract.md
"In Dataiku, what syntax is used for unnamed capture groups in regular expressions?",(pattern),(?<groupname>pattern),(groupname:pattern),(pattern:groupname),A,../data/dataiku\preparation\processors\pattern-extract.md
Which of the following modes allows a Python function to produce a new cell for each row in a non-vectorized manner?,Cell,Row,Rows,Batch,A,../data/dataiku\preparation\processors\python-custom.md
What is the main advantage of using vectorized processing in Dataiku's Python function?,It allows the use of Jython.,It provides much improved performance.,It restricts the use of Python libraries.,It simplifies error handling.,B,../data/dataiku\preparation\processors\python-custom.md
What does the Split HTTP Query String processor do?,Combines multiple HTTP query strings into one.,Splits the elements of an HTTP query string.,Encrypts the HTTP query string.,Deletes the HTTP query string.,B,../data/dataiku\preparation\processors\querystring-split.md
"In the Split HTTP Query String processor, what is the output format?",One column for each chunk of the query string with a prefix and key.,A single column with concatenated query string values.,A JSON object with all query string values.,An XML file with query string values.,A,../data/dataiku\preparation\processors\querystring-split.md
"When removing rows where cells are empty, how is a row considered matching if multiple columns are selected?",If all selected columns are empty,If at least one of the selected columns is empty,If none of the selected columns are empty,If the first selected column is empty,B,../data/dataiku\preparation\processors\remove-empty.md
What is the purpose of specifying 'Chunk Overlap' in the text splitting processor?,To ensure context continuity across chunks.,To define the maximum number of characters each chunk can contain.,To specify the separators to consider for splitting the text.,To keep separators in the output chunks.,A,../data/dataiku\preparation\processors\split-into-chunks.md
Which of the following options is NOT a default separator used by the text splitting processor?,Double new lines,New line,Comma,Space,C,../data/dataiku\preparation\processors\split-into-chunks.md
What is the primary function of the 'Split and Unfold' processor in Dataiku?,To merge multiple columns into a single column.,To split the values of a column based on a separator and transform them into several binary columns.,To delete duplicate rows in a dataset.,To aggregate data based on a specific key.,B,../data/dataiku\preparation\processors\split-unfold.md
Which of the following transformations can be applied using the 'Transform string' feature in Dataiku?,Convert to uppercase/lowercase,Encode/decode URL,Escape/unescape XML entities,All of the above,D,../data/dataiku\preparation\processors\string-transform.md
Which of the following normalization modes allows for case-insensitive matches?,Exact mode,Lowercase mode,Normalize mode,Strict mode,B,../data/dataiku\preparation\processors\switch-case.md
Which of the following output modes of the tokenizer generates a new row for each token?,Convert to array,One token per row,One token per column,Normalize text,B,../data/dataiku\preparation\processors\tokenizer.md
What does the 'Normalize text' simplification option do?,"Transforms to lowercase, removes accents, and performs Unicode normalization",Removes stop words,Transforms each word into its grammatical root,Sorts all words of the text alphabetically,A,../data/dataiku\preparation\processors\tokenizer.md
What is the primary purpose of the 'Triggered unfold' processor in Dataiku?,To split a dataset into multiple files,To reassemble several rows when a specific value is encountered,To sort data by user ID,To delete non-terminated sessions,B,../data/dataiku\preparation\processors\triggered-unfold.md
Which of the following is a limitation of the 'Triggered unfold' processor?,It can only handle datasets with fewer than 100 rows,It does not work on unsorted data,It cannot be used for session analysis,It requires data to be split into multiple files,B,../data/dataiku\preparation\processors\triggered-unfold.md
"In the 'Triggered unfold' example, what is the trigger value used to mark the beginning of a new session?",event_type2,login_event,timestamp,user_id,B,../data/dataiku\preparation\processors\triggered-unfold.md
What does the 'Unfold an array' processor do in Dataiku?,Transforms a column containing JSON-formatted arrays into several columns with the number of occurrences of each term.,Combines multiple columns into a single JSON-formatted array.,Deletes columns containing JSON-formatted arrays.,Transforms a column containing JSON-formatted arrays into a single binary column.,A,../data/dataiku\preparation\processors\unfold-array.md
What option should be unchecked to transform the original column into binary columns in the 'Unfold an array' processor?,Prefix,Max nb. columns to create,Count of Values,Number of Occurrences,C,../data/dataiku\preparation\processors\unfold-array.md
What is another term for the 'Unfold' processor in Dataiku?,Normalization,Dummification,Standardization,Binarization,B,../data/dataiku\preparation\processors\unfold.md
What is a potential limitation of using the Unfold processor in Dataiku?,It can only handle numerical data.,It cannot create more than 10 columns.,It may cause performance issues if the column has more than 100 values.,It does not support prefixing new columns.,C,../data/dataiku\preparation\processors\unfold.md
Which of the following is NOT a possible value for the 'type' column created by the User-Agent processor?,BROWSER,TABLET,ROBOT,LIBRARY,C,../data/dataiku\preparation\processors\user-agent.md
What does the 'category' column in the User-Agent processor aim to distinguish?,Different versions of the same operating system,Browsers of the same brand that behave differently,Different types of devices,Different brands of browsers,B,../data/dataiku\preparation\processors\user-agent.md
Which of the following pieces of information is NOT used by the processor to generate a best-effort visitor id?,IP address,User-agent string,Browser language,Operating system,D,../data/dataiku\preparation\processors\visitor-id.md
What is the primary function of the 'Zip JSON arrays' processor?,Combining multiple JSON arrays into a single JSON object.,Combining N input columns containing arrays (as JSON) into a single output column.,Splitting a single JSON array into multiple columns.,Converting JSON arrays into CSV format.,B,../data/dataiku\preparation\processors\zip-arrays.md
Which of the following operations can be performed on projects using the DSS public API?,"Create, list, and delete projects",Manage project permissions,Manage project metadata,All of the above,D,../data/dataiku\publicapi\features.md
What can you do with datasets using the DSS public API?,"Create, list, and delete datasets",Manage datasets metadata and schema,Read datasets data,All of the above,D,../data/dataiku\publicapi\features.md
Which of the following is NOT an operation you can perform on jobs using the DSS public API?,Start jobs / build datasets,List jobs,Manage project metadata,Abort jobs,C,../data/dataiku\publicapi\features.md
Which of the following is the recommended way to interact with the DSS public API?,Using the HTTP REST API,Using the Python API client,Using a Java API client,Using a command-line interface,B,../data/dataiku\publicapi\index.md
What can the DSS public API be used for?,Performing administration and maintenance operations,Accessing datasets managed by DSS,Both A and B,Neither A nor B,C,../data/dataiku\publicapi\index.md
Which type of API key in DSS can only be created by the end user?,Project-level API keys,Global API keys,Personal API keys,Admin API keys,C,../data/dataiku\publicapi\keys.md
Which of the following permissions can be granted on a set of datasets using project-level API keys?,READ_CONF,WRITE_CONF,READ_DATA,MANAGE_USERS,C,../data/dataiku\publicapi\keys.md
What special permission gives global admin rights to a global API key in DSS?,READ_CONF,WRITE_CONF,GLOBAL_ADMIN,MANAGE_USERS,C,../data/dataiku\publicapi\keys.md
Which HTTP status code indicates a successful request in the DSS REST API?,1xx,2xx,3xx,4xx,B,../data/dataiku\publicapi\rest.md
What must be the format of the request body for POST and PUT requests in the DSS REST API?,XML,HTML,JSON,Plain Text,C,../data/dataiku\publicapi\rest.md
Which of the following is NOT a method of authentication for the DSS REST API?,Bearer Token Authentication,Basic Authentication,OAuth,API Key Authentication,C,../data/dataiku\publicapi\rest.md
Which of the following methods can be used to display Bokeh charts in a Jupyter notebook?,Using the 'show()' function after importing necessary Bokeh modules.,Using the 'display()' function from the Bokeh library.,Embedding the chart directly in HTML code.,Using the 'plot()' function from the Bokeh library.,A,../data/dataiku\python\bokeh.md
Where should images be stored to be displayed in a Bokeh webapp?,In the 'DATADIR/local/static' directory.,In the 'DATADIR/local/images' directory.,In the 'DATADIR/webapp/static' directory.,In the 'DATADIR/webapp/images' directory.,A,../data/dataiku\python\bokeh.md
What is the recommended way to include interactive controls in a Bokeh chart displayed in a Jupyter notebook?,Using the 'output_notebook()' function.,Using the 'show()' function.,Using interactive controls like sliders and inputs.,Embedding the chart in a webapp.,C,../data/dataiku\python\bokeh.md
Which of the following is NOT a way to use Python code in DSS?,In recipes,In Jupyter notebooks,In standard webapp backends,In Excel spreadsheets,D,../data/dataiku\python\index.md
Which Python package is mentioned for creating interactive web applications in DSS?,Matplotlib,SpaCy,Bokeh,Plot.ly,C,../data/dataiku\python\index.md
What is the recommended method for installing Python packages in DSS?,Installing in a specific code environment,Installing globally on the system,Using a virtual machine,Directly editing the system's Python path,A,../data/dataiku\python\index.md
Which of the following is NOT a way to display Matplotlib charts in DSS?,In a Jupyter notebook,On a Dashboard,In a DSS recipe,In a PowerPoint presentation,D,../data/dataiku\python\matplotlib.md
What command should be executed first in a Jupyter notebook to display Matplotlib charts inline?,%pylab inline,import matplotlib.pyplot as plt,import numpy as np,"plt.plot(t, s)",A,../data/dataiku\python\matplotlib.md
Which backend must be used in a DSS recipe to render Matplotlib charts in-memory?,Tkinter,Agg,TCL/TK,Inline,B,../data/dataiku\python\matplotlib.md
What is the recommended way to install your own Python packages in DSS?,Directly on the system,In a code environment,Using a virtual machine,Through a Docker container,B,../data/dataiku\python\packages.md
What should you do if you get an error when installing a Python package in DSS?,Restart the system,Refer to code environment troubleshooting,Reinstall DSS,Contact customer support immediately,B,../data/dataiku\python\packages.md
Which of the following is NOT a way to display Plot.ly charts in DSS?,In a Jupyter notebook,On a Dashboard,As a static image in a report,Through an online service,C,../data/dataiku\python\plotly.md
What is the recommended method for installing the 'plotly' package in DSS?,Using pip directly in the terminal,Using a code environment,Downloading from the Plot.ly website,Using conda,B,../data/dataiku\python\plotly.md
How can you refresh Plot.ly charts automatically on a dashboard?,By manually updating the chart,By using a scenario to re-run the code,By refreshing the browser page,By exporting the chart as a new insight,B,../data/dataiku\python\plotly.md
Which of the following is NOT a method provided by DSS for reusing Python code?,"Packaging your code as functions or modules, and making them available in a specific project",Importing code that has been made available from one project to another,"Packaging your code as functions or modules, and making them available in all projects",Using a third-party cloud service to store and retrieve Python code,D,../data/dataiku\python\reusing-code.md
What is required for a folder to be a valid Python module when creating subfolders in a Python source folder?,A README.md file,A __init__.py file,A setup.py file,A requirements.txt file,B,../data/dataiku\python\reusing-code.md
What permission is needed to use the global Python library editor in DSS?,Read project content,Write project content,Edit lib folders,Manage code environments,C,../data/dataiku\python\reusing-code.md
Which of the following is NOT a feature of the SpaCy library?,Tokenization,Named Entity Recognition,Image Processing,Pre-trained models for several languages,C,../data/dataiku\python\spacy.md
Where can you find the documentation for SpaCy?,https://spacy.io/,https://github.com/explosion/spacy-models,https://spacy.io/usage/models#download-pip,https://spacy.io/models,A,../data/dataiku\python\spacy.md
Which command is used to load a SpaCy model in a Python notebook?,import spacy_model,nlp = spacy.load('en_core_web_sm'),spacy.load_model('en_core_web_sm'),import spacy_model as nlp,B,../data/dataiku\python\spacy.md
Which of the following is NOT a place where you can leverage the Python tools provided by the DSS API?,Recipes,Notebooks,Scenarios,Dashboards,D,../data/dataiku\python-api\index.md
Which of the following is required to display dygraphs charts in a Jupyter notebook?,Using the dygraph() method alone,Using the dkuDisplayDygraph() function,Installing the dygraphs package in a Conda environment,Creating a time-series object using xts,B,../data/dataiku\R\dygraphs.md
How can dygraphs charts be shared on a DSS dashboard?,By using the dygraph() function directly,By creating static insights,By converting the chart to a PNG image,By embedding the chart in an HTML file,B,../data/dataiku\R\dygraphs.md
What is the recommended method to install the dygraphs package in DSS?,Using the install.packages() function in R,Using a code environment,Downloading from the dygraphs website,Using the RStudio interface,B,../data/dataiku\R\dygraphs.md
Which method should be used to display ggvis charts properly in a Jupyter notebook?,dkuInstallGgvisDependenciesOnce,dkuDisplayGgvis,dkuSaveGgvisInsight,layer_points,B,../data/dataiku\R\ggvis.md
What is required to install ggvis frontend dependencies if the DSS instance has the User Isolation Framework enabled?,Run dkuInstallGgvisDependenciesOnce in a Jupyter notebook,Run dkuInstallGgvisDependenciesOnce in an R notebook,DSS administrator needs to run from a command-line ./bin/R prompt,Install the r-ggvis package in a Conda environment,C,../data/dataiku\R\ggvis.md
How can ggvis charts be shared on a DSS dashboard?,Using the 'dynamic insights' system,Using the 'static insights' system,By exporting the chart as a CSV file,By embedding the chart in an HTML file,B,../data/dataiku\R\ggvis.md
Which method should be used to display googleVis charts properly in a Jupyter R notebook?,plot(),dkuDisplayGooglevis(),gvisLineChart(),dkuSaveGooglevisInsight(),B,../data/dataiku\R\googlevis.md
How can googleVis charts be shared on a DSS dashboard?,Using the plot() method,Using the dkuDisplayGooglevis method,Using the static insights system,Using the gvisLineChart method,C,../data/dataiku\R\googlevis.md
Which of the following is NOT a way to write R code in DSS?,In recipes,In Jupyter notebooks,In Excel,In Shiny webapps,C,../data/dataiku\R\index.md
What is the recommended way to install R packages in DSS?,Installing in the root DSS environment,Installing in a specific code environment,Installing without Internet access,Installing directly from RStudio,B,../data/dataiku\R\index.md
Which R package is mentioned for displaying charts in a Jupyter notebook and on a dashboard?,ggplot2,dplyr,caret,randomForest,A,../data/dataiku\R\index.md
Which method is recommended for installing R packages in DSS?,Installing in the root DSS environment,Installing in a specific code environment,Using the 'R' binary on your PATH,Using the DSS data directory,B,../data/dataiku\R\packages.md
What is required to install packages in the built-in DSS environment?,Internet access,Shell access on the host running DSS,A specific code environment,A DSS user account,B,../data/dataiku\R\packages.md
Which of the following is required to install the STAN and Prophet packages for time series forecasting on a CentOS 7.6 system?,Installing the software collection library (SCL),Installing the latest Developer Toolset,Activating the developer toolset in the DSS user session,All of the above,D,../data/dataiku\R\prophet.md
What error might you encounter if the C++14 standard is requested but not defined?,Error in .shlib_internal(args) : C++14 standard requested but CXX14 is not defined,Error in .shlib_internal(args) : C++11 standard requested but CXX11 is not defined,Error in .shlib_internal(args) : C++17 standard requested but CXX17 is not defined,Error in .shlib_internal(args) : C++20 standard requested but CXX20 is not defined,A,../data/dataiku\R\prophet.md
Which function is used to import R functions defined in the project’s library editor into the global namespace?,sourceLibR,dkuSourceLibR,importLibR,loadLibR,B,../data/dataiku\R\reusing-code.md
What is required to import libraries from one project to another in Dataiku?,Read project content permission on the source project and write project content permission on the target project,Write project content permission on both projects,Admin permissions on both projects,No special permissions are required,A,../data/dataiku\R\reusing-code.md
What is the purpose of packaging R code as a complete R package in Dataiku?,To make the code available only in the current project,To enforce namespacing and allow installation using install.packages,To avoid using the dkuSourceLibR function,To share the code with non-coder users,B,../data/dataiku\R\reusing-code.md
Which of the following actions can be performed using the RStudio extensions in RStudio Desktop after installing the 'dataiku' package?,Setup connection to a DSS instance,Download the code of a R recipe to edit locally,Upload the edited R recipe back to DSS,All of the above,D,../data/dataiku\R\rstudio.md
What is required to embed the RStudio Server UI directly within the Dataiku UI?,Edit '/etc/rstudio/rserver.conf' and add a line 'www-frame-origin = BASE_URL_OF_DSS',Restart RStudio Server,Edit 'DSS_DATA_DIR/config/dip.properties' and add a line 'dku.rstudioServerEmbedURL=http(s)://URL_OF_RSTUDIO_SERVER/',All of the above,D,../data/dataiku\R\rstudio.md
What additional features are available when RStudio Server is running on the same host as DSS and using the User Isolation Framework?,Automatic installation of the 'dataiku' package in the user's R library,Automatic setup of connectivity between DSS and RStudio Server,Automatic creation of a RStudio Server project corresponding to a Dataiku project,All of the above,D,../data/dataiku\R\rstudio.md
Which function is used to retrieve authentication information in Dataiku from R code?,dkuGetAuthInfo,dkuGetUserInfo,dkuGetAPIKey,dkuGetSessionInfo,A,../data/dataiku\R-api\authinfo.md
What is the purpose of retrieving authentication information in Dataiku?,To know who is running the code,To enhance data visualization,To improve code performance,To manage database connections,A,../data/dataiku\R-api\authinfo.md
Which of the following is a valid use case for identifying the user from HTTP headers in Dataiku?,Running a machine learning model,Identifying the user in a Shiny webapp,Generating a report,Cleaning data,B,../data/dataiku\R-api\authinfo.md
Which of the following R packages is NOT part of the Dataiku R API?,dataiku,dataiku.spark,dataiku.spark2,dataiku.dss,D,../data/dataiku\R-api\index.md
What type of support is provided for SparkR and sparklyr in Dataiku?,Tier 1 support,Tier 2 support,Tier 3 support,No support,B,../data/dataiku\R-api\index.md
Which of the following is NOT a method to set up the connection with DSS using the R API?,Using the RStudio integration,Through code,Through environment variables,Through a web browser,D,../data/dataiku\R-api\outside-usage.md
What is required to connect to DSS using the R API?,The URL of DSS and a REST API key,The URL of DSS and a username,A REST API key and a password,A username and a password,A,../data/dataiku\R-api\outside-usage.md
How can you disable SSL certificate check when connecting to DSS using the R API?,Through code or environment variables,Through code or configuration file,Through environment variables or configuration file,Through RStudio integration,B,../data/dataiku\R-api\outside-usage.md
Which package provides datatable-based interaction in the DSS R API?,dataiku,dataiku.spark,dataiku.spark2,dataiku.sparklyr,A,../data/dataiku\R-api\reference-doc.md
Which package should be used for Spark interaction using the 'SparkR' native API for Spark 2.X?,dataiku,dataiku.spark,dataiku.spark2,dataiku.sparklyr,C,../data/dataiku\R-api\reference-doc.md
Where can the reference documentation for the 'dataiku.sparklyr' package be found?,https://doc.dataiku.com/dss/api/13/R/dataiku/reference,https://doc.dataiku.com/dss/api/13/R/dataiku.spark/reference,https://doc.dataiku.com/dss/api/13/R/dataiku.spark2/reference,https://doc.dataiku.com/dss/api/13/R/dataiku.sparklyr/reference,D,../data/dataiku\R-api\reference-doc.md
Which of the following features was introduced in DSS version 1.0.3?,Basic support for Avro files,Visual analytics,MongoDB support,R recipes (basic support),C,../data/dataiku\release_notes\1.0.md
What was one of the bug fixes in DSS version 1.0.2?,Fix in Hive datasets synchronization when schema contains map types,Fix layout issues in SQL datasets and HTML apps,Fix mapping of 'int' type in Pig,Fix syncing of partitioned filesystem datasets to Vertica,B,../data/dataiku\release_notes\1.0.md
Which of the following features was part of the initial official release of DSS 1.0?,"Support for HTTP, FTP and SSH remote files",Basic support for Avro files,MongoDB support,Fix layout issues in SQL datasets and HTML apps,A,../data/dataiku\release_notes\1.0.md
Which of the following new features was introduced in DSS 1.1.0 to enhance collaboration?,Support for Apache Cassandra,New projects system,New storage system for Flow states,Excel extractor improvements,B,../data/dataiku\release_notes\1.1.md
What was a major enhancement in DSS 1.1.3?,Support for Apache Cassandra,New storage system for Flow states,Custom formula editor improvements,SQL notebooks using Impala,B,../data/dataiku\release_notes\1.1.md
Which of the following bug fixes was included in DSS 1.1.4?,Fix publication to pinboard of 'web app' insights,Scrollbar in SQL notebook history fixed,Fix encoding issue with Python recipe in 'write_tuples' mode,Fix custom JDBC properties,C,../data/dataiku\release_notes\1.1.md
Which of the following is a new feature introduced in Dataiku DSS version 1.2.0?,Support for complex types in dataset schemas,Fixed installation on CentOS following IUS repository location change,Fixed various UI issues,Added Indian holidays,A,../data/dataiku\release_notes\1.2.md
What enhancement was made to the prediction/scoring/clustering recipes in Dataiku DSS version 1.2.3?,Improved performance,Added write_dataframe in dataset writers,Fixed SQL datasets when name contains some reserved keywords,Fixed scrolling issues,A,../data/dataiku\release_notes\1.2.md
Which of the following is a bug fix related to machine learning in Dataiku DSS version 1.2.3?,Fixed 'Display outliers' in Clustering results scatter plot,Fixed 'Analyse' in explore view for numerical columns containing NULL values,Fixed Fuzzy matching when used in Recipe,Fixed 'Run' button in Recipes Actions,A,../data/dataiku\release_notes\1.2.md
Which of the following new features was introduced in DSS version 1.3.0?,Support for RH 6.6 and above,New R support including R recipe and R notebook,Fixed logistic regression in multiclass mode,Fixed writing to S3 datasets when target data did not already exist,B,../data/dataiku\release_notes\1.3.md
What issue was fixed in DSS version 1.3.1 related to machine learning?,Fixed various issues with Cassandra datasets,Fixed handling of multi-dimension-partitioning in scheduler,Fixed logistic regression in multiclass mode,Fixed ‘;’ splitting in Hive recipes,C,../data/dataiku\release_notes\1.3.md
"In DSS version 1.3.0, what is recommended to do with machine learning models during migration?",Delete all old models,Retrain all machine learning models,Export models to a different format,No action is required,B,../data/dataiku\release_notes\1.3.md
Which of the following new features was introduced in DSS version 1.4.5?,Support for Hive 1.1,Official support for Oracle,Support for GeoJSON files,New reshaping processor: 'fold the keys of an array',A,../data/dataiku\release_notes\1.4.md
"In DSS version 1.4.0, which feature allows DSS to connect to a corporate LDAP directory?",Custom variables expansion,Reverse geocoding,Configuring LDAP authentication,Geo charts,C,../data/dataiku\release_notes\1.4.md
Which of the following bug fixes was included in DSS version 1.4.1?,Fixed FTP listing for WS_FTPD,Fixed compatibility with MapR 4.0,Fixed Python processors in recipes over filesystem datasets,Fixed explore in the Twitter dataset,B,../data/dataiku\release_notes\1.4.md
Which of the following new features was added to Dataiku DSS 10.0.6 for machine learning?,No-code image classification,Counterfactuals and Actionable Recourse,LightGBM,Geo-join recipe,A,../data/dataiku\release_notes\10.0.md
What is the purpose of the new 'Geo-join recipe' introduced in Dataiku DSS 10.0.0?,To create an area around a geopoint,To visually match and enrich geospatial datasets,To compute distance between geometries,To simplify a geometry,B,../data/dataiku\release_notes\10.0.md
Which of the following features is deprecated in Dataiku DSS 10.0 and will be removed in a future release?,Support for Ubuntu 16.04 LTS,Support for MapR,Support for AmazonLinux 1,Support for Hortonworks HDP 2,B,../data/dataiku\release_notes\10.0.md
Which of the following new features was introduced in DSS 11.0.0?,Visual Time Series Forecasting,Support for R 4,Python 3.11,Group K-Fold,A,../data/dataiku\release_notes\11.md
"What is the purpose of the 'Create if, then, else' processor introduced in DSS 11.0.0?",To create new columns based on conditions on the values of other columns,To perform hyperparameter search for time series models,To export models to MLflow,To manage Databricks tables in DSS,A,../data/dataiku\release_notes\11.md
Which of the following enhancements was made to Visual Machine Learning in DSS 11.4.0?,Added support for Python 3.11,Improved logs when distributed hyperparameter search failed,Added ability to rename datasets,Fixed issues with dates in Window recipe with BigQuery datasets,B,../data/dataiku\release_notes\11.md
Which of the following new features in DSS 12.0.0 allows you to add an extra layer of human control over the models’ predictions?,Universal Feature Importance,Causal Prediction,Machine Learning Overrides,Auto Feature Generation,C,../data/dataiku\release_notes\12.md
"What new feature in DSS 12.0.0 helps explain models in an agnostic, comparable way and is fully based on Shapley values?",Causal Prediction,Universal Feature Importance,Machine Learning Overrides,Auto Feature Generation,B,../data/dataiku\release_notes\12.md
Which new feature in DSS 12.0.0 is designed to optimize outcomes based on actionable variables rather than just predicting them?,Universal Feature Importance,Causal Prediction,Machine Learning Overrides,Auto Feature Generation,B,../data/dataiku\release_notes\12.md
Which new feature in DSS 13.1.0 allows for fine-tuning LLMs using your data?,Gauge Chart,Managed LLM Fine-Tuning,Enhanced Python Dataset Read API,Builtin Git Merging,B,../data/dataiku\release_notes\13.md
What is the new clustering algorithm added in DSS 13.0.0?,K-Means,DBSCAN,HDBSCAN,Agglomerative Clustering,C,../data/dataiku\release_notes\13.md
Which feature in DSS 13.0.0 allows for deploying API services to Snowpark Container Services?,Multimodal Embeddings,Databricks Serving in Unified Monitoring,Deploy Models to Snowflake Snowpark Container Services,LLM Mesh,C,../data/dataiku\release_notes\13.md
Which of the following is a new feature introduced in DSS 2.0.3?,DSS can now read and perform advanced extraction on XML files.,Flag login cookies as HTTP-only.,Fixed regression with H2O models.,Fixed layout of SQL and R recipe editors.,A,../data/dataiku\release_notes\2.0.md
What is one of the new features in the machine learning component of DSS 2.0.0?,Support for MongoDB 3.0.,Advanced cross-validation policies.,Fixed regression with H2O models.,Fixed layout of SQL and R recipe editors.,B,../data/dataiku\release_notes\2.0.md
Which of the following is a new feature introduced in DSS 2.1?,Integration with Apache Spark,Support for SQL Server,Integration with Google Analytics,Support for PostgreSQL,A,../data/dataiku\release_notes\2.1.md
What is the purpose of 'Managed Folders' in DSS 2.1?,To store structured data only,To store any kind of data and appear in Flow for dependencies computation,To manage user permissions,To create visualizations,B,../data/dataiku\release_notes\2.1.md
Which of the following is a bug fix related to machine learning in DSS 2.1?,Fixed Gradient Boosting Tree in kfold mode with multiple losses,Fixed support for Spark 1.5.1,Fixed UI glitch in lists editor,Fixed detection of Excel dates,A,../data/dataiku\release_notes\2.1.md
Which of the following is a new feature introduced in DSS 2.2.0?,Support for reading database tables containing blobs,Experimental Spark-on-S3 and EMR support,Prediction API server,Fix for vstack recipe on Oracle,C,../data/dataiku\release_notes\2.2.md
What issue was fixed in the Machine Learning section of DSS 2.2.3?,Fix handling of timezones in Impala recipes,Fix filters UI in 'explicit extract from two datasets' mode,Fix ability to use Unicode column names in Python recipes,Fix a deadlock leading to DSS freezing,B,../data/dataiku\release_notes\2.2.md
Which of the following was a fix related to machine learning in DSS 2.3.5?,Fix wrongly computed multiclass metrics,Add support for CDH 5.7 and Hive-on-Spark,Fix DateParser in multi-columns mode,Add percentage mode on pie/donut chart,A,../data/dataiku\release_notes\2.3.md
What new feature was introduced in the Spark engine for machine learning in DSS 2.3.2?,Support for probabilities in Random forest,Fix fuzzy join hanging in rare conditions,Fix support for Hive in MapR-ecosystem versions 1601 and above,Add support for CDH 5.7 and Hive-on-Spark,A,../data/dataiku\release_notes\2.3.md
Which of the following metrics can be tracked in DSS 3.0?,The size of a dataset,The number of invalid rows for a given meaning in a column,All performance metrics of a saved model,All of the above,D,../data/dataiku\release_notes\3.0.md
What new feature in DSS 3.0 replaces scheduled jobs?,Automation node,Scenarios,API node,Metrics and checks,B,../data/dataiku\release_notes\3.0.md
Which library was upgraded from version 0.16 to 0.17 in DSS 3.0?,Pandas,Scikit-learn,XGBoost,NumPy,A,../data/dataiku\release_notes\3.0.md
Which of the following is a new feature introduced in DSS 3.1.0?,Support for Scala recipes and notebooks,Integration with TensorFlow,Support for PostgreSQL,Integration with Apache Flink,A,../data/dataiku\release_notes\3.1.md
What is a notable limitation when upgrading to DSS 3.1?,Models trained with previous DSS versions will not work in DSS 3.1,DSS 3.1 does not support any SQL databases,DSS 3.1 cannot be installed on Windows OS,DSS 3.1 does not support any machine learning algorithms,A,../data/dataiku\release_notes\3.1.md
Which of the following machine learning visualizations was introduced in DSS 3.1?,ROC Curve,Confusion Matrix,Partial dependency plots for Gradient Boosting,Precision-Recall Curve,C,../data/dataiku\release_notes\3.1.md
Which of the following new features was introduced in DSS 4.0 for anomaly detection?,Isolation Forest,Random Forest,Gradient Boosting,Support Vector Machine,A,../data/dataiku\release_notes\4.0.md
What must be done to models trained with prior versions of DSS when upgrading to DSS 4.0?,They must be retrained,They must be deleted,They must be exported,They must be archived,A,../data/dataiku\release_notes\4.0.md
Which of the following libraries was upgraded in DSS 4.0?,NumPy,Pandas,scikit-learn,TensorFlow,C,../data/dataiku\release_notes\4.0.md
Which of the following new features in DSS 4.1 allows you to create an arbitrary number of code environments for Python or R?,Auto-ML features,Multiple code environments,Pivot recipe,SQL queries in the API node,B,../data/dataiku\release_notes\4.1.md
What must be done to models trained with prior versions of DSS when upgrading to 4.1?,"Nothing, they work as is",They must be retrained,They must be deleted,They must be exported,B,../data/dataiku\release_notes\4.1.md
Which of the following is a new feature in DSS 4.1 that supports creating interactive web apps using the Shiny and Bokeh libraries?,RMarkdown,Custom charts on the dashboard,Shiny and Bokeh support,Mass actions and view on the Flow,C,../data/dataiku\release_notes\4.1.md
Which of the following is a new feature added in DSS 4.2.3 for machine learning?,Support for sample weights in visual machine learning,Ability to revert the design of a prediction task to a previously trained model,Integration with collectd for system monitoring,Support for Hive views,B,../data/dataiku\release_notes\4.2.md
What should be done to machine-learning models when upgrading to DSS 4.2?,No action is required,Retrain the models,Delete the models,Export the models,B,../data/dataiku\release_notes\4.2.md
Which of the following is NOT a feature of the new 'Hive' dataset in DSS 4.2?,Support for Hive views,Support for ACID Hive tables,Better support for 'decimal' and 'date' data types,Support for sample weights,D,../data/dataiku\release_notes\4.2.md
Which of the following new features was introduced in DSS 4.3.0?,API Deployer,Support for Java 9 and Java 10,Fixed UI issue with SQL autocompletion,Fixed error when reverting changes using 'Revert this change only' mode,A,../data/dataiku\release_notes\4.3.md
What is recommended to do before starting the upgrade procedure to DSS 4.3?,Perform a full backup of your DSS data directory,Uninstall the previous version of DSS,Disable all plugins,Delete all existing datasets,A,../data/dataiku\release_notes\4.3.md
Which of the following is a limitation when upgrading to DSS 4.3?,Models trained with prior versions should be retrained,You must uninstall all previous versions of Java,You need to delete all existing projects,You must disable all API nodes,A,../data/dataiku\release_notes\4.3.md
Which of the following features is not available anymore when running Java 7 in DSS 5.0?,Reading of GeoJSON files,Reading of Shapefiles,Geographic charts,Deep learning capabilities,D,../data/dataiku\release_notes\5.0.md
What should be done to machine-learning models trained with prior versions of DSS when upgrading to 5.0?,Delete the models,Retrain the models,Export the models,No action needed,B,../data/dataiku\release_notes\5.0.md
Which of the following is a new feature introduced in DSS 5.0?,Support for Java 7,Support for XGBoost on GPU,Support for R versions below 3.4,Support for Hadoop versions below 2.5,B,../data/dataiku\release_notes\5.0.md
Which of the following Python packages was upgraded in DSS 5.1?,PANDAS (0.20 -> 0.23),NUMPY (1.13 -> 1.15),SCIKIT-LEARN (0.19 -> 0.20),ALL OF THE ABOVE,D,../data/dataiku\release_notes\5.1.md
What must be done to XGBoost models trained with prior versions of DSS when upgrading to 5.1?,NO ACTION IS REQUIRED,THEY MUST BE RETRAINED,THEY MUST BE DELETED,THEY MUST BE EXPORTED,B,../data/dataiku\release_notes\5.1.md
Which of the following is a new feature in DSS 5.1 related to machine learning?,ABILITY TO DUPLICATE A MACHINE LEARNING TASK,SUPPORT FOR CDH 6,NEW JOBS UI,EXPORTING THE FLOW AS AN IMAGE,A,../data/dataiku\release_notes\5.1.md
Which of the following features was introduced in DSS 6.0 for handling time-oriented datasets?,Managed Kubernetes clusters,Partitioned models,Time-aware cross-validation and evaluation,Global search toolbar,C,../data/dataiku\release_notes\6.0.md
What is the recommended engine to use instead of the deprecated Hadoop Mapreduce engine for the prepare recipe in DSS 6.0?,Pig,Spark,SQL,Python,B,../data/dataiku\release_notes\6.0.md
Which new feature in DSS 6.0 allows for the automatic management of Spark jobs on Kubernetes?,Managed Kubernetes clusters,Managed Spark on Kubernetes,Partitioned models,SQL pipelines,B,../data/dataiku\release_notes\6.0.md
Which of the following methods for row-level interpretability is NOT included in Dataiku DSS?,ICE (Individual Conditional Explanations),Shapley Values,LIME (Local Interpretable Model-agnostic Explanations),Partial Dependence Plots,C,../data/dataiku\release_notes\7.0.md
Which of the following is a new feature introduced in DSS 7.0 for exploratory data analysis?,Univariate analysis,Bivariate analysis,Statistical tests,All of the above,D,../data/dataiku\release_notes\7.0.md
Which of the following is NOT a deprecated feature in DSS 7.0?,Support for Hive CLI execution modes,Support for Microsoft HDInsight,Support for Machine Learning through Vertica Advanced Analytics,Support for Spark 2,D,../data/dataiku\release_notes\7.0.md
Which of the following features was introduced in DSS 8.0 to help organize complex flows into more manageable sub-parts?,Flow Zones,Flow Nodes,Flow Sections,Flow Parts,A,../data/dataiku\release_notes\8.0.md
What new feature in DSS 8.0 allows Dataiku designers to make their projects reusable and consumable by business users?,Dataiku Applications,Dataiku Projects,Dataiku Templates,Dataiku Modules,A,../data/dataiku\release_notes\8.0.md
Which deprecated feature in DSS 8.0 is recommended to be replaced by HiveServer2?,Hive CLI execution modes,Pig,Vertica Advanced Analytics,Hive SequenceFile formats,A,../data/dataiku\release_notes\8.0.md
Which of the following features was introduced in DSS 9.0 for machine learning?,Model assertions,Interactive scoring and What-if,Smart Pattern Builder,All of the above,D,../data/dataiku\release_notes\9.0.md
What is the purpose of the 'Unified Deployer' introduced in DSS 9.0?,To provide a unified environment for fully-managed production deployments of both projects and API services,To enhance the formula editor with better code completion,To support Spark 3,To add support for Azure Synapse,A,../data/dataiku\release_notes\9.0.md
Which of the following machine learning features was fixed in DSS 9.0.6?,Creation of cluster recipes on foreign datasets,Support for Cloudera Data Platform CDP Private Cloud Base,Support for static private IP for Fleet Manager,Support for PostGIS types,A,../data/dataiku\release_notes\9.0.md
Which of the following is a new feature introduced in DSS 13.1.0?,Managed LLM fine-tuning,Python 2.7 builtin env removal,Unified Monitoring,Interactive hierarchical clustering,A,../data/dataiku\release_notes\index.md
What is a major change in the handling of SQL datasets in DSS 13.1.0?,Support for multiple Hadoop clusters,Handling of schema mismatch,Enhanced user management capabilities,New editable dataset UI,B,../data/dataiku\release_notes\index.md
Which feature was removed in DSS 13?,Python 2.7 builtin env,Support for R 4,Support for CDH 6,Support for HANA Calculation views,A,../data/dataiku\release_notes\index.md
Which of the following features was introduced in Dataiku version 0.8.0?,UserVoice integration,Live validation of Pig and Hive recipes,Fix for single-file HDFS datasets,Basic support for Hadoop Sequence File,B,../data/dataiku\release_notes\pre-1.0.md
What issue was fixed in Dataiku version 0.6.13?,Fix Null Pointer Exception when running a Shaker recipe on an Apache log file,Fix recipes with multiple outputs that could generate overly long job ids,Fix ElasticSearch mirroring of non-partitioned datasets,Fix partitioning for patterns like /%Y%M%D/.*,B,../data/dataiku\release_notes\pre-1.0.md
Which of the following sampling methods in DSS ensures that the distribution of values in a column is respected in the sampling?,First records,Random sampling (fixed number of records),Stratified (fixed number of records),Class rebalancing (approximate number of records),C,../data/dataiku\sampling\index.md
In which of the following locations in DSS is the 'No sampling' method available?,Exploration,Visual data preparation,Charts,Machine learning,D,../data/dataiku\sampling\index.md
Which sampling method in DSS is useful for selecting a subset of customers by choosing all rows with specific values in a column?,First records,Column values subset,Stratified (approximate ratio),Last records,B,../data/dataiku\sampling\index.md
Which of the following actions can a custom scenario perform in Dataiku?,Execute steps defined in a step-based scenario,Read metadata about executed steps,Activate new versions of trained models,All of the above,D,../data/dataiku\scenarios\custom_scenarios.md
Which of the following is NOT a capability of the scenario API in Dataiku?,Send custom messages through reporters,Read detailed parameters of the trigger that initiated the scenario,Activate new versions of trained models,Be used in a Python recipe/Jupyter notebook,D,../data/dataiku\scenarios\custom_scenarios.md
"In the context of sending custom reports in Dataiku, which of the following is a valid messaging channel?",SMTP mail,Twilio SMS,Microsoft Teams,All of the above,D,../data/dataiku\scenarios\custom_scenarios.md
Which of the following best describes a 'scenario' in DSS?,"A set of actions to do, with condition(s) to run it.",A fixed list of steps that are always run in the same order.,A python script that runs in an environment allowing it to launch scenario steps.,A condition attached to a scenario that is evaluated periodically.,A,../data/dataiku\scenarios\definitions.md
What is the main difference between step-based scenarios and custom python scenarios in DSS?,"Step-based scenarios are made of a fixed list of steps, while custom python scenarios consist of a python script.",Step-based scenarios offer more flexibility in deciding whether a step needs to be run.,Custom python scenarios always run steps in the same order.,Step-based scenarios are evaluated periodically.,A,../data/dataiku\scenarios\definitions.md
What is the role of 'reporters' in DSS scenarios?,To evaluate conditions periodically and launch the scenario.,To prepare messages and send them on messaging channels.,To define the sequence of actions in a scenario.,To run a python script in an environment allowing it to launch scenario steps.,B,../data/dataiku\scenarios\definitions.md
Which of the following is NOT a type of trigger for launching a scenario?,Time-based triggers,Dataset modification triggers,SQL triggers,API triggers,D,../data/dataiku\scenarios\index.md
Which of the following steps is used to execute custom Python code in a scenario?,Execute SQL,Execute Python code,Run another scenario,Send message,B,../data/dataiku\scenarios\index.md
Which of the following is a reporter type that can be used to send scenario run notifications?,Mail reporter,SMS reporter,FTP reporter,HTTP reporter,A,../data/dataiku\scenarios\index.md
Which of the following is NOT a method to send a message during a scenario run in Dataiku DSS?,Add a reporter to the scenario in the Settings pane and set 'Send on scenario' to Start.,Add a 'Send message' step in a step-based scenario.,Use 'get_message_sender()' on the 'Scenario' object in a script scenario.,Use 'send_message_now()' function in the scenario settings.,D,../data/dataiku\scenarios\reporters.md
Which of the following steps in Dataiku can be used to delete the contents of datasets or managed folders?,Build / Train,Clear,Verify rules or run checks,Compute metrics,B,../data/dataiku\scenarios\steps.md
What is the purpose of the 'Synchronize Hive table' step in Dataiku?,To execute SQL statements on a DSS connection,To re-publish a Python notebook to an insight,To regenerate the schema of the Hive table to match the dataset,To send a message during a scenario run,C,../data/dataiku\scenarios\steps.md
Which step in Dataiku scenarios allows you to run a chunk of Python code in the context of the scenario?,Execute SQL,Execute Python code,Define variables,Run global variables update,B,../data/dataiku\scenarios\steps.md
Which of the following options allows a step to run regardless of whether previous steps failed or not?,Never,Always,If no prior step failed,If some prior step failed,B,../data/dataiku\scenarios\step_flow_control.md
What is the default behavior of steps in a step-based scenario?,Always run,Run only if no prior step failed,Run only if some prior step failed,Run based on a formula,B,../data/dataiku\scenarios\step_flow_control.md
Which mode allows a step to run only if a failure occurred in the preceding steps?,Never,Always,If no prior step failed,If some prior step failed,D,../data/dataiku\scenarios\step_flow_control.md
Which type of trigger would you use to launch a scenario at regular intervals?,Dataset modification trigger,SQL trigger,Python trigger,Time-based trigger,D,../data/dataiku\scenarios\triggers.md
What is the purpose of a 'marker' file in a dataset modification trigger?,To specify a file whose change indicates that the data has changed,To prevent the trigger from activating while the dataset files are being modified,To protect against situations where refreshing of a dataset can hang,All of the above,D,../data/dataiku\scenarios\triggers.md
What happens if multiple active triggers are present in a scenario?,The scenario will be triggered only when all trigger conditions are true,The scenario will be triggered when any trigger condition is true,The scenario will not be triggered,The scenario will be triggered based on the first trigger condition that becomes true,B,../data/dataiku\scenarios\triggers.md
Which of the following keywords can be used to specify the current date when setting a partition identifier in a step-based scenario?,CURRENT_DAY,CURRENT_MONTH,CURRENT_YEAR,ALL OF THE ABOVE,D,../data/dataiku\scenarios\variables.md
"In a step-based scenario, which step is used to add new variables by evaluating a DSS formula?",Define variables,Compute metrics,Run checks,Execute SQL,A,../data/dataiku\scenarios\variables.md
What is the purpose of the 'Scenario' object in a Python script scenario?,To define new variables,To get and set variables,To execute SQL queries,To compute metrics,B,../data/dataiku\scenarios\variables.md
What does the 'data quality bar' in the 'Explore' view show by default?,The storage type of each column.,The inferred meaning of each column.,Which rows are valid for their meaning.,The number of empty rows in the dataset.,C,../data/dataiku\schemas\basic.md
"In the 'Explore' view, what is displayed for each column?",Only the storage type.,Only the inferred meaning.,Both the storage type and the inferred meaning.,Neither the storage type nor the inferred meaning.,C,../data/dataiku\schemas\basic.md
"In Dataiku DSS, what is the default behavior for schema inference from DSS 12 onwards?",Always infer data types for all input dataset formats.,Lock strongly typed inputs and infer for loosely-typed ones.,Lock all inputs preemptively.,Infer data types only for new columns.,B,../data/dataiku\schemas\data-preparation.md
Which of the following is true about visual analysis in Dataiku DSS?,It has persistence of output data.,Columns in analysis have a notion of storage types.,Only meanings are shown in analysis.,It is used to create a new dataset.,C,../data/dataiku\schemas\data-preparation.md
What is the purpose of type inference in a prepare recipe in Dataiku DSS?,To ensure numeric columns from a CSV are treated as strings.,To lock the data types of the input columns preemptively.,To compute the 'best' storage type matching the data in the sample.,To remove invalid values from the dataset.,C,../data/dataiku\schemas\data-preparation.md
Which of the following types of datasets does Data Science Studio (DSS) automatically detect column names and types for?,SQL and Cassandra datasets,Text-based files datasets,Typed files datasets,All of the above,D,../data/dataiku\schemas\datasets.md
What happens if the schema of the underlying table changes for a SQL or Cassandra dataset in DSS?,DSS will automatically update the schema without any user action.,DSS will update the schema only when you go to the edition page for the dataset.,The schema will remain unchanged until the user manually updates it.,DSS will send a notification to the user to update the schema.,B,../data/dataiku\schemas\datasets.md
"In DSS, what is the recommended approach if you need to clean, enrich, or preprocess your data in a text-based files dataset?",Directly set the storage type in the dataset settings screen.,Leave all storage types to 'string' and use a Data Preparation recipe.,Manually edit the schema to match the expected data types.,Use the 'Redetect' button to force DSS to redetect format and schema.,B,../data/dataiku\schemas\datasets.md
"In Dataiku DSS, how are dates displayed in charts?",In the local time zone of the server,In the time zone specified by the user,In UTC,In the time zone of the data source,C,../data/dataiku\schemas\dates.md
What happens if you format a date as '16:00+0200' and select the output to be a string in Dataiku DSS?,The string value will be preserved but it’s not a date anymore,The date will be converted to UTC,The date will be displayed in the local time zone,The date will be displayed in the original time zone,A,../data/dataiku\schemas\dates.md
"When reading a date column from an SQL table, what does DSS assume if the 'assumed time zone' setting is set to 'Local'?",The date is read as UTC,The date is read as the local time zone of the server,The date is read as the time zone of the data source,The date is read as the time zone specified by the user,B,../data/dataiku\schemas\dates.md
Which of the following is NOT a storage type in Data Science Studio (DSS)?,STRING,BOOLEAN,URL,GEOMETRY,C,../data/dataiku\schemas\definitions.md
What is the purpose of using precise storage types in DSS?,To enhance the visual representation of data.,"To ensure data is stored in a format that is compatible with SQL, Hadoop, and Spark.",To automatically detect the semantic meaning of data.,To allow for the storage of invalid data.,B,../data/dataiku\schemas\definitions.md
Which of the following is a geospatial meaning recognized by Dataiku?,TEXT,LATITUDE / LONGITUDE,BOOLEAN,NATURAL LANGUAGE,B,../data/dataiku\schemas\index.md
"In Dataiku, which recipe is used for scoring in machine learning?",PREPARE,JOIN,SPLIT,MACHINE LEARNING (SCORING),D,../data/dataiku\schemas\index.md
Which of the following is a type of user-defined meaning in Dataiku?,BOOLEAN,GEOMETRY,DECLARATIVE,DATE,C,../data/dataiku\schemas\index.md
Which of the following meanings recognized by DSS is used for expressing a point in geometric coordinates?,Latitude / Longitude,Geopoint,Geometry,Country,B,../data/dataiku\schemas\meanings-list.md
What format must dates be in to be recognized by the 'Date' meaning in DSS?,MM/DD/YYYY,DD-MM-YYYY,ISO-8601,YYYY/MM/DD,C,../data/dataiku\schemas\meanings-list.md
Which of the following recipes in DSS automatically computes the output schema each time you Save the recipe?,Sync,Prepare,"Sample, Filter, Group, Window, Join, Split, Stack",Shell,C,../data/dataiku\schemas\recipes.md
What is the default mode of operation for the Sync recipe when it comes to schema handling?,Free output schema (name-based matching),Strict schema equality,Schema inferred at runtime,Schema not updated,B,../data/dataiku\schemas\recipes.md
Which recipe unconditionally updates the schema at runtime?,"Python, R, PySpark, SparkR","Hive, Impala, SQL",SparkSQL,Shell,C,../data/dataiku\schemas\recipes.md
Which of the following is NOT a kind of user-defined meaning in DSS?,Declarative,Values list,Pattern,Descriptive,D,../data/dataiku\schemas\user-defined-meanings.md
What is the main purpose of the 'Values mapping' kind of user-defined meaning?,To specify a list of possible values for validation,To map internal values to human-readable ones,To define a pattern that values must match,To document the meaning of a column,B,../data/dataiku\schemas\user-defined-meanings.md
Which of the following keys should be added to the 'security' section in the general-settings.json file to hide error stacks from logged-in users?,"""hideErrorStacks"" : true","""hideVersionStringsWhenNotLogged"" : true","""secureCookies"" : true","""forceSingleSessionPerUser"" : true",A,../data/dataiku\security\advanced-options.md
What is the effect of setting 'dku.exports.disableAllExports=true' in the dip.properties file?,Disables exports of datasets globally on the instance.,Disables ability to attach datasets to emails in scenarios.,Disables ability to download files of zips from managed folders.,All of the above.,D,../data/dataiku\security\advanced-options.md
Which of the following HTTP headers can be set in the install.ini file for security purposes?,X-Frame-Options,Content-Security-Policy,Strict-Transport-Security,All of the above,D,../data/dataiku\security\advanced-options.md
What does the READ authorization mode allow for a dataset in Dataiku?,View and create insights on the dataset,Modify the dataset content,Delete the dataset,Share the dataset with external users,A,../data/dataiku\security\authorized-objects.md
Which authorization mode allows a user to see a dataset and a limited amount of metadata without accessing its content?,READ,WRITE,DISCOVER,RUN,C,../data/dataiku\security\authorized-objects.md
What is required to manage the 'Authorized objects' list in a Dataiku project?,Read project content permission,Manage authorized objects permission,Publish on workspaces permission,Run scenarios permission,B,../data/dataiku\security\authorized-objects.md
What does 'freely using' a DSS connection NOT include?,Creating new datasets on the connection,Modifying the settings of a dataset using the connection,Browsing in any way the connection,Reading datasets which have already been defined on a connection,D,../data/dataiku\security\connections.md
Which of the following is required to access connections with an API key?,A non-personal API key,An admin API key or a personal API key,A group API key,A global service credential,B,../data/dataiku\security\connections.md
What is the recommended action to ensure good performance in Spark when using HDFS and S3 connections?,Granting 'freely use' permission,Granting 'Details readable by' permission,Using a non-personal API key,Disabling per-user credentials,B,../data/dataiku\security\connections.md
"In Dataiku Govern, what is a Blueprint?",A specific template for Govern Projects with field definitions and workflows.,A category of objects such as a Govern Project.,The actual object containing fields defined in the Blueprint Version.,A set of rules defining what actions are allowed for which roles.,B,../data/dataiku\security\govern-permissions.md
Which of the following is NOT a criterion that can be used in Roles Assignment Rules in Dataiku Govern?,Blueprint Version criterion,Field value criterion,Workflow criterion,User action criterion,D,../data/dataiku\security\govern-permissions.md
What permission is automatically granted if 'Artifact Write' permission is granted in Dataiku Govern?,Artifact Read,Artifact Create,Artifact Delete,Field Write,A,../data/dataiku\security\govern-permissions.md
Which of the following is NOT a supported LDAP directory configuration template in Dataiku?,Standard LDAP directory using RFC2307 schema,Microsoft Active Directory,Google Directory,RedHat Identity Management and FreeIPA servers,C,../data/dataiku\security\index.md
Which of the following is a feature supported by OpenID Connect (OIDC) in Dataiku?,Using secure LDAP connections,Configuring DSS for OIDC authentication,Managing project access requests,Setting security-related HTTP headers,B,../data/dataiku\security\index.md
"In Dataiku, which of the following is a possible reason for a SAML SSO login failure?",Assertion audience does not include issuer,Using local users,Configuring DSS for OIDC authentication,Using secure cookies,A,../data/dataiku\security\index.md
Which of the following statements about DSS password management is true?,DSS mandates strict password security rules for the local passwords database.,Passwords in the local passwords database are encrypted using a reversible hash function.,"In most enterprise deployments, users come from an LDAP directory instead of the local passwords database.",DSS does not support integration with 3rd party systems.,C,../data/dataiku\security\passwords-security.md
What encryption algorithm is used for encrypting passwords in the local passwords database?,AES-128,PBKDF2,HMAC-SHA256,RSA,B,../data/dataiku\security\passwords-security.md
Which encryption algorithm is used for encrypting 3rd party system credentials in DSS?,RSA,PBKDF2,AES in CTR mode,Blowfish,C,../data/dataiku\security\passwords-security.md
Which of the following permissions allows a group to modify which objects of the project are usable by dashboard-only users?,Manage authorized objects,Write dashboards,Read dashboards,Publish to Data Collections,A,../data/dataiku\security\permissions.md
What permission is implied by the 'Write project content' permission?,Read project content,Read dashboards,Run scenarios,All of the above,D,../data/dataiku\security\permissions.md
Which permission allows a group to create and manage clusters?,Manage all/own clusters,Create personal connections,Develop plugins,Create published API services,A,../data/dataiku\security\permissions.md
Which of the following is NOT a characteristic of discoverable projects?,Visible to all users through Home > Projects page or the global search.,Displayed on the user’s homepage even without permissions.,Only a subset of project information is displayed to users without access.,"Includes details like project key, name, owner, status, and tags.",B,../data/dataiku\security\project-access.md
What happens when a project access request is automatically updated to 'approved'?,The requester is deleted.,The project is deleted.,The project administrator manually grants the requester any 'read' permissions.,The requester is granted 'write' permissions.,C,../data/dataiku\security\project-access.md
Which of the following permissions do analysts of project B NOT have on a shared dataset from project A?,View the dataset’s data with arbitrary sampling settings,Build machine learning models on it,View or change the settings of the dataset,Use it in recipes,C,../data/dataiku\security\shared-objects.md
What is required to share an object from project A to project B using managed sharing?,Read project content permission on project A,Manage shared objects permission on project A,Write project content permission on project B,Export datasets data permission on project B,B,../data/dataiku\security\shared-objects.md
Which of the following objects can be shared between projects in DSS?,Wiki articles,Visual analyses,SQL notebooks,Saved models,D,../data/dataiku\security\shared-objects.md
Which of the following SSO protocols is NOT supported by DSS?,OpenID Connect (OIDC),SAML v2,SPNEGO / Kerberos,OAuth 2.0,D,../data/dataiku\security\sso.md
What is the recommended SSO protocol to use with DSS for easier setup and better compatibility?,OpenID Connect (OIDC),SAML v2,SPNEGO / Kerberos,OAuth 2.0,A,../data/dataiku\security\sso.md
Which of the following is NOT a feature supported by DSS's OIDC integration?,Authorization code grant flow,ID token signed with RSA or EC,Public OIDC client support,DSS behind a proxy,C,../data/dataiku\security\sso.md
Which of the following user profiles in Dataiku DSS has full access to all Dataiku features?,Designer,Reader,Explorer,Administrator,A,../data/dataiku\security\user-profiles.md
What can a Reader profile in Dataiku DSS do?,Read the content of projects and perform modifications,Read the content of projects and access dashboards and webapps,Create their own dashboards and insights,Use Dataiku Applications,B,../data/dataiku\security\user-profiles.md
Which profile in Dataiku DSS can create their own dashboards and insights?,Designer,Reader,Explorer,Administrator,C,../data/dataiku\security\user-profiles.md
Which of the following is NOT a way to use user secrets in DSS?,A recipe,A notebook,A plugin recipe,A shared global secret,D,../data/dataiku\security\user-secrets.md
What must be included in the code to retrieve a user secret in Python?,The secret's name,The user's profile ID,The user's password,The API endpoint,A,../data/dataiku\security\user-secrets.md
Which of the following is NOT a function of the DSS Azure AD User Supplier?,Provisioning Azure AD users in DSS,Synchronizing Azure AD users in DSS,Authenticating users using Azure AD API,Combining with another authentication method for user authentication,C,../data/dataiku\security\authentication\azure-ad.md
What type of credentials can DSS use to connect to Azure AD?,Username and Password,Secret or Certificate,API Key,SSH Key,B,../data/dataiku\security\authentication\azure-ad.md
Which Azure AD permissions are required for DSS to connect to Azure AD?,Group.Read.All and User.Read.All,User.Read and Group.Read,Admin.Read and User.Read,Group.Write.All and User.Write.All,A,../data/dataiku\security\authentication\azure-ad.md
Which of the following is NOT a type of component you can create for a custom authenticator and/or user supplier in DSS?,Authenticator,User supplier,Authenticator and User supplier,Multiple custom user suppliers,D,../data/dataiku\security\authentication\custom.md
What tool is required to compile the source code of a custom authenticator and/or user supplier into a .jar library?,Maven,Gradle,Apache Ant,Jenkins,C,../data/dataiku\security\authentication\custom.md
Which step is necessary after packaging a custom authenticator and/or user supplier plugin to ensure it functions correctly in DSS?,Reinstall DSS,Restart DSS,Recompile the plugin,Update the DSS license,B,../data/dataiku\security\authentication\custom.md
Which of the following is NOT an authenticator supported by Dataiku DSS?,Local,SSO,OAuth,LDAP,C,../data/dataiku\security\authentication\index.md
"In Dataiku DSS, what is the process of retrieving a user from an external source and creating or synchronizing the corresponding user in DSS called?",Authentication,User provisioning,User synchronization,User mapping,B,../data/dataiku\security\authentication\index.md
Which of the following user suppliers can only provision/synchronize a user with the username information and only for users authenticated using it?,SSO,LDAP,Azure AD,PAM,D,../data/dataiku\security\authentication\index.md
Which of the following is NOT a mandatory parameter when configuring the connection to an LDAP directory in Data Science Studio?,LDAP server URL,Bind DN,User filter by username,Group name attribute,B,../data/dataiku\security\authentication\ldap.md
What is the default port for LDAP over SSL (LDAPS) connections?,389,636,443,8080,B,../data/dataiku\security\authentication\ldap.md
"Where can an administrator add, remove, or edit named 'template' configurations for Spark?",In the Spark section of the general settings from the Administration menu.,In the Spark section of the user settings from the User menu.,In the Spark section of the project settings from the Project menu.,In the Spark section of the job settings from the Job menu.,A,../data/dataiku\spark\configuration.md
What must be done to non-HDFS datasets or sampled HDFS datasets in most recipes to ensure they fit in a Spark worker's RAM?,They must be repartitioned.,They must be compressed.,They must be encrypted.,They must be duplicated.,A,../data/dataiku\spark\configuration.md
Which of the following datasets fully benefit from the Spark distributed nature out of the box?,Local datasets,HDFS datasets,S3 datasets,Both B and C,D,../data/dataiku\spark\datasets.md
What permission must be granted to the user running the Spark job to read HDFS or S3 datasets directly?,Read-only permission,Write permission,Details readable by permission,Execute permission,C,../data/dataiku\spark\datasets.md
What is a common setup to provide both isolation and ability to read datasets with good performance?,Using a single connection for all users,"Using multiple connections, each with its own credential and accessible to only one group of users",Using no credentials at all,Using a public connection for all datasets,B,../data/dataiku\spark\datasets.md
Which of the following is NOT a way to use Spark in DSS?,SparkSQL recipes,Visual recipes,Java code,Python code,C,../data/dataiku\spark\index.md
What is Spark primarily used for in DSS?,Distributed computation,Data visualization,Database management,Web development,A,../data/dataiku\spark\index.md
Which of the following is NOT a way to set up Spark in Dataiku?,Using Dataiku Cloud installation,Using Dataiku Cloud Stacks installation,Using a custom installation with Elastic AI,Using a custom installation with Apache Flink,D,../data/dataiku\spark\installation.md
What is recommended instead of using Unmanaged Spark on Kubernetes?,Using Dataiku Cloud,Using Dataiku Cloud Stacks,Using Elastic AI capabilities,Using Hadoop,C,../data/dataiku\spark\installation.md
Which of the following is NOT a recommended practice when using Spark in DSS?,Using Spark for data that fits in the memory of a single machine.,Using HDFS datasets for better performance on Spark.,Avoiding sampling with filter for input datasets.,Using a filtering recipe instead of sampling with filter.,A,../data/dataiku\spark\limitations.md
Which of the following is NOT supported in Spark pipelines?,PySpark code recipes,Spark Scala code recipes in 'Free-form' code mode,SparkSQL query recipes with multiple statements,Grouping visual recipes,D,../data/dataiku\spark\pipelines.md
What must be done to enable Spark pipelines in DSS?,"Go to the project Settings, then Pipelines, select Enable Spark pipelines, and Save settings","Go to the project Settings, then Pipelines, select Enable Spark pipelines, and Restart DSS","Go to the project Settings, then Pipelines, select Enable Spark pipelines, and Rebuild all datasets","Go to the project Settings, then Pipelines, select Enable Spark pipelines, and Clear cache",A,../data/dataiku\spark\pipelines.md
What is one of the main benefits of using Spark pipelines in DSS?,"It allows for the execution of multiple recipes as a single Spark job, boosting performance",It enables the use of PySpark and SparkR code recipes,It automatically writes all intermediate datasets to storage,It eliminates the need for any dataset configuration,A,../data/dataiku\spark\pipelines.md
Which of the following visual data-transformation recipes does NOT support running on Spark?,Prepare,Sync,Join,Normalize,D,../data/dataiku\spark\usage.md
What must you do after changing the named Spark configuration used by Python or R notebooks?,Restart DSS,Clear the cache,Reinstall Spark,Update the configuration file,A,../data/dataiku\spark\usage.md
Which of the following APIs is NOT supported for writing Spark code in R?,SparkR,sparklyr,PySpark,None of the above,C,../data/dataiku\spark\usage.md
Which of the following statements is true about external SQL table datasets in Dataiku?,You can edit the schema of an external SQL table dataset.,DSS automatically tests SQL queries for external SQL table datasets.,You need to upcast the types of columns with unsigned integer types in MySQL datasets.,External SQL table datasets allow you to write data back to the SQL table.,C,../data/dataiku\sql\datasets.md
What is a key characteristic of managed SQL datasets in Dataiku?,They can be created from SQL query datasets.,They can only target non-existing tables.,They allow you to override the dataset schema with the current schema of the table.,They do not allow you to test if the table exists in the database.,C,../data/dataiku\sql\datasets.md
Which of the following is NOT a capability of DSS when working with SQL databases?,Create datasets representing SQL tables,Execute Visual recipes directly in-database,Create datasets representing the results of a SQL query,Generate machine learning models automatically,D,../data/dataiku\sql\index.md
What feature allows DSS to perform interactive querying on SQL databases?,SQL Notebook,SQL Pipelines,SQL Recipes,SQL Charts,A,../data/dataiku\sql\index.md
Which of the following statements is true about partitioning in Dataiku DSS?,Only SQL databases with native partitioning support can be partitioned in DSS.,All SQL datasets can be partitioned in DSS.,Partitioning is not supported on SQL databases in DSS.,Partitioning is only supported on non-SQL datasets in DSS.,B,../data/dataiku\sql\partition.md
Which type of SQL datasets can you write to?,External query datasets,External table datasets,Managed SQL datasets,Both B and C,D,../data/dataiku\sql\write_and_execute.md
What can you use for interactive querying in Dataiku?,SQL recipes,SQL Notebook,Code recipes,External query datasets,B,../data/dataiku\sql\write_and_execute.md
What is a SQL pipeline in DSS?,A process that combines several consecutive recipes using different SQL engines.,A process that combines several consecutive recipes using the same SQL engine.,A process that combines several consecutive recipes using Python scripts.,A process that combines several consecutive recipes using R scripts.,B,../data/dataiku\sql\pipelines\index.md
What is one of the main benefits of using SQL pipelines in DSS?,It allows for the use of multiple SQL engines in a single job activity.,It boosts performance by avoiding unnecessary writes and reads of intermediate datasets.,It enables the use of Python scripts within SQL recipes.,It allows for the integration of machine learning models directly into SQL queries.,B,../data/dataiku\sql\pipelines\index.md
What is the current status of the SQL pipeline feature in DSS?,Fully supported and stable.,Deprecated and no longer available.,Experimental and not officially supported.,Only available for enterprise users.,C,../data/dataiku\sql\pipelines\index.md
What is partitioning in the context of Dataiku DSS?,The process of splitting a dataset along meaningful dimensions.,The process of merging multiple datasets into one.,The process of deleting unnecessary data from a dataset.,The process of encrypting a dataset for security purposes.,A,../data/dataiku\sql\pipelines\partitions.md
Which of the following statements is true about partitioning support in Dataiku DSS?,Dataiku DSS only supports partitioning on SQL databases with native partitioning support.,Dataiku DSS can provide partitioning support on SQL databases with or without native partitioning support.,Dataiku DSS does not support partitioning on SQL databases.,Dataiku DSS only supports partitioning on non-SQL databases.,B,../data/dataiku\sql\pipelines\partitions.md
"In a SQL pipeline, is it possible to go from a partitioned dataset to a non-partitioned dataset?","Yes, it is possible.","No, it is not possible.",Only if the datasets have even partition dependencies.,Only if the datasets have the same number of partitions.,A,../data/dataiku\sql\pipelines\partitions.md
Which of the following is NOT a supported database for SQL pipelines in Dataiku DSS?,Snowflake,MongoDB,Redshift,BigQuery,B,../data/dataiku\sql\pipelines\sql_pipelines.md
What must be true for SQL datasets to be part of the same SQL pipeline?,They must be part of the same database connection.,They must be virtualizable.,They must be used in visual recipes only.,They must be part of different database connections.,A,../data/dataiku\sql\pipelines\sql_pipelines.md
What is the prefix used in the naming convention for SQL views in DSS?,VIEW_,SQLVIEW_,DSSVIEW_,PIPEVIEW_,C,../data/dataiku\sql\pipelines\views.md
Which version of the Simba BigQuery JDBC driver should you upgrade to in order to avoid the view cleanup loop error?,1.1.5.1005,1.2.0.1000,1.3.0.1000,1.2.5.1005,B,../data/dataiku\sql\pipelines\views.md
What is the purpose of the 'Automated Selection' feature in Dataiku's assistant for dataset exploration?,To manually select variables for analysis.,To help discover patterns by suggesting analyses on variables of interest.,To generate a final report automatically.,To clean and preprocess the dataset.,B,../data/dataiku\statistics\assisted-data-exploration.md
How many points are used by Dataiku's assistant to perform calculations for card suggestions?,500,1000,1500,2000,B,../data/dataiku\statistics\assisted-data-exploration.md
Which of the following plots in Dataiku DSS is used to display the values of two numerical variables using Cartesian coordinates?,Histogram,Box Plot,Mosaic Plot,Scatter Plot,D,../data/dataiku\statistics\bivariate.md
"In a bivariate analysis card in Dataiku DSS, which plot summarizes the distribution of data by showing quartiles?",Histogram,Box Plot,Mosaic Plot,Scatter Plot,B,../data/dataiku\statistics\bivariate.md
Which statistical option in a bivariate analysis card computes the correlation between a pair of variables using correlation coefficients?,Histogram,Box Plot,Summary Stats,Frequency Table,C,../data/dataiku\statistics\bivariate.md
Which of the following distributions is NOT supported by the Fit Distribution card in Dataiku DSS?,Beta,Exponential,Gamma,Weibull,C,../data/dataiku\statistics\fit.md
What additional parameters does the 2D KDE plot provide in the 2D Fit Distribution card?,X relative bandwidth and Y relative bandwidth,X axis range and Y axis range,X axis label and Y axis label,X axis scale and Y axis scale,A,../data/dataiku\statistics\fit.md
"When creating a Fit Curve card, which curve types can you specify?",Linear and Exponential,Polynomial and Isotonic,Logarithmic and Exponential,Quadratic and Cubic,B,../data/dataiku\statistics\fit.md
Which of the following analyses in Dataiku DSS falls under the area of dimensionality reduction?,Univariate Analysis,Bivariate Analysis,Principal Component Analysis,Hypothesis Testing,C,../data/dataiku\statistics\index.md
Which of the following is NOT a type of card option available for univariate analysis in Dataiku DSS?,Histogram,Box Plot,Mosaic Plot,Summary Stats,C,../data/dataiku\statistics\index.md
Which statistical test in Dataiku DSS is used to check for stationarity in time series data?,Student t-test,Chi-square independence test,Augmented Dickey-Fuller (ADF) test,Kolmogrov-Smirnov test,C,../data/dataiku\statistics\index.md
"Which menu in the worksheet header allows you to create a new worksheet or rename, duplicate, delete, and switch between worksheets?",New Card,Sampling & Filtering,Worksheet,Confidence Level,C,../data/dataiku\statistics\interface.md
What does the 'Split by' menu in a card allow you to do?,Edit the settings of a card,Delete a card,Select a variable to split the data into subsets,Publish a card,C,../data/dataiku\statistics\interface.md
Which of the following is NOT a type of card available in a worksheet?,Univariate analysis,Bivariate analysis,Fit curves and distributions,Predictive modeling,D,../data/dataiku\statistics\interface.md
Which of the following tools is NOT provided by the Multivariate Analysis cards in Dataiku?,Principal Component Analysis (PCA),Correlation matrix,Scatter plot 3D,Histogram,D,../data/dataiku\statistics\multivariate.md
What does the PCA card in Dataiku display to represent the dataset in a reduced dimension?,A scree plot of eigenvalues and cumulative explained variance,A bar chart of variable frequencies,A pie chart of categorical data,A line graph of time series data,A,../data/dataiku\statistics\multivariate.md
Which correlation coefficient is computed by default in the Correlation matrix card in Dataiku?,Pearson correlation coefficient,Spearman’s rank correlation coefficient,Kendall’s tau coefficient,Point-biserial correlation coefficient,B,../data/dataiku\statistics\multivariate.md
Which of the following tests is used to determine if the mean of a population is a specific value?,Student t-test (one-sample),Sign test (one-sample),Shapiro-Wilk test,Chi-square independence test,A,../data/dataiku\statistics\tests.md
What does the Shapiro-Wilk test determine?,If the mean of a population is a specific value,If the median of a population is a specified value,If a population is normally distributed,If two categorical variables are independent,C,../data/dataiku\statistics\tests.md
Which test is used to determine if the medians of two populations are equal?,Student t-test (two-sample),Median mood test (two-sample),Kolmogrov-Smirnov test (two-sample),One-way ANOVA,B,../data/dataiku\statistics\tests.md
Which of the following tests is used to analyze series data for consistently increasing or decreasing trends?,Augmented Dickey-Fuller (ADF) test,Zivot-Andrews test,Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test,Mann-Kendall trend test,D,../data/dataiku\statistics\time-series.md
What does the Durbin-Watson statistic measure in a time series?,The presence of a unit root,The degree of correlation with lagged versions of itself,The first order auto-correlation,The trend and seasonality,C,../data/dataiku\statistics\time-series.md
Which test would you use to check if a time series is stationary?,Augmented Dickey-Fuller (ADF) test,Zivot-Andrews test,Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test,Mann-Kendall trend test,C,../data/dataiku\statistics\time-series.md
Which of the following options is NOT available in the general menu (⋮) of a univariate analysis card section?,Treat the variable as categorical or continuous,Duplicate the section to a new card,View the JSON representation of the section,Add a new variable to the section,D,../data/dataiku\statistics\univariate.md
What does the numerical histogram show in a univariate analysis?,The distribution of a categorical variable,The distribution of a continuous variable,The summary statistics of a variable,The frequency table of a variable,B,../data/dataiku\statistics\univariate.md
Which of the following is a graphical tool that summarizes the distribution of numerical data by showing quartiles?,Histogram,Box Plot,Summary Stats,Frequency Table,B,../data/dataiku\statistics\univariate.md
Which of the following is NOT a type of streaming endpoint?,A TOPIC IN A KAFKA CLUSTER,A QUEUE IN SQS,A HTTP SSE ENDPOINT,A SQL DATABASE,D,../data/dataiku\streaming\concepts.md
What is the main difference between regular recipes and continuous recipes?,REGULAR RECIPES CAN ONLY RUN ONCE,CONTINUOUS RECIPES CAN WORK WITHOUT EVER STOPPING,REGULAR RECIPES REQUIRE MANUAL RESTART,CONTINUOUS RECIPES USE STATIC DATASETS,B,../data/dataiku\streaming\concepts.md
What is the role of a continuous activity in Dataiku?,TO MONITOR AND RESTART CONTINUOUS RECIPES IF THEY FAIL,TO PROVIDE DATA FOR REGULAR RECIPES,TO GENERATE REPORTS BASED ON DATASETS,TO CREATE VISUALIZATIONS FROM DATA,A,../data/dataiku\streaming\concepts.md
Which of the following packages is required to consume messages from a Kafka topic using the StreamingEndpoint class?,boto3,sseclient,pykafka,pandas,C,../data/dataiku\streaming\cpython.md
What is the purpose of the flush() method when writing to a streaming endpoint?,To clear the message buffer,To ensure messages are sent to DSS for writing,To reset the connection to the streaming endpoint,To validate the schema of the messages,B,../data/dataiku\streaming\cpython.md
Which of the following is NOT a field returned by the pykafka message object?,timestamp,offset,partition_key,event,D,../data/dataiku\streaming\cpython.md
What is a primary use of a continuous sync recipe in Dataiku?,To generate static reports.,To capture a stream into a dataset for analysis.,To perform batch processing of data.,To create visualizations.,B,../data/dataiku\streaming\csync.md
Which of the following conditions must be met for a continuous sync recipe to offer exactly-once guarantees?,The input streaming endpoint must be replayable.,The output must be a SQL database.,The input must be a file-based dataset.,The output can be atomically checkpointed.,"A, D",../data/dataiku\streaming\csync.md
How does a continuous sync recipe handle partitioning when the output dataset is partitioned with a single time dimension?,It writes messages in a random partition.,It writes messages in a partition corresponding to the time the message was received.,It writes messages in a single static partition.,It does not support partitioning.,B,../data/dataiku\streaming\csync.md
Which of the following message formats is supported by both Kafka and AWS SQS in Dataiku DSS?,XML,JSON,CSV,YAML,B,../data/dataiku\streaming\index.md
Where can you enable streaming features in Dataiku DSS?,Administration > Settings > Misc. > Other,Administration > Settings > Streaming > Enable,Settings > Streaming > Activate,Settings > Misc. > Streaming,A,../data/dataiku\streaming\index.md
Which of the following protocols does DSS support for communicating with Kafka brokers?,HTTP and HTTPS,PLAINTEXT and SASL,FTP and SFTP,SMTP and IMAP,B,../data/dataiku\streaming\kafka.md
What is required when using SSL with Kafka in DSS?,Only a keystore,Only a truststore,Both a keystore and a truststore,No additional requirements,C,../data/dataiku\streaming\kafka.md
Which format does DSS use to read Avro messages from Kafka?,Base64,XML,SerDes from Confluent,Protobuf,C,../data/dataiku\streaming\kafka.md
What format do SQS messages support in Dataiku DSS?,Binary and Text,Text and JSON,XML and JSON,Binary and XML,B,../data/dataiku\streaming\sqs.md
Which method is used to access data from streaming endpoints in DSS?,getStream(),getData(),fetchStream(),retrieveData(),A,../data/dataiku\streaming\streaming_spark_scala.md
What is the purpose of the awaitTermination() call in DSS streaming operations?,To initialize the streaming context,To wait for the sources to stop,To start the streaming process,To terminate the streaming context,B,../data/dataiku\streaming\streaming_spark_scala.md
Which of the following statements is true about wide format time series data?,It stores values from distinct time series in the same column.,It has an identifier column that provides context for the value in each row.,Each distinct time series is in a separate column.,It is a compact way of representing multiple time series.,C,../data/dataiku\time-series\data-formatting.md
Which processor can be used to convert data from wide to long format?,Pivot recipe,Fold multiple columns processor,Aggregate recipe,Join recipe,B,../data/dataiku\time-series\data-formatting.md
Which of the following is NOT a use-case for time series analysis and prediction?,Forecasting of quarterly sales and profits,Weather forecasting,Trend detection,Image classification,D,../data/dataiku\time-series\index.md
What is the primary purpose of time series forecasting?,To classify data into different categories.,To predict future values of time series based on prior values.,To cluster similar data points together.,To reduce the dimensionality of data.,B,../data/dataiku\time-series\time-series-forecasting.md
What type must the timestamp be to use the time series line chart in Dataiku?,String,Integer,Date,Float,C,../data/dataiku\time-series\time-series-visualization.md
Which of the following best describes a univariate time series?,A single variable that depends on time.,Two or more interrelated variables that depend on time.,Multiple unrelated time series.,A single variable that does not depend on time.,A,../data/dataiku\time-series\understanding-time-series.md
What is a characteristic of a multivariate time series?,It consists of a single variable that depends on time.,It consists of two or more interrelated variables that depend on time.,It consists of multiple unrelated time series.,It consists of a single variable that does not depend on time.,B,../data/dataiku\time-series\understanding-time-series.md
Which of the following is an example of multiple time series?,Daily closing stock prices for a specific company.,Temperature and humidity levels of a city.,Daily prices for a group of unrelated companies.,Volume of stocks traded daily for a specific company.,C,../data/dataiku\time-series\understanding-time-series.md
Which of the following is NOT a component of the trend/seasonal decomposition in Dataiku?,Trend,Seasonality,Residuals,Variance,D,../data/dataiku\time-series\time-series-preparation\decomposition.md
What is the decomposition model type that is only compatible with positive numerical values?,Additive,Multiplicative,Linear,Exponential,B,../data/dataiku\time-series\time-series-preparation\decomposition.md
Which parameter controls how rapidly the seasonal component can change in STL decompositions?,Season length,Frequency,Seasonal smoother,Robust to outliers,C,../data/dataiku\time-series\time-series-preparation\decomposition.md
Which of the following window shapes can be applied to the Sum and Average operations in the extrema extraction recipe?,Rectangular,Gaussian,Hamming,All of the above,D,../data/dataiku\time-series\time-series-preparation\extrema-extraction.md
What type of data does the extrema extraction recipe work on?,Categorical columns,Text columns,Numerical columns,Boolean columns,C,../data/dataiku\time-series\time-series-preparation\extrema-extraction.md
What is the purpose of the 'Causal window' option in the extrema extraction recipe?,To include future observations only,To include past and present observations,To include only present observations,"To include past, present, and future observations",B,../data/dataiku\time-series\time-series-preparation\extrema-extraction.md
Which of the following operations is NOT included in the Dataiku DSS time series preparation plugin?,Resampling,Windowing,Extrema extraction,Normalization,D,../data/dataiku\time-series\time-series-preparation\index.md
What is required to install the time series preparation plugin in Dataiku DSS?,Administrative privileges,Python 2.7,A premium account,A special license,A,../data/dataiku\time-series\time-series-preparation\index.md
"From version 2.0.0, which Python version does the time series preparation plugin support?",Python 2.7,Python 3.6,Python 3.7,Python 3.8,B,../data/dataiku\time-series\time-series-preparation\index.md
Which of the following parameters specifies the minimum acceptable value in the time series interval for the interval extraction recipe?,Time column,Minimal valid value,Maximum valid value,Acceptable deviation,B,../data/dataiku\time-series\time-series-preparation\interval-extraction.md
What is the purpose of the 'Long format checkbox' in the interval extraction recipe?,To indicate that the input data is in the long format,To specify the minimum acceptable value,To define the unit of acceptable deviation,To name the column containing timestamps,A,../data/dataiku\time-series\time-series-preparation\interval-extraction.md
Which of the following units can be used to specify the acceptable deviation and minimal segment duration in the interval extraction recipe?,"Days, Hours, Minutes, Seconds, Milliseconds, Microseconds, Nanoseconds","Kilometers, Meters, Centimeters, Millimeters","Bytes, Kilobytes, Megabytes, Gigabytes","Liters, Milliliters, Cubic meters",A,../data/dataiku\time-series\time-series-preparation\interval-extraction.md
Which of the following methods is NOT available for interpolating missing values in the resampling recipe?,Linear,Quadratic,Exponential,Cubic,C,../data/dataiku\time-series\time-series-preparation\resampling.md
What is the purpose of the 'Long format checkbox' parameter in the resampling recipe?,To indicate that the input data is in the long format,To specify the time step for resampling,To name the column that contains the timestamps,To select the interpolation method,A,../data/dataiku\time-series\time-series-preparation\resampling.md
Which of the following units is NOT available for specifying the time step in the resampling recipe?,Weeks,Hours,Decades,Milliseconds,C,../data/dataiku\time-series\time-series-preparation\resampling.md
Which of the following window shapes is specified as a nonlinear window in the shape of a Gaussian distribution?,Rectangular,Triangle,Gaussian,Hamming,C,../data/dataiku\time-series\time-series-preparation\windowing.md
What is the minimum width of the window that can be specified in the windowing recipe?,Equal to the frequency of the time series,Smaller than the frequency of the time series,Larger than the frequency of the time series,Any value,A,../data/dataiku\time-series\time-series-preparation\windowing.md
Which of the following operations can be performed on a window of time series data in the windowing recipe?,Retrieve,Min,First order derivative,All of the above,D,../data/dataiku\time-series\time-series-preparation\windowing.md
Which log file should you check for errors on a Govern node in Dataiku DSS?,backend.log,ipython.log,hproxy.log,governserver.log,D,../data/dataiku\troubleshooting\diagnosing.md
What is the first step to take if you encounter an error message in the DSS UI that you cannot understand?,Contact Dataiku Support immediately,Restart the DSS instance,Study the log files of DSS,Reinstall DSS,C,../data/dataiku\troubleshooting\diagnosing.md
Which of the following is NOT included in the DSS instance diagnosis?,Information about the machine running DSS,Log files of DSS,Your datasets or managed folders data,Information about your Hadoop and Spark setup,C,../data/dataiku\troubleshooting\diagnosing.md
Which of the following support options is specifically available for Dataiku Cloud customers?,Academy,Community Answers,Live chat,Editor support,D,../data/dataiku\troubleshooting\index.md
What is the error code for 'Job activity directory size limit reached'?,ERR_ACTIVITY_DIRECTORY_SIZE_LIMIT_REACHED,ERR_BUNDLE_ACTIVATE_BAD_CONNECTION_PERMISSIONS,ERR_CODEENV_CONTAINER_IMAGE_FAILED,ERR_CONNECTION_API_BAD_CONFIG,A,../data/dataiku\troubleshooting\index.md
Which of the following is NOT a common issue listed in the context?,DSS does not start / Cannot connect,Cannot login to DSS,DSS crashes / The 'Disconnected' overlay appears,Failed to establish HiveServer2 connection,D,../data/dataiku\troubleshooting\index.md
Which of the following support options is available specifically for Dataiku Cloud customers using a hosted instance?,Live chat on Dataiku website,Public community,Native built-in support window,Support portal,C,../data/dataiku\troubleshooting\obtaining-support.md
What is the default priority that should be selected for most issues and questions when opening a support ticket?,P1/Urgent,P2/High,P3/Medium,P4/Low,C,../data/dataiku\troubleshooting\obtaining-support.md
Which of the following support tiers means that Dataiku will make best efforts to solve issues but may consider them as requests for future enhancements with no special priority?,Supported (default),Experimental,Tier 2 support,Public Preview,B,../data/dataiku\troubleshooting\support-tiers.md
Which support tier indicates that Dataiku Support is not able to provide support on the particular capabilities?,Supported (default),Experimental,Tier 2 support,Not supported,D,../data/dataiku\troubleshooting\support-tiers.md
What does it mean when a feature is marked as 'Deprecated' in Dataiku DSS?,The feature is still supported but scheduled for removal in a future release.,The feature is not supported at all.,The feature is in Public Preview and may change significantly.,The feature is experimental and may be upgraded or deprecated.,A,../data/dataiku\troubleshooting\support-tiers.md
What is the default size limit (in megabytes) for the job activity directory in DSS?,10000 MB,15000 MB,20000 MB,25000 MB,C,../data/dataiku\troubleshooting\errors\ERR_ACTIVITY_DIRECTORY_SIZE_LIMIT_REACHED.md
Which of the following actions can a DSS administrator take to prevent the ERR_ACTIVITY_DIRECTORY_SIZE_LIMIT_REACHED error?,Increase the server's disk space,Change the value of dku.recipes.visual.h2based.directorySize.maxMB in config/dip.properties,Use a different data directory,Restart the DSS server,B,../data/dataiku\troubleshooting\errors\ERR_ACTIVITY_DIRECTORY_SIZE_LIMIT_REACHED.md
What happens if a container image build fails when creating or updating a code environment in DSS?,The code environment cannot be used at all.,The code environment can still be used on the DSS backend but not on containers.,The code environment will automatically retry the build.,The code environment will be deleted.,B,../data/dataiku\troubleshooting\errors\ERR_CODEENV_CONTAINER_IMAGE_FAILED.md
Where can you find the logs of the 'docker build' command if a container image build fails?,In the DSS backend settings.,In the code environment creation/update window or in the code environment logs.,In the Dataiku support portal.,In the system's root directory.,B,../data/dataiku\troubleshooting\errors\ERR_CODEENV_CONTAINER_IMAGE_FAILED.md
What is the consequence of the ERR_CODEENV_JUPYTER_SUPPORT_INSTALL_FAILED error in Dataiku?,You cannot use the code environment in code recipes.,You cannot use the code environment for Visual ML.,You cannot use the code environment in Jupyter notebooks.,You cannot use the code environment at all.,C,../data/dataiku\troubleshooting\errors\ERR_CODEENV_JUPYTER_SUPPORT_INSTALL_FAILED.md
What should be done if a code environment for deep learning with pretrained models does not exist?,Ignore the operation and proceed without it.,Install the relevant code environment in the Administration > Misc settings.,Restart the system and try again.,Contact customer support for assistance.,B,../data/dataiku\troubleshooting\errors\ERR_CODEENV_MISSING_DEEPHUB_ENV.md
What should you check first if you encounter the ERR_DATASET_CSV_ROW_TOO_LARGE error?,The dataset's quoting style and quote/escape characters.,The dataset's column names.,The dataset's file size.,The dataset's encoding format.,A,../data/dataiku\troubleshooting\errors\ERR_DATASET_CSV_ROW_TOO_LARGE.md
"If the long row in a CSV file is inherent to the dataset and not due to format configuration, what can you do to resolve the ERR_DATASET_CSV_ROW_TOO_LARGE error?",Increase the limit 'Maximum characters per row'.,Change the dataset's delimiter.,Remove the long row from the dataset.,Convert the CSV file to another format.,A,../data/dataiku\troubleshooting\errors\ERR_DATASET_CSV_ROW_TOO_LARGE.md
What is the most likely cause of the ERR_DATASET_CSV_UNTERMINATED_QUOTE error in Dataiku DSS?,A broken CSV file,Incorrect CSV quoting style,Insufficient memory,Network connectivity issues,B,../data/dataiku\troubleshooting\errors\ERR_DATASET_CSV_UNTERMINATED_QUOTE.md
Which CSV quoting style should you try first if you encounter the ERR_DATASET_CSV_UNTERMINATED_QUOTE error?,Excel,Unix,Escaping only,None of the above,B,../data/dataiku\troubleshooting\errors\ERR_DATASET_CSV_UNTERMINATED_QUOTE.md
What is the first step to remediate an invalid configuration for an API endpoint?,Refer to the documentation on the API Node Endpoints.,Run test queries to validate the endpoint’s configuration.,Go to the settings screen for the affected endpoint.,Ensure the version of the Design/Automation node and the API node is the same.,C,../data/dataiku\troubleshooting\errors\ERR_ENDPOINT_INVALID_CONFIG.md
What should you do if the column selected to represent Object Detection bounding boxes has the wrong format?,Ignore the error and proceed.,Make sure your column follows the supported format by DSS.,Change the column type to integer.,Delete the column and create a new one.,B,../data/dataiku\troubleshooting\errors\ERR_FORMAT_BOUNDING_BOXES.md
Which of the following is NOT a common reason for an HTTP connection failure in DSS?,"The connection settings (host, port, …) to the server are invalid",The HTTP server is not running or not accepting connections,The URL is incorrectly labeled as HTTPS instead of HTTP or vice-versa,The dataset contains too many records,D,../data/dataiku\troubleshooting\errors\ERR_FSPROVIDER_HTTP_CONNECTION_FAILED.md
What is the first step in solving an HTTP connection failure issue in DSS?,Check if a firewall is blocking connections,Try accessing the URLs using the curl command line from the DSS server,Go to the dataset’s or download recipe settings screen,Refer to the documentation on HTTP dataset,C,../data/dataiku\troubleshooting\errors\ERR_FSPROVIDER_HTTP_CONNECTION_FAILED.md
Which of the following is NOT a common reason for an HTTP request failure in DSS?,The URL is incorrect,There was an error on the HTTP server’s side,The URL is correctly labeled as HTTPS instead of HTTP,There is a proxy preventing DSS from fetching that URL,C,../data/dataiku\troubleshooting\errors\ERR_FSPROVIDER_HTTP_REQUEST_FAILED.md
What is the first step for solving an HTTP request failure issue in DSS?,Check if a proxy is between DSS and the HTTP server(s),Refer to the documentation on HTTP dataset,Go to the dataset’s or download recipe settings screen,Try accessing these URLs using the curl command line from the DSS server,C,../data/dataiku\troubleshooting\errors\ERR_FSPROVIDER_HTTP_REQUEST_FAILED.md
Which of the following is NOT a common reason for failure to establish an SSH connection in DSS?,The connection settings to the server are invalid,The SSH server is not running or not accepting connections,There is a firewall blocking connection attempts from DSS to the SSH server,The DSS application is outdated,D,../data/dataiku\troubleshooting\errors\ERR_FSPROVIDER_SSH_CONNECTION_FAILED.md
Which of the following actions can trigger the ERR_FSPROVIDER_TOO_MANY_FILES error in Dataiku DSS?,Trying to read from a dataset that is made up of a very large number of files.,Clicking on the 'List files' button before setting the root path of a dataset.,Trying to browse a folder that contains a very large number of files.,All of the above.,D,../data/dataiku\troubleshooting\errors\ERR_FSPROVIDER_TOO_MANY_FILES.md
Which of the following is NOT a common reason for failure to establish a connection to HiveServer2?,"The connection settings (host, port, …) to the server are invalid",The HiveServer2 server is not running or not accepting connections,The HiveServer2 server is overloaded with requests,There is a firewall blocking connection attempts from DSS to HiveServer2,C,../data/dataiku\troubleshooting\errors\ERR_HIVE_HS2_CONNECTION_FAILED.md
What is the main limitation of the current Hive version mentioned in the context?,It does not support JOIN operations.,It does not support UNION clause but only supports UNION ALL.,It does not support SELECT DISTINCT.,It does not support GROUP BY clause.,B,../data/dataiku\troubleshooting\errors\ERR_HIVE_LEGACY_UNION_SUPPORT.md
Which of the following is NOT a suggested remediation for the Hive UNION clause limitation?,Use stack recipe without distinct postfilter and create a distinct recipe for the result.,Apply distinct recipe on each of the stacked datasets and then stack them.,Create a Hive recipe with a query that uses UNION ALL and then wrap the whole query with SELECT DISTINCT.,Upgrade the Hive version to support UNION clause.,D,../data/dataiku\troubleshooting\errors\ERR_HIVE_LEGACY_UNION_SUPPORT.md
Which of the following is NOT a cause of hard failures of DSS?,Hard crash of DSS,Hard reboot of the machine,Out of disk space,Network latency issues,D,../data/dataiku\troubleshooting\errors\ERR_MISC_EIDB.md
What is a recommended alternative to using internal databases in DSS to avoid corruption?,Using an external PostgreSQL database,Increasing disk space,Regularly rebooting the machine,Using cloud storage solutions,A,../data/dataiku\troubleshooting\errors\ERR_MISC_EIDB.md
What are the two usual underlying causes for the ERR_ML_MODEL_DETAILS_OVERFLOW error in Dataiku?,The model is computed on a dataset with a very large number of columns and the model is tree-based with a large number of nodes.,The model is computed on a dataset with a very large number of rows and the model is tree-based with a large number of nodes.,The model is computed on a dataset with a very large number of columns and the model is linear with a large number of coefficients.,The model is computed on a dataset with a very large number of rows and the model is linear with a large number of coefficients.,A,../data/dataiku\troubleshooting\errors\ERR_ML_MODEL_DETAILS_OVERFLOW.md
What is one way a DSS administrator can remediate the ERR_ML_MODEL_DETAILS_OVERFLOW error?,By increasing the number of columns in the dataset.,By setting the value of dku.fileSizeLimit.modelDetailJson in config/dip.properties.,By reducing the number of rows in the dataset.,By increasing the number of trees in the model.,B,../data/dataiku\troubleshooting\errors\ERR_ML_MODEL_DETAILS_OVERFLOW.md
"As of DSS version 9, which Machine Learning backend is no longer supported?",Hadoop,Vertica,Spark,TensorFlow,B,../data/dataiku\troubleshooting\errors\ERR_ML_VERTICA_NOT_SUPPORTED.md
Which of the following is NOT a common cause for the error 'Cannot check schema consistency because of recipe configuration' in Dataiku DSS?,A Pivot recipe is configured to always recompute output schema.,The modality list of a Pivot recipe is not up-to-date.,A SparkSQL recipe is using global metastore mode.,A Sync recipe is configured in 'strict schema' mode.,D,../data/dataiku\troubleshooting\errors\ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_WITH_RECIPE_CONFIG.md
What is one remediation step for a Pivot recipe that causes a schema consistency error in Dataiku DSS?,Disable the global metastore mode.,Switch the Sync recipe to strict schema mode.,Disable the schema re-computation and run the recipe.,Manually set the schema on the output dataset.,C,../data/dataiku\troubleshooting\errors\ERR_RECIPE_CANNOT_CHECK_SCHEMA_CONSISTENCY_WITH_RECIPE_CONFIG.md
Which of the following is NOT a common example of an incompatibility issue with the selected engine for a recipe in Dataiku?,Using the SQL engine with non-SQL input datasets.,Using SQL computed columns on DSS engine.,Changing the model for one with a different backend.,Using numerical features for ML in SQL.,D,../data/dataiku\troubleshooting\errors\ERR_RECIPE_CANNOT_USE_ENGINE.md
What does the error ERR_SPARK_FAILED_DRIVER_OOM indicate?,The DSS machine is out of memory.,The Spark driver experienced an out of memory situation.,The Spark executors are out of memory.,The Spark code has syntax errors.,B,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_DRIVER_OOM.md
What is a recommended remediation if a Spark code recipe fails due to an out of memory error in the driver?,Increase the number of Spark executors.,Check the code for large allocations performed in the driver.,Decrease the size of the dataset.,Switch to a different programming language.,B,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_DRIVER_OOM.md
What is the default value of the 'spark.driver.memory' setting if not explicitly set?,512 megabytes,2 gigabytes,1 gigabyte,4 gigabytes,C,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_DRIVER_OOM.md
What does the error ERR_SPARK_FAILED_TASK_OOM indicate?,The DSS machine is out of memory.,The Spark processing failed due to a Java out of memory situation in one of the executors.,The Spark code has syntax errors.,The Spark cluster is not configured properly.,B,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_TASK_OOM.md
What is the default value of 'spark.executor.memory' if not set?,512 megabytes,2 gigabytes,1 gigabyte,4 gigabytes,C,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_TASK_OOM.md
"If Spark is running in 'local' master mode, which setting should be increased to allocate more memory?",spark.executor.memory,spark.driver.memory,spark.memory.fraction,spark.memory.storageFraction,B,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_TASK_OOM.md
What does the error ERR_SPARK_FAILED_YARN_KILLED_MEMORY indicate?,YARN has forcefully killed the Spark components due to excessive memory usage.,Spark has completed its task successfully.,YARN has successfully allocated additional memory to Spark.,Spark has encountered a disk space issue.,A,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_YARN_KILLED_MEMORY.md
What is the default memory overhead for Spark executors on YARN?,5% of executor memory,10% of executor memory,15% of executor memory,20% of executor memory,B,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_YARN_KILLED_MEMORY.md
What is the recommended range for setting the value of spark.yarn.executor.memoryOverhead?,100 to 300 MB,300 to 500 MB,500 to 1000 MB,1000 to 1500 MB,C,../data/dataiku\troubleshooting\errors\ERR_SPARK_FAILED_YARN_KILLED_MEMORY.md
What is a JDBC driver used for in DSS?,To connect to SQL databases,To create visualizations,To manage user permissions,To perform data cleaning,A,../data/dataiku\troubleshooting\errors\ERR_SQL_CANNOT_LOAD_DRIVER.md
Which of the following actions can cause the ERR_SQL_CANNOT_LOAD_DRIVER error in DSS?,Trying to create a new connection to a SQL database,Attempting to visualize data,Running a Python script,Exporting a dataset to a CSV file,A,../data/dataiku\troubleshooting\errors\ERR_SQL_CANNOT_LOAD_DRIVER.md
Who can fix the ERR_SQL_CANNOT_LOAD_DRIVER issue in DSS?,Any user,A DSS administrator,A data analyst,A project manager,B,../data/dataiku\troubleshooting\errors\ERR_SQL_CANNOT_LOAD_DRIVER.md
Which of the following is NOT a common reason for the ERR_SQL_DB_UNREACHABLE error in Dataiku DSS?,The connection settings to the database are invalid.,The database server is not running or not accepting connections.,The SQL query syntax is incorrect.,There is a firewall blocking connection attempts from DSS to the database server.,C,../data/dataiku\troubleshooting\errors\ERR_SQL_DB_UNREACHABLE.md
Who can fix the ERR_SQL_DB_UNREACHABLE issue in Dataiku DSS?,Any user with access to the project.,A DSS administrator.,A database administrator.,A data scientist.,B,../data/dataiku\troubleshooting\errors\ERR_SQL_DB_UNREACHABLE.md
What is a possible remediation for the error ERR_SQL_POSTGRESQL_TOOMANYSESSIONS?,Increase the number of simultaneous connections allowed by the database.,Run fewer recipes at the same time.,Limit the number of concurrent activities on the Postgres connection.,All of the above.,D,../data/dataiku\troubleshooting\errors\ERR_SQL_POSTGRESQL_TOOMANYSESSIONS.md
What is the operation called when Vertica merges ROS containers?,Mergein,Mergeout,Mergeup,Mergedown,B,../data/dataiku\troubleshooting\errors\ERR_SQL_VERTICA_TOOMANYROS.md
Which of the following actions can a database administrator take to manually trigger a mergeout in Vertica?,SELECT DO_TM_TASK('mergein');,SELECT DO_TM_TASK('mergeout');,SELECT DO_TM_TASK('mergeup');,SELECT DO_TM_TASK('mergedown');,B,../data/dataiku\troubleshooting\errors\ERR_SQL_VERTICA_TOOMANYROS.md
What is a possible solution if your Vertica database has too many active sessions simultaneously accessing it?,Increase the number of simultaneous connections allowed.,Run fewer recipes at the same time.,Limit the number of concurrent activities on the Vertica connection.,All of the above.,D,../data/dataiku\troubleshooting\errors\ERR_SQL_VERTICA_TOOMANYSESSIONS.md
Which of the following is NOT a reason for the ERR_SYNAPSE_CSV_DELIMITER error when loading CSV data from Azure Blob Storage to Synapse?,The data contains multiline fields.,The field separator is present in the data without a quoting character.,The string delimiter is present in the data and quoting is used.,The CSV file is too large to be processed by Azure.,D,../data/dataiku\troubleshooting\errors\ERR_SYNAPSE_CSV_DELIMITER.md
What is one solution if multiline fields are causing the ERR_SYNAPSE_CSV_DELIMITER error?,Disable the fast-path and use the DSS (stream) engine.,Force using quoting for the recipe.,Switch the input dataset format to CSV with Unix style.,Add a property 'azure_to_synapse.use.quoting -> true' on the input dataset.,A,../data/dataiku\troubleshooting\errors\ERR_SYNAPSE_CSV_DELIMITER.md
Which user profiles are likely to encounter the error ERR_USER_ACTION_FORBIDDEN_BY_PROFILE when attempting to build a Visual Machine Learning model?,READER and DATA_ANALYST,DESIGNER and ADMINISTRATOR,READER and DESIGNER,DATA_ANALYST and ADMINISTRATOR,A,../data/dataiku\troubleshooting\errors\ERR_USER_ACTION_FORBIDDEN_BY_PROFILE.md
What should you do to resolve the error ERR_USER_ACTION_FORBIDDEN_BY_PROFILE?,Restart the DSS application,Update the DSS license,Ask the DSS administrator to update the user profile,Reinstall the DSS software,C,../data/dataiku\troubleshooting\errors\ERR_USER_ACTION_FORBIDDEN_BY_PROFILE.md
What is the potential issue when the maximum heap size (Xmx) is set between 32g and 48g in the Java Virtual Machine?,"Compressed references are enabled, leading to increased memory usage.","Compressed references are disabled, leading to less available heap memory.",The application crashes due to insufficient memory.,The JVM does not start.,B,../data/dataiku\troubleshooting\errors\WARN_JVM_CONFIG_XMX_IN_RED_ZONE.md
What is a potential issue caused by having too many scenario run logs or logs that are too old in a Dataiku instance?,Increased security vulnerabilities,Performance issues and disk space usage,Loss of data integrity,Increased computational power,B,../data/dataiku\troubleshooting\errors\WARN_PROJECT_LARGE_SCENARIO_HISTORY.md
What is one recommended remediation for handling too many or too old scenario run logs in Dataiku?,Increase disk space,Upgrade the Dataiku instance,Manually delete the scenario runs logs,Increase the number of scenario runs,C,../data/dataiku\troubleshooting\errors\WARN_PROJECT_LARGE_SCENARIO_HISTORY.md
What is a potential issue caused by having too many or too old continuous activities logs in a Dataiku project?,Increased security vulnerabilities,Performance issues and disk space fill-up,Loss of project data,Inaccurate machine learning model predictions,B,../data/dataiku\troubleshooting\errors\WARN_PROJECT_LARGE_STREAMING_HISTORY.md
What is one recommended remediation for handling too many or too old continuous activities logs in Dataiku?,Upgrade the Dataiku instance,Increase disk space,Manually delete the logs or run the 'Clear continuous activities logs' macro,Reboot the Dataiku server,C,../data/dataiku\troubleshooting\errors\WARN_PROJECT_LARGE_STREAMING_HISTORY.md
What permission must an administrator grant to allow a Spark job to read or write an S3 dataset directly?,Read connection details,Write connection details,Execute connection details,Modify connection details,A,../data/dataiku\troubleshooting\errors\WARN_RECIPE_SPARK_INDIRECT_S3.md
What can cause a Spark executor running in a Kubernetes pod to be forcefully terminated?,Underestimating the memory overhead factor.,Overestimating the memory overhead factor.,Setting the executor memory too high.,Running too few subprocesses.,A,../data/dataiku\troubleshooting\errors\WARN_SPARK_K8S_KILLED_EXECUTORS.md
Which properties can be adjusted to prevent Spark executors from being killed in Kubernetes?,spark.executor.cores and spark.executor.instances,spark.executor.memory and spark.kubernetes.memoryOverheadFactor,spark.driver.memory and spark.driver.cores,spark.executor.memory and spark.executor.cores,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_K8S_KILLED_EXECUTORS.md
Which of the following operations in PySpark can cause the Spark driver to call the executors directly?,Using 'collect()',Using 'toPandas()',Using 'broadcast join',All of the above,D,../data/dataiku\troubleshooting\errors\WARN_SPARK_MISSING_DRIVER_TO_EXECUTOR_CONNECTIVITY.md
What is a common issue that can prevent the Spark driver from contacting the executors directly?,Insufficient memory on the driver,Firewall rules blocking access to the IPs of the pods,Incorrect Spark version,Driver running out of disk space,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_MISSING_DRIVER_TO_EXECUTOR_CONNECTIVITY.md
Which Spark property can be set to avoid using broadcast join in a PySpark recipe?,spark.sql.autoBroadcastJoinThreshold -> -1,spark.executor.memory -> 4g,spark.driver.maxResultSize -> 2g,spark.sql.shuffle.partitions -> 200,A,../data/dataiku\troubleshooting\errors\WARN_SPARK_MISSING_DRIVER_TO_EXECUTOR_CONNECTIVITY.md
Which of the following storage types is recommended for distributed reads in Spark?,Local Disk,HDFS,FTP Server,NFS,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_NON_DISTRIBUTED_READ.md
What is a potential consequence of Spark reading data in a non-distributed way?,Increased parallelization,Improved network performance,Single worker handling all I/O and conversion,Decreased I/O activity,C,../data/dataiku\troubleshooting\errors\WARN_SPARK_NON_DISTRIBUTED_READ.md
Which of the following storage options is NOT recommended for ensuring compatibility with distributed writes in Spark?,HDFS,S3,Local Disk,GCS,C,../data/dataiku\troubleshooting\errors\WARN_SPARK_NON_DISTRIBUTED_WRITE.md
What is a potential consequence of Spark not being able to handle the writing part directly?,Increased parallelization,Single worker handling all I/O and conversion,Improved network activity,Enhanced data processing speed,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_NON_DISTRIBUTED_WRITE.md
What is one of the recommended remediations for Spark tasks encountering disk space issues in a Kubernetes cluster?,Increase the number of tasks each executor handles concurrently.,Reduce the number of tasks each executor handles concurrently by setting spark.executor.cores to 1.,Decrease parallelism by reducing the number of partitions.,Provision nodes with smaller disks for the Kubernetes cluster.,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_TASK_DISKFULL.md
What is the property name for setting memory overhead in Yarn for Spark executors?,spark.executor.memory,spark.executor.memoryOverhead,spark.kubernetes.memoryOverheadFactor,spark.executor.memoryFactor,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_TASK_OOM.md
What is the minimum recommended value for 'spark.executor.memory'?,1g,2g,4g,8g,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_TASK_OOM.md
What happens when a Spark task exceeds the memory limit enforced by its container?,The task is paused and resumed later.,The container is forcefully terminated by its manager.,The task continues running with reduced performance.,The task is moved to another executor.,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_TASK_OOM.md
What is a potential issue when using Python UDFs in PySpark?,Incompatible Python versions between the driver and executors.,Insufficient memory allocation.,Network latency between the driver and executors.,Lack of support for Python in Spark.,A,../data/dataiku\troubleshooting\errors\WARN_SPARK_UDFS_MAY_BE_BROKEN.md
Which of the following steps is NOT part of the remediation process for Python UDF issues in PySpark?,Go to the code environment used by the recipe in Administration.,Ensure that the Spark config used by the recipe is among the Spark configs in the Containerized execution tab of the code environment.,Update the code environment to force a rebuild of the images.,Increase the memory allocation for the Spark executors.,D,../data/dataiku\troubleshooting\errors\WARN_SPARK_UDFS_MAY_BE_BROKEN.md
What happens if you select a SparkSQL recipe or the Spark engine to execute a job on a Databricks dataset?,The workload will be placed in the Databricks cluster.,The workload will be placed wherever the Spark configuration points.,The job will fail to execute.,The job will run twice as fast.,B,../data/dataiku\troubleshooting\errors\WARN_SPARK_WITH_DATABRICKS_DATASET.md
Which type of recipe should you use to leverage Databricks compute resources when working with Databricks datasets?,Python recipe,SparkSQL recipe,SQL recipe,R recipe,C,../data/dataiku\troubleshooting\errors\WARN_SPARK_WITH_DATABRICKS_DATASET.md
Which of the following browsers are supported by DSS for logging in?,Internet Explorer,Edge,Chrome,Safari,C,../data/dataiku\troubleshooting\problems\cannot-login.md
What are the default login credentials for DSS Free Edition?,"login: user, password: user","login: admin, password: admin","login: root, password: root","login: guest, password: guest",B,../data/dataiku\troubleshooting\problems\cannot-login.md
What should you do if you have lost all access to DSS and need to reset the admin password?,Contact Dataiku support,Use the DSS web interface to reset the password,Use command-line access to the server and run specific commands,Reinstall DSS,C,../data/dataiku\troubleshooting\problems\cannot-login.md
What should you do if the 'dss status' command indicates that some processes are not started?,Ignore the issue and try to connect using a browser.,Restart DSS using './bin/dss restart'.,Reinstall DSS completely.,Check for updates and install the latest version.,B,../data/dataiku\troubleshooting\problems\does-not-start.md
"If you receive a 'Server requires authentication' message, what is the likely cause?",The DSS server is down.,You ran the DSS installer or a dssadmin command while DSS was still running.,There is a network connectivity issue.,The server port is already in use.,B,../data/dataiku\troubleshooting\problems\does-not-start.md
What should you check if you cannot connect to DSS from your browser but local connectivity tests are successful?,Reinstall DSS.,Check for firewalls and proxies along the way.,Restart your computer.,Update your browser.,B,../data/dataiku\troubleshooting\problems\does-not-start.md
Which of the following is a common issue that can prevent DSS from starting?,Server port already in use,Incorrect file permissions,Outdated browser,Insufficient disk space,A,../data/dataiku\troubleshooting\problems\index.md
What should you check if you cannot login to DSS and you are using the Dataiku Cloud Trial?,Server port,Local connectivity,Your credentials,WebSocket connection,C,../data/dataiku\troubleshooting\problems\index.md
What is a possible cause for seeing the 'Could not establish WebSocket connection' message in DSS?,Server port already in use,MemoryError,Incorrect credentials,Network issues,D,../data/dataiku\troubleshooting\problems\index.md
What should you do if a job fails due to a source dataset not being ready?,Retry the job immediately.,Check the error message for the name of the dataset having issues and fix the source data.,Restart the Dataiku instance.,Contact Dataiku Support immediately.,B,../data/dataiku\troubleshooting\problems\job-fails.md
What is the purpose of generating a job diagnosis in Dataiku?,To automatically fix the job failure.,To provide detailed information about the job and DSS instance to Dataiku Support.,To delete the failed job.,To restart the job with new parameters.,B,../data/dataiku\troubleshooting\problems\job-fails.md
What is a common cause of the 'MemoryError' during in-memory training of a machine learning model?,Insufficient disk space,Exhaustion of the machine's memory,Incorrect data types in the dataset,Network connectivity issues,B,../data/dataiku\troubleshooting\problems\ml-train-fails.md
Which of the following methods can help reduce memory requirements during machine learning preprocessing?,Increase the dataset sample size,Use dummy-encoding instead of impact-encoding,Switch dummy-encoding to impact-encoding,Enable hashing for text features,C,../data/dataiku\troubleshooting\problems\ml-train-fails.md
What is a common cause of receiving a 'java.lang.NoClassDefFoundError: org/apache/hadoop/' error in Dataiku DSS?,The Hadoop integration has not been run.,The Java version is outdated.,The DSS installation is corrupted.,The dataset is too large.,A,../data/dataiku\troubleshooting\problems\no-class-def-found.md
What should you do if you receive a 'java.lang.NoClassDefFoundError' while performing normal actions and there is no reference to Hadoop in the error message?,Reinstall DSS.,Restart DSS.,Upgrade Hadoop libraries.,Clear the dataset cache.,B,../data/dataiku\troubleshooting\problems\no-class-def-found.md
What should you do if a scenario fails and you cannot find a reason in the error details or log files?,Restart the DSS instance.,Generate a scenario diagnosis and send it to Dataiku Support.,Delete the scenario and create a new one.,Ignore the error and proceed with other tasks.,B,../data/dataiku\troubleshooting\problems\scenario-fails.md
What is the most common issue with scenario failures in Dataiku?,Network connectivity issues.,One of the 'build' or 'train' steps failed.,Insufficient storage space.,User authentication failure.,B,../data/dataiku\troubleshooting\problems\scenario-fails.md
What should you do if the scenario diagnosis Zip file is larger than 15 MB?,Compress the file further using a different tool.,Send the file directly to Dataiku Support via email.,Use a file transfer service like WeTransfer to send the diagnosis.,Split the file into smaller parts and send them separately.,C,../data/dataiku\troubleshooting\problems\scenario-fails.md
What is the primary reason for the error 'The server selected protocol version TLS10 is not accepted by client preferences [TLS12]' when connecting to a dataset?,The database server is down.,The database does not have TLS 1.2 enabled.,The client is using an outdated SQL driver.,The dataset is corrupted.,B,../data/dataiku\troubleshooting\problems\tls-error.md
"As of April 2021, which versions of TLS were disabled by default in OpenJDK?",TLS 1.0 and TLS 1.1,TLS 1.1 and TLS 1.2,TLS 1.2 and TLS 1.3,TLS 1.0 and TLS 1.3,A,../data/dataiku\troubleshooting\problems\tls-error.md
What should a database administrator do to resolve the TLS error mentioned in the context?,Enable TLS 1.2 on the database server host.,Downgrade the Java version on the database server.,Restart the database server.,Update the SQL driver on the client machine.,A,../data/dataiku\troubleshooting\problems\tls-error.md
Which user profile in DSS generally has all rights?,Reader,Data Scientist,Administrator,Developer,B,../data/dataiku\troubleshooting\problems\user-profile-issues.md
What should you do if you receive a 'Your user profile does not allow' error in DSS?,Restart your computer,Switch to a higher user profile,Reinstall DSS,Contact technical support,B,../data/dataiku\troubleshooting\problems\user-profile-issues.md
What is the default user profile if a user has no profile associated with it in DSS?,Administrator,Data Scientist,Reader,Developer,C,../data/dataiku\troubleshooting\problems\user-profile-issues.md
Which of the following is a common cause of Websocket errors in Data Science Studio (DSS)?,Using an outdated browser,Connecting to DSS through a proxy,Insufficient RAM,Incorrect login credentials,B,../data/dataiku\troubleshooting\problems\websockets.md
Which feature of DSS will NOT work if Websockets are not functioning?,Data import from CSV files,Dynamic notifications,SQL query execution,Data visualization,B,../data/dataiku\troubleshooting\problems\websockets.md
Which plugin can be used to transcribe audio files containing speech in Dataiku?,Audio Manipulation,Speech to Text,Text to Speech,Audio Transcription,B,../data/dataiku\unstructured-data\audio\index.md
Is the 'Speech to Text' plugin fully supported by Dataiku?,Yes,No,Partially,Not mentioned,B,../data/dataiku\unstructured-data\audio\index.md
Which of the following tasks can be performed natively in Dataiku's Visual ML?,Image classification and object detection,Image segmentation and feature extraction,Object recognition and image segmentation,Feature extraction and deep learning,A,../data/dataiku\unstructured-data\image\index.md
Which of the following features is available only with the User Isolation Framework (UIF) in Dataiku?,Access control on projects,Per-user credentials on SQL connections,Authentication against LDAP directory,Connecting to secure Hadoop clusters (Kerberos),B,../data/dataiku\user-isolation\capabilities-summary.md
"What is the isolation status of executing regular code (Python, R) locally without UIF?",Isolated,Not isolated,Partially isolated,Depends on the code,B,../data/dataiku\user-isolation\capabilities-summary.md
Which of the following is NOT isolated without UIF?,"Execution of regular code (Python, R) on Kubernetes","Execution of Spark code (Python, R, Scala) on YARN","Execution of Spark code (Python, R, Scala) on Kubernetes",Authentication with Single-Sign-On,B,../data/dataiku\user-isolation\capabilities-summary.md
Which mechanism does DSS use for local code isolation on the DSS host?,Docker,Proxy user,Sudo,Native impersonation,C,../data/dataiku\user-isolation\concepts.md
What is the fundamental layer required for the User Isolation Framework (UIF) to function?,Hadoop impersonation,Local code isolation,Docker containerization,SQL database impersonation,B,../data/dataiku\user-isolation\concepts.md
"In the context of DSS, what does 'impersonation' mean?",Running code in isolated Docker containers,Submitting work to a cluster on behalf of another user,Mapping DSS end-users to UNIX/Hadoop users,Ensuring code in one container cannot access another container,B,../data/dataiku\user-isolation\concepts.md
What is the primary purpose of the User Isolation Framework in Dataiku DSS?,To enhance the performance of data processing tasks.,To provide a user-friendly interface for data analysis.,To guarantee traceability and prevent hostile users from attacking the dssuser.,To simplify the installation process of DSS.,C,../data/dataiku\user-isolation\index.md
Which of the following is NOT a limitation of the default behavior of DSS without User Isolation?,Lack of traceability on Hadoop clusters.,Inability to run code recipes.,Potential for hostile users to run arbitrary code as UNIX dssuser.,Actions performed on external databases use the credentials configured in the database connection.,B,../data/dataiku\user-isolation\index.md
"In the context of Dataiku DSS, what does the term 'dssuser' refer to?",A default user account on the host machine that runs DSS services.,A type of data processing task.,A specific feature for data visualization.,A configuration setting for database connections.,A,../data/dataiku\user-isolation\index.md
Which of the following is NOT a prerequisite for the initial setup of the local code isolation capability of the User Isolation Framework?,Having a keytab for the dssuser,Having administrator access to the Hadoop cluster,Having root access to the local machine,Having a list of end-user groups allowed to use the impersonation mechanisms,D,../data/dataiku\user-isolation\initial-setup.md
What is the recommended order of setup when integrating DSS with Spark and UIF?,"Setup UIF first, then Spark integration","Setup Spark integration first, then UIF",Setup both simultaneously,It does not matter which is set up first,B,../data/dataiku\user-isolation\initial-setup.md
Which of the following is a valid identity mapping rule example for DSS to UNIX user mapping?,"Matching pattern: `([^@]*)@mydomain.com`, Replacement (UNIX): `$1`","Matching pattern: `([^@]*)@domain.com`, Replacement (UNIX): `$2`","Matching pattern: `([^@]*)@mydomain.com`, Replacement (UNIX): `$2`","Matching pattern: `([^@]*)@domain.com`, Replacement (UNIX): `$1`",A,../data/dataiku\user-isolation\initial-setup.md
Which of the following is a prerequisite for setting up the User Isolation Framework on a local machine?,The user must have a valid UNIX account.,Kerberos security must be enabled.,You need a keytab for the 'dssuser'.,You must have Ranger enabled.,A,../data/dataiku\user-isolation\prerequisites-limitations.md
What is required on a Hadoop cluster to set up user isolation?,ACL support must be enabled on the filesystem.,You need root access to setup the User Isolation Framework.,You need administrator access to your Hadoop cluster.,The user must have a writable home directory on HDFS.,C,../data/dataiku\user-isolation\prerequisites-limitations.md
Which of the following features are not available for end-users unless they have the 'Write unsafe code' permission?,Using a HiveContext in Spark code recipes.,Write custom partition dependency functions.,Using table definitions from the Hive metastore in Spark code recipes.,Running some visual recipes on Spark.,B,../data/dataiku\user-isolation\prerequisites-limitations.md
What should you do if you encounter a sudo error due to devtoolset-8 being installed and UIF being enabled?,Uninstall devtoolset-8,Force the use of the default sudo,Disable UIF,Reinstall DSS,B,../data/dataiku\user-isolation\troubleshooting.md
"When user isolation for Hadoop is enabled, where is the actual data written if the dataset’s configured location is `/user/dataiku/datasets/MYPROJECT/mydataset`?",/user/dataiku/datasets/MYPROJECT/mydataset,/user/dataiku/datasets/MYPROJECT/mydataset/data,/user/dataiku/datasets/MYPROJECT/mydataset/files,/user/dataiku/datasets/MYPROJECT/mydataset/actual_data,B,../data/dataiku\user-isolation\advanced\hdfs-datasets.md
What is the purpose of the sudoers rule added by DSS during impersonation setup?,To allow DSS to run user-submitted code as root.,To execute user-submitted code as the end-user UNIX accounts.,To change the DSS installation directory.,To install additional software packages.,B,../data/dataiku\user-isolation\advanced\local-security.md
Where does DSS consult the security module configuration when running a command on behalf of an end-user?,/etc/sudoers.d/dataiku-dss-THE_DSS_USER-RANDOM_STRING,/etc/dataiku-security/INSTALL_ID/security/security-config.ini,/usr/local/dataiku/security-config.ini,/var/dataiku/security/security-config.ini,B,../data/dataiku\user-isolation\advanced\local-security.md
Which of the following is NOT a limitation of the DSS-managed ACL synchronization mode?,HDFS has a strong and non-workaroundable limitation to 32 ACL entries per file.,Appending in a HDFS dataset can only be done by a single user.,HDFS ACLs are supported for per-project single user permissions.,Occasional failures can happen with partitioned datasets and require retries.,C,../data/dataiku\user-isolation\capabilities\hadoop-impersonation.md
What is the primary advantage of using Ranger over DSS-managed ACLs?,Centralized authorization in Sentry/Ranger.,Increased number of HDFS ACLs per path.,Better integration with Spark on YARN.,Improved performance of HiveServer2.,A,../data/dataiku\user-isolation\capabilities\hadoop-impersonation.md
What happens when UIF for Hadoop is enabled in DSS?,Access to HDFS is performed with the 'dssuser' identity.,Access to HDFS is impersonated with an end-user identity.,DSS does not interact with HDFS permissions.,DSS automatically manages all authorizations on HDFS data.,B,../data/dataiku\user-isolation\capabilities\hadoop-impersonation.md
What is a fundamental property of Kubernetes when running non-Spark workloads?,Each container can access others without restrictions.,Each container is independent and cannot access others.,Containers require specific permissions to run.,Containers are always started by the root user.,B,../data/dataiku\user-isolation\capabilities\kubernetes.md
What mechanism does DSS use to start the Spark driver process when running Spark workloads on Kubernetes?,Direct API calls,Sudo mechanism of the local code isolation capability,Root user privileges,Manual user intervention,B,../data/dataiku\user-isolation\capabilities\kubernetes.md
What is recommended for security when using the 'managed Spark on Kubernetes' mode?,Using one namespace per user or team,Using a single namespace for all users,Granting all users root access,Disabling namespace creation,A,../data/dataiku\user-isolation\capabilities\kubernetes.md
Which of the following is NOT a prerequisite for setting up DSS with Cloudera?,Having a keytab for the `dssuser`,Having administrator access to the Cloudera cluster,Having root access to the local machine,Having a dedicated server for DSS,D,../data/dataiku\user-isolation\reference-architectures\cloudera.md
What is the recommended mode for deploying UIF on Cloudera?,DSS-managed ACL synchronization,Ranger mode,Manual ACL management,None of the above,B,../data/dataiku\user-isolation\reference-architectures\cloudera.md
What is the primary advantage of using Ranger mode over DSS-managed ACLs?,Centralized authorization in Ranger,Easier setup process,Better performance,Lower cost,A,../data/dataiku\user-isolation\reference-architectures\cloudera.md
Which of the following setups is NOT mentioned in the reference architectures for Dataiku?,Local-code only,Setup with Kubernetes,Setup with Cloudera,Setup with AWS,D,../data/dataiku\user-isolation\reference-architectures\index.md
What is a fundamental property of Kubernetes when running non-Spark workloads?,Each container can access others without restrictions.,Each container is independent and cannot access others.,Containers require impersonation to run.,Containers share the same namespace by default.,B,../data/dataiku\user-isolation\reference-architectures\kubernetes.md
What mechanism does DSS use to start the spark-submit process for Spark workloads on Kubernetes?,Direct API calls,sudo mechanism of the local code isolation capability,Manual user intervention,Automated scripts,B,../data/dataiku\user-isolation\reference-architectures\kubernetes.md
"In the 'one-namespace-per-user' setup for Spark workloads, what happens at the end of the job?",The namespace is deleted.,The service account is destroyed.,The Spark driver is restarted.,The user credentials are revoked.,B,../data/dataiku\user-isolation\reference-architectures\kubernetes.md
Which of the following languages can be used to retrieve custom variables in Dataiku DSS?,SQL,Python,R,All of the above,D,../data/dataiku\variables\index.md
"Which of the following chart layouts puts the binning column on the X axis and the summary column on the Y axis, and creates separate bars for each subgroup for each bin?",Histogram,Stacked,Lines,Pie,A,../data/dataiku\visualization\charts-basics.md
"In which chart layout does the binning column go on the X axis and the summary column on the Y axis, and stacks each subgroup within each bin?",Histogram,Stacked,Lines,Mixed,B,../data/dataiku\visualization\charts-basics.md
Which of the following chart types is most appropriate when there is no inherent ordering of the bins?,Histogram,Stacked Area,Pie,Lines,C,../data/dataiku\visualization\charts-basics.md
Which of the following map chart layouts allows you to add optional Color and Size columns that change the color and size of the points based upon the column values?,Scatter Map,Geometry Map,Density Map,Grid Map,A,../data/dataiku\visualization\charts-maps.md
What is required to create Administrative maps in Dataiku?,Installing the Reverse Geocoding plugin,Installing the Mapbox plugin,Using a GeoJSON dataset,Using a WKT format column,A,../data/dataiku\visualization\charts-maps.md
Which of the following is NOT a valid way to obtain geographic columns in DSS?,By reading a GeoJSON or Shapefile dataset,By reading any dataset containing a column in WKT format,By applying a geocoding processor in a preparation script,By using a Mapbox API key,D,../data/dataiku\visualization\charts-maps.md
"Which of the following charts is used to depict the flow of resources, quantities, or values from one set of entities to another?",Boxplot,2D Distribution,Sankey,Radar,C,../data/dataiku\visualization\charts-other.md
Which chart type is deprecated and provided for models in Visual ML results?,Lift Chart,Treemap,KPI,Gauge,A,../data/dataiku\visualization\charts-other.md
Which chart builds a visualization of the hierarchical structure of tree diagrams?,Boxplot,Treemap,Radar,Gauge,B,../data/dataiku\visualization\charts-other.md
Which of the following Scatter chart layouts allows you to add an optional Shape column that changes the shape of the points based upon the column’s values?,Basic,Multi-pair,Grouped,Binned,A,../data/dataiku\visualization\charts-scatters.md
In which Scatter chart layout is it not possible to define a color dimension?,Basic,Multi-pair,Grouped,Binned,B,../data/dataiku\visualization\charts-scatters.md
Which type of regression line can only be calculated for x values bigger than 0?,Linear,Polynomial,Logarithmic,Exponential,C,../data/dataiku\visualization\charts-scatters.md
Which control in the Setup tab is used to animate a chart in Dataiku?,Filters,Color,Animation,Tooltip,C,../data/dataiku\visualization\common.md
Which of the following is NOT a predefined aggregation function available in Dataiku?,SUM,COUNT,AVERAGE,MEDIAN,D,../data/dataiku\visualization\custom-aggregations.md
What must custom aggregations adhere to in Dataiku?,The structure of a simple calculation,The structure of an aggregation,The structure of a new column,The structure of a dataset,B,../data/dataiku\visualization\custom-aggregations.md
Which of the following is a valid aggregation operation that can be used in custom aggregations in Dataiku?,COLUMN_1 + 10,countd,COLUMN_2 * 5,COLUMN_3 / 2,B,../data/dataiku\visualization\custom-aggregations.md
What is the default behavior for dashboards regarding the inclusion or exclusion of other values in filter settings?,Include other values,Exclude other values,Only relevant values,All values in sample,B,../data/dataiku\visualization\filter-settings.md
Which option in filter settings allows values to be restricted based on other filters?,Include other values,Exclude other values,Only relevant values,All values in sample,C,../data/dataiku\visualization\filter-settings.md
Which of the following is a feature of charts in a dataset but not in a visual analysis?,Charts can work in real-time on the output of a data preparation script.,Charts can be published as insights for inclusion in dashboards.,Charts can make use of the in-database execution engine.,Charts can be transferred during deployment to the output dataset.,B,../data/dataiku\visualization\index.md
"Which tab in the Columns, Sampling & Engine panel allows you to set how the dataset is processed?",Columns tab,Sampling & Engine tab,Chart type selector,Chart definition elements,B,../data/dataiku\visualization\interface.md
What does the Publish button do in the Charts interface?,Creates a new chart,Deletes an existing chart,Creates an Insight from the Chart and publishes it to a Dashboard,Duplicates an existing chart,C,../data/dataiku\visualization\interface.md
Where can you find the measures and dimensions settings in the Charts interface?,Chart type selector,Columns tab,Chart definition elements,Common chart elements,C,../data/dataiku\visualization\interface.md
Which type of color palette in DSS is used when the color dimension is categorical?,Discrete palettes,Continuous palettes,Diverging palettes,Custom palettes,A,../data/dataiku\visualization\palettes.md
What is a key feature of diverging palettes in DSS?,They are used for categorical data.,They have at least 3 colors and can easily select the value for the middle color.,They define a fixed number of colors and interpolate between them.,They are local to a chart and cannot be shared between charts.,B,../data/dataiku\visualization\palettes.md
What is the purpose of quantization in continuous palettes in DSS?,To assign a color per value.,To divide the color space and pick the middle color for each range.,To create reusable charts by adding them to a plugin.,To associate a color to each of the values in a 'values list'.,B,../data/dataiku\visualization\palettes.md
Which of the following charts does NOT support the 'Displayed Aggregation' source for reference lines?,Vertical bars,Horizontal stacked bars,Scatterplot,Lines,C,../data/dataiku\visualization\reference-lines.md
Which of the following is NOT a source choice for defining reference lines?,Constant Value,Displayed Aggregation,Dataset Column,Chart Axis,D,../data/dataiku\visualization\reference-lines.md
Which of the following statements about the DSS engine is NOT true?,The DSS engine uses a highly-optimized column-based and compressed storage format.,The DSS engine requires that the chart data be loaded in memory.,The DSS engine can perform visual analytic queries on all data sources that DSS supports.,The DSS engine extracts data from your data source and transforms it in its optimized format.,B,../data/dataiku\visualization\sampling.md
Which of the following datasets is NOT supported by the In-database engine for visual analytic queries?,PostgreSQL,MySQL,Vertica,CSV files,D,../data/dataiku\visualization\sampling.md
What is the default memory usage limit for a chart in DSS?,100MB,150MB,200MB,250MB,B,../data/dataiku\visualization\sampling.md
Which of the following BI tools can Dataiku export datasets to using a plugin?,Tableau,Microsoft PowerBI,Qlik,All of the above,D,../data/dataiku\visualization\third-party-bi-tools.md
Which plugin allows Dataiku to produce a .hyper file for Tableau Desktop?,Tableau Hyper Export plugin,PowerBI plugin,Qlik QVX plugin,Microstrategy plugin,A,../data/dataiku\visualization\third-party-bi-tools.md
What is a typical usage scenario for integrating Dataiku with Microsoft PowerBI?,"Building workflows in DSS to create complex data transformation pipelines and machine learning models, then pushing the output to PowerBI for visualization.",Building workflows in DSS to create simple data transformation pipelines and pushing the output to Qlik for visualization.,"Building workflows in DSS to create complex data transformation pipelines and machine learning models, then pushing the output to Tableau for visualization.",Building workflows in DSS to create simple data transformation pipelines and pushing the output to Microstrategy for visualization.,A,../data/dataiku\visualization\third-party-bi-tools.md
Which of the following statements is true about creating Dash web apps in DSS?,Dash is installed by default in DSS.,You need to install the dash package and create a code environment.,DSS supports only Dash versions 0.x.,You cannot use the regular Python API in Dash web apps.,B,../data/dataiku\webapps\dash.md
What happens to the dashboard when you save a webapp in Dataiku DSS?,The dashboard remains unchanged.,The dashboard is updated immediately with the latest version of the webapp.,The dashboard shows an error message.,The dashboard needs to be manually refreshed.,B,../data/dataiku\webapps\dashboard.md
How can 'Standard' web apps access dashboard filters in Dataiku DSS?,By using a special API endpoint.,By listening for messages sent from the dashboard.,By manually updating the filter settings.,By reloading the webapp.,B,../data/dataiku\webapps\dashboard.md
Which of the following is NOT a type of web app supported by Dataiku DSS?,Standard web apps,Shiny web apps,Bokeh web apps,React web apps,D,../data/dataiku\webapps\index.md
Which section of the Developer Guide would you refer to for tutorials on using webapps in Dataiku DSS?,Introduction to DSS webapps,Example use cases,Publishing webapps on the dashboard,Webapps and security,A,../data/dataiku\webapps\index.md
What is one of the example use cases for webapps in Dataiku DSS?,Data cleaning,Custom visualizations,Model training,Data storage,B,../data/dataiku\webapps\index.md
Which of the following web app types in DSS allows you to write both server and frontend code using the R programming language?,Standard web apps,Shiny web apps,Bokeh web apps,Dash web apps,B,../data/dataiku\webapps\introduction.md
Which of the following libraries is used for creating Bokeh web apps in DSS?,d3.js,Shiny,Bokeh,Dash,C,../data/dataiku\webapps\introduction.md
"In a standard web app in DSS, which of the following languages is NOT typically used?",HTML,CSS,Javascript,R,D,../data/dataiku\webapps\introduction.md
What is required to make a webapp public in DSS?,Adding the webappId to the authentication whitelist,Granting 'Read dashboards' permission to all users,Granting 'Read project content' permission to all users,"No special requirements, all webapps are public by default",A,../data/dataiku\webapps\public.md
Who typically decides whether a webapp can be made public in DSS?,Project Manager,Administrator,Any logged-in user,Dashboard user,B,../data/dataiku\webapps\public.md
Which of the following is the default behavior for webapp user authentication in Dataiku?,Webapps do not require users to be authenticated.,Webapps require users to be authenticated.,Webapps require users to be authenticated only during the first access.,Webapps require users to be authenticated only if they access sensitive data.,B,../data/dataiku\webapps\security.md
"In Dataiku, which method can be used to retrieve the request headers in a Bokeh webapp?",get_bokeh_session_headers,get_bokeh_request_headers,get_bokeh_headers,get_bokeh_auth_headers,A,../data/dataiku\webapps\security.md
What permission must be granted to the 'run-as-user' of a webapp to allow impersonation of other users in the Dataiku API?,Impersonation in webapps,API access,User management,Data access,A,../data/dataiku\webapps\security.md
Which of the following objects can be shared into a Dataiku workspace?,Applications,Dashboards,Datasets,All of the above,D,../data/dataiku\workspaces\index.md
Which of the following roles in a workspace can add or remove users?,Admins,Contributors,Members,All of the above,A,../data/dataiku\workspaces\managing.md
What permission is granted to everyone in a workspace by default?,Write,Read,Execute,Delete,B,../data/dataiku\workspaces\managing.md
How can a user share DSS objects into a Dataiku Workspace?,From the object’s Right Pane within the project or from the (+) button within the workspace.,Only from the object’s Right Pane within the project.,Only from the (+) button within the workspace.,By sending an email to the workspace administrator.,A,../data/dataiku\workspaces\sharing-to-workspaces.md
What happens to the visibility of a DSS object once it is shared with a Dataiku Workspace?,It is visible only to the user who shared it.,"It is visible to all users in the workspace, regardless of role.",It is visible only to users with administrative privileges.,It is visible only to users with editing permissions.,B,../data/dataiku\workspaces\sharing-to-workspaces.md
