{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 27 16:07:54 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000                On | 00000000:AF:00.0 Off |                  Off |\n",
      "| 42%   59C    P8                8W / 300W|  14172MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000                On | 00000000:D8:00.0 Off |                  Off |\n",
      "| 35%   63C    P2               93W / 300W|  33688MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3624      C   python                                      740MiB |\n",
      "|    0   N/A  N/A      3861      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A     17918      C   python                                     2620MiB |\n",
      "|    0   N/A  N/A     36934      C   ...unners/cuda_v11/ollama_llama_server    10770MiB |\n",
      "|    1   N/A  N/A      3861      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from prompts import GRADE_QA_PROMPT,QUESTION_FORMAT\n",
    "\n",
    "\n",
    "def remove_duplicate_answer(answer: str) -> str:\n",
    "    selections = answer.split(\", \")\n",
    "    selections = sorted(list(set(selections)))\n",
    "    return \", \".join(selections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run LLM through Ollama for better inference speed ###\n",
    "# docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "# docker exec -it ollama ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(base_url=\"http://localhost:11434\", model=\"llama3\", request_timeout=60.0, temperature=0.1)\n",
    "dataset = pd.read_csv(\"../data/qa_dataset/dataiku_multiple_choice_qa.csv\")\n",
    "\n",
    "questions = [QUESTION_FORMAT.format(**row.to_dict()) for _, row in dataset.iterrows()]\n",
    "prompts = [GRADE_QA_PROMPT.format(query_str=question) for question in questions]\n",
    "\n",
    "assert dataset.shape[0] == len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.copy()\n",
    "\n",
    "foundation_llama3 = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = llm.complete(prompt)\n",
    "    foundation_llama3.append(response.text)\n",
    "    \n",
    "results[\"answer_foundation_llama3\"] = foundation_llama3\n",
    "\n",
    "results[\"answer\"] = results[\"answer\"].apply(remove_duplicate_answer)\n",
    "results[\"answer_foundation_llama3\"] = results[\"answer_foundation_llama3\"].apply(remove_duplicate_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_foundation_llama3\n",
       "D             509\n",
       "A             399\n",
       "B             322\n",
       "C             247\n",
       "A, B          219\n",
       "A, C          124\n",
       "B, C           75\n",
       "A, D           31\n",
       "A, B, C        21\n",
       "B, D            9\n",
       "C, D            8\n",
       "A, B, D         5\n",
       "A, B, C, D      5\n",
       "B, C, D         2\n",
       "A, C, D         2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"answer_foundation_llama3\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.606673407482305"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation_llama3_scores = (results[\"answer\"] == results[\"answer_foundation_llama3\"]).sum() / results.shape[0] * 100\n",
    "foundation_llama3_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../data/results/foundation_llama3_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
